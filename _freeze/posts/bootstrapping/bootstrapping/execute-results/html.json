{
  "hash": "5398a31bdad3e25e256102e48261b6bb",
  "result": {
    "markdown": "---\ntitle: \"Bootstrapping\"\nauthor: \"Johannes Buck\"\ndate: \"2023-08-01\"\ncategories: [statistics, risk management]\ndescription: The bootstrap method is a statistical resampling technique used to estimate the properties of a population by repeatedly sampling with replacement from a given data set.\nimage: bootstrap.jpg\n---\n\n\n## Bootstrapping Method\n\nBootstrapping is a statistical method that resamples a single data set to create many samples. This process allows to calculate standard errors, construct confidence intervals, and perform hypothesis testing for numerous types of sample statistics.\n\nThe basic idea of bootstrapping is that inference about a population from sample data (sample → population) can be modeled by resampling the sample data and performing inference about a sample from resampled data (resampled → sample).\n\n![](/posts/bootstrapping/Traditional%20statistics%20method.png)\n\nFigure 1: Traditional statistical method\n\n![](/posts/bootstrapping/Bootstrapping%20method.png)\n\nFigure 2: Bootstrapping method\n\n**Simple Random Samples (SRS)**\n\nA simple random sample (SRS) of size $n$ consists of $n$ individuals from the population, selected so that each set of $n$ individuals has an equal chance of being the sample actually selected.\n\n**Populations vs. Samples**\n\nParameters describe populations, statistics describe samples.\n\n-   The mean ($\\mu$) and standard deviation ($\\sigma$) of a population are parameters.\n\n-   The mean ($\\bar{x}$) and standard deviation ($s$) of a sample are statistics.\n\n## Types of bootstrapping\n\n### Parametric Bootstrapping\n\nParametric bootstraps resample a known distribution function \\$F\\$ (e.g., normal) whose parameters (e.g., mean, variance) are estimated from the sample data.\n\n### Nonparametric Bootstraps\n\nNonparametric bootstraps make no assumptions $\\hat{F}$ about the underlying data distribution $F$.\n\nProcess steps:\n\n1.  Estimate the parameters of the hypothesized parametric model (parametric bootstrapping only).\n\n2.  Take a random sample from the original data by sampling with replacement, with the same number of items as in the original data set.\n\n3.  Compute the statistic of interest (e.g., mean, median, standard deviation) from the bootstrapped sample.\n\n4.  Repeat steps $(2)$ and $(3)$ a large number of times.\n\n5.  Analyze the distribution of the bootstrap statistics or statistics of interest (e.g., mean, standard deviation, confidence intervals).\n\n::: callout-note\n**Parametric bootstrapping** can be efficient when the underlying data distribution is well known. However, it can lead to biased results if the assumed model is incorrect.\n\n**Nonparametric bootstrapping** is more flexible because it does not rely on specific distribution assumptions. It can be particularly useful when the underlying data distribution is unknown or complex. However, it may require a larger number of bootstrap samples to obtain accurate estimates compared to parametric bootstrapping.\n\n$\\rightarrow$ Parametric bootstrapping assumes a specific distribution and estimates parameters from the data, while nonparametric bootstrapping makes no assumption about the distribution and estimates the sampling distribution directly through resampling.\n:::\n\n## Bootstrapping vs. Monte Carlo Simulation\n\nBootstrapping and Monte Carlo simulation are two different statistical techniques that are used for different purposes. Bootstrapping can be seen as a specific application of Monte Carlo methods, where random resampling is used to estimate sampling distributions and perform statistical inference.\n\n### About Bootstrapping\n\nBootstrapping is a resampling technique used to estimate the sampling distribution of a statistic by repeatedly sampling with replacement from the original data.\n\n**Advantages**\n\n1.  **Nonparametric:** Bootstrapping does not assume a specific underlying distribution, making it useful when the distribution of the data is unknown or non-standard.\n\n2.  **Easy to implement:** Bootstrapping is conceptually simple and easy to implement.\n\n3.  **Useful with small sample sizes:** It works with limited data where traditional statistical tests may be less reliable.\n\n**Disadvantages**\n\n1.  **Computationally intensive:** Bootstrapping involves many resampling iterations, which can be computationally expensive for large datasets or complex models.\n\n2.  **Data quality dependence:** Bootstrapping results are highly dependent on the quality and representativeness of the original data.\n\n3.  **Not suitable for all scenarios:** While powerful, bootstrapping may not be the best approach for certain statistical problems, especially those involving complex models or non-standard scenarios.\n\n## Monte Carlo Simulation\n\nMonte Carlo simulation is a technique used to model and analyze the probability of different outcomes in a process that involves uncertainty and randomness. It relies on random sampling to estimate complex mathematical functions or to simulate different scenarios.\n\n**Advantages**\n\n1.  **Flexibility:** Monte Carlo simulation is applicable to a wide range of problems, including optimization, probability estimation, and risk analysis, making it a versatile tool.\n\n2.  **Provides a range of outcomes:** It provides a distribution of possible outcomes, giving insight into the likelihood of different scenarios occurring.\n\n3.  **Works well with optimization problems:** Monte Carlo simulation can be integrated with optimization algorithms to find optimal solutions in complex systems.\n\n**Disadvantages**\n\n1.  **Resource intensive:** Monte Carlo simulation requires a large number of iterations to produce reliable results, which can be computationally expensive and time consuming.\n\n2.  **Convergence issues:** In some cases, Monte Carlo simulation may converge slowly or require special techniques to ensure accurate results.\n\nKey Differences Between Monte Carlo Simulation and Bootstrapping\n\n| Feature     | Mote Carlo simulation                                    | Bootstrapping method                                                      |\n|-------------------|---------------------------|--------------------------|\n| Generality  | Can be used to estimate the uncertainty of any statistic | Only used to estimate the uncertainty of statistics from a sample of data |\n| Efficiency  | Less efficient than bootstrapping                        | More efficient than Monte Carlo simulation                                |\n| Ease of use | More difficult to use than bootstrapping                 | Easier to use than Monte Carlo simulation                                 |\n\n## R code\n\nThere are several R packages that can be used for bootstrapping calculations with R:\n\n-   boot\n\n-   bootstrap\n\n-   nptest\n\n### Example 1 **--** Univariate Statistic (Median)\n\nFor this example, we will generate $n = 100$ observations from a standard normal distribution, and use the median as the parameter/statistic of interest. Note that the true (population) median is zero. Since the median is a univariate statistic, the bootstrap distribution will be a vector of length $R + 1$ containing the bootstrap replicates of the median.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Univariate Statistic (Median)\n\nlibrary(nptest)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nPackage 'nptest' version 1.1\nType 'citation(\"nptest\")' to cite this package.\n```\n:::\n\n```{.r .cell-code}\n# generate 100 standard normal observations\nset.seed(1)\nn <- 100\nx <- rnorm(n)\n\n# nonparametric bootstrap\nnpbs <- np.boot(x = x, statistic = median)\nnpbs\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nNonparametric Bootstrap of Univariate Statistic\nusing R = 9999 bootstrap replicates\n\n  t0: 0.1139\n  SE: 0.1394\nBias: 0.0185 \n\nBCa Confidence Intervals:\n      lower  upper\n90% -0.0566 0.3411\n95% -0.0811 0.3673\n99% -0.1351 0.3940\n```\n:::\n\n```{.r .cell-code}\n# check t0, SE, and bias\nmedian(x)                          # t0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1139092\n```\n:::\n\n```{.r .cell-code}\nsd(npbs$boot.dist)                 # SE\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1393567\n```\n:::\n\n```{.r .cell-code}\nmean(npbs$boot.dist) - npbs$t0     # Bias\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.01845341\n```\n:::\n\n```{.r .cell-code}\n# bootstrap distribution\nhist(npbs$boot.dist,\n     xlab = \"t*\",\n     ylab = \"Density\",\n     main = \"Bootstrap Distribution\")\n\n# Observed media\nabline(v = npbs$t0,\n       lty = 2,\n       col = \"red\")\n\n# CI 95% lower boundary\nabline(v = npbs$bca[2, 1],\n       lty = 2,\n       col = \"blue\")\n\n# CI 95% upper boundary\nabline(v = npbs$bca[2, 2],\n       lty = 2,\n       col = \"blue\")\n\nlegend(\n  \"topleft\",\n  legend = c(\"t0\", \"CI 95%\"),\n  col = c(\"red\", \"blue\"),\n  lty = 2,\n  bty = \"n\"\n)\n```\n\n::: {.cell-output-display}\n![](bootstrapping_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n### Example 2 **--** Multivariate Statistic (Median)\n\nFor this example, we will generate $n = 100$ observations from a standard normal distribution, and use the quartiles as the parameters/statistics of interest. Note that the true (population) quartiles are $Q1 = qnorm(0.25) = -0.6744898$, $Q2 = qnorm(0.5) = 0$, and $Q3 = qnorm(0.75) = 0.6744898$. Since the quartiles are a multivariate statistic, the bootstrap distribution will be a matrix of dimension $R + 1 × 3$, where each column contains the bootstrap replicates of the corresponding quartile.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Multivariate Statistic (Quartiles)\n\n# generate 100 standard normal observations\nset.seed(1)\nn <- 100\nx <- rnorm(n)\n\n# nonparametric bootstrap (using ... to enter 'probs' argument)\nnpbs <- np.boot(x = x,\n                statistic = quantile,\n                probs = c(0.25, 0.5, 0.75))\nnpbs\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nNonparametric Bootstrap of Multivariate Statistic\nusing R = 9999 bootstrap replicates\n\n          25%    50%     75%\n  t0: -0.4942 0.1139  0.6915\n  SE:  0.1172 0.1394  0.0933\nBias:  0.0058 0.0185 -0.0170\n\n95% BCa Confidence Intervals:\n          25%     50%    75%\nlower -0.6941 -0.0811 0.5047\nupper -0.2534  0.3673 0.8811\n```\n:::\n\n```{.r .cell-code}\n# bootstrap distribution\npar(mfrow = c(1, 3))\nfor (j in 1:3) {\n  hist(\n    npbs$boot.dist[, j],\n    xlab = \"t*\",\n    ylab = \"Density\",\n    main = paste0(\"Bootstrap Distribution\", \": Q\", j)\n  )\n  abline(v = npbs$t0[j],\n         lty = 2,\n         col = \"red\")\n  legend(\n    \"topright\",\n    paste0(\"t0[\", j, \"]\"),\n    lty = 2,\n    col = \"red\",\n    bty = \"n\"\n  )\n}\n```\n\n::: {.cell-output-display}\n![](bootstrapping_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "bootstrapping_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}