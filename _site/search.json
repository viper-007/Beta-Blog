[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog is about cyber security and risk management. Blog posts will be published in a loose series.\nThe following tools were used to create this blog:\n\nQuarto (open source publishing system)\nRStudio for web authoring\nGitHub for version control\nNetlify for web hosting\n\nThe initial setup took place on June 2, 2023\n\nDisclaimer of liability\nThe author reserves the right not to be responsible for the topicality, correctness, completeness or quality of the information provided. Liability claims regarding damage caused by the use of any information provided, including any kind of information which is incomplete or incorrect, will therefore be rejected.\nParts of the pages or the complete publication including all offers and information might be extended, changed or partly or completely deleted by the author without separate announcement.\n\n\nReferrals and links\nLinks to third party websites are provided for convenience only and do not imply any responsibility for or approval of the information published on those other websites.\n\n\nCopyright Notice\nThe entire content of this website is licensed under a Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0).\n\n\nPrivacy Policy\nThis website does not use any cookies. By visiting this website, information about the access (date, time, page views) may be stored on the server. Personal data may be provided voluntarily (e.g. by comments)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Beta Blog",
    "section": "",
    "text": "Risk reporting\n\n\n\n\n\n\nstatistics\n\n\nrisk management\n\n\n\nRisk reporting communicates identification, assessment, mitigation, and monitoring of risks and opportunities to stakeholders for informed decision-making and protection the organization and its assets.\n\n\n\n\n\nFeb 16, 2024\n\n\nJohannes Buck\n\n\n\n\n\n\n\n\n\n\n\n\nReturn on Security Investment (ROSI)\n\n\n\n\n\n\nstatistics\n\n\nrisk management\n\n\n\nReturn on Security Investment (ROSI) is used to measure the effectiveness of security investments.\n\n\n\n\n\nJan 31, 2024\n\n\nJohannes Buck\n\n\n\n\n\n\n\n\n\n\n\n\nCyber security risk assessments with lognormal distributions\n\n\n\n\n\n\nstatistics\n\n\nrisk management\n\n\n\nThe lognormal distribution is often used for operational risk management. It has several desirable properties that make it well suited for modeling cyber security risks.\n\n\n\n\n\nJan 6, 2024\n\n\nJohannes Buck\n\n\n\n\n\n\n\n\n\n\n\n\nCyber security risk assessments with triangular distributions\n\n\n\n\n\n\nstatistics\n\n\nrisk management\n\n\n\nTriangular distributions and opinion pooling are a simple and straightforward approach to quantitative cyber security risk management.\n\n\n\n\n\nJan 6, 2024\n\n\nJohannes Buck\n\n\n\n\n\n\n\n\n\n\n\n\nLognormal Distribution\n\n\n\n\n\n\nstatistics\n\n\nrisk management\n\n\n\nThe lognormal distribution is a continuous probability distribution that can only take positive values. It describes the distribution of a random variable X if the random variable Y = ln(X) transformed by the logarithm is normally distributed.\n\n\n\n\n\nDec 26, 2023\n\n\nJohannes Buck\n\n\n\n\n\n\n\n\n\n\n\n\nProbability Density Function (PDF) - Cumulative Distribution function (CDF)\n\n\n\n\n\n\nstatistics\n\n\nrisk management\n\n\n\nThe probability density function (PDF) and the cumulative distribution function (CDF) are both concepts from probability theory and statistics.\n\n\n\n\n\nDec 12, 2023\n\n\nJohannes Buck\n\n\n\n\n\n\n\n\n\n\n\n\nStandard Deviation, Standard Error, and Confidence Interval\n\n\n\n\n\n\nstatistics\n\n\nrisk management\n\n\n\nWhen to use standard deviation, standard error, and confidence interval?\n\n\n\n\n\nAug 2, 2023\n\n\nJohannes Buck\n\n\n\n\n\n\n\n\n\n\n\n\nBootstrapping\n\n\n\n\n\n\nstatistics\n\n\nrisk management\n\n\n\nThe bootstrap method is a statistical resampling technique used to estimate the properties of a population by repeatedly sampling with replacement from a given data set.\n\n\n\n\n\nAug 1, 2023\n\n\nJohannes Buck\n\n\n\n\n\n\n\n\n\n\n\n\nConfidence Intervall\n\n\n\n\n\n\nstatistics\n\n\nrisk management\n\n\n\nA confidence interval is a statistical interval that contains the location of a true parameter of a population with a certain probability.\n\n\n\n\n\nJul 30, 2023\n\n\nJohannes Buck\n\n\n\n\n\n\n\n\n\n\n\n\nTriangle Distribution\n\n\n\n\n\n\nstatistics\n\n\nrisk management\n\n\n\nThe triangular distribution (also known as the Simpson distribution) is a continuous probability distribution. The graph of the probability density function looks like a triangle and gives this distribution its name.\n\n\n\n\n\nJul 18, 2023\n\n\nJohannes Buck\n\n\n\n\n\n\n\n\n\n\n\n\nMonte Carlo Simulation\n\n\n\n\n\n\nstatistics\n\n\nrisk management\n\n\n\nMonte Carlo simulation is a technique for estimating the probability distribution of a system using random sampling techniques.\n\n\n\n\n\nJun 27, 2023\n\n\nJohannes Buck\n\n\n\n\n\n\n\n\n\n\n\n\nSimulation Models\n\n\n\n\n\n\nstatistics\n\n\nrisk management\n\n\n\nAll models are wrong, but some are useful.\n\n\n\n\n\nJun 17, 2023\n\n\nJohannes Buck\n\n\n\n\n\n\n\n\n\n\n\n\nSystem Models\n\n\n\n\n\n\nstatistics\n\n\nrisk management\n\n\n\nQuantitative risk assessment relies on models. A model is an abstraction of reality used to gain clarity about a problem and its potential solutions by reducing the complexity.\n\n\n\n\n\nJun 16, 2023\n\n\nJohannes Buck\n\n\n\n\n\n\n\n\n\n\n\n\nRisk perception and assessment\n\n\n\n\n\n\ncyber security\n\n\nrisk management\n\n\n\nCognitive biases are systematic errors in the way individuals view the world based on subjective perceptions of reality.\n\n\n\n\n\nJun 8, 2023\n\n\nJohannes Buck\n\n\n\n\n\n\n\n\n\n\n\n\nRisk Matrices – Why they don’t work\n\n\n\n\n\n\ncyber security\n\n\nrisk management\n\n\n\nDo we use risk matrices even though we know the limitations and shortcomings of this popular risk assessment tool?\n\n\n\n\n\nJun 4, 2023\n\n\nJohannes Buck\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJun 2, 2023\n\n\nJohannes Buck\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/risk matrices/index.html",
    "href": "posts/risk matrices/index.html",
    "title": "Risk Matrices – Why they don’t work",
    "section": "",
    "text": "This post provides some insights that can help you use risk matrices more effectively. It adapted from an article I published on my website, B Advisory, on April 1, 2019.\n\nRisk matrices\nRisk matrices are used in risk assessments to define the level of risk by considering the likelihood of occurrence and impact. It is a simple mechanism for visualizing risk and supporting management decision making. Risk matrices are mentioned in several standards such as ISO and NIST. However, risk matrices have several shortcomings.\n\n\nRisk basics\nRisk is expressed as a combination of the likelihood of an event occurring and the impact on the business expressed in the equation:\n\n\n\nFigure 1: Risk Equation\n\n\n\n\nSelective representation of individual risks\nThe risk matrix is a presentation of individual risks, often arbitrarily selected. Dependencies, interactions and dynamics between individual risks are ignored.\nRisk matrices, by their nature, are not suitable for risk aggregation or risk portfolio presentation. Risks cannot simply be added up. Risk aggregation requires risk quantification based on probability distributions or stochastic processes. Monte Carlo simulation is one method used for risk aggregation.\n\n\n\n\n\nFigure 2: Aggregation scheme for a distribution-based determination of the overall risk\n\n\nPoor resolution\nA 5x5 risk matrix is often used for risk assessment.\n\n\n\n\n\nFigure 3: classic 5x5 risk matrix with linear scales*\nMost security risks do not have a linear risk distribution where five categories (rare, unlikely, possible, likely, certain) or (negligible, minor, moderate, critical, catastrophic) would be appropriate. A logarithmic representation for both likelihood and impact provides a much better resolution with a wider spread. A risk matrix with a double-logarithmic scale allows to illustrate even so-called tail risks with extremely low probability and very high impact to be represented.\n\n\n\n\n\nFigure 4: risk matrix with logarithmic scales\n\n\nAmbiguous inputs and outputs\nInputs to risk matrices (likelihood and impact) are often subjective and biased. Different assessors may arrive at opposite ratings for the same risks. Sometimes the likelihood ratings are underscored with frequencies for the occurrence of an event, e.g., three times a year, and the impact categories are defined by financial losses, e.g., $ 500,000, to guide risk assessors and avoid subjectivity and bias 1). However, the actual ratings are still based on the judgment of individuals and are not supported or derived from reliable data. In addition, most risk assessors rely on their experience based on past events and incidents. In fact, risk assessors should be trying to anticipate the likelihood and impact of future events, which is obviously a challenge.\n1) The National Institute of Standards & Technology (NIST) standard SP 800-30r1 “Guide for Conducting Risk Assessments” provides a starting point for rating scales in Appendix G, which should be tailored to fit any organization-specific conditions. For both likelihood and impact, the qualitative values of very low, low, moderate, high, and high are converted to semi-quantitative values of 0-4, 5-20, 21-79, 80-95, and 96-100, respectively.\n\n\nMisleading risk representation\nA correct representation of the semi-quantitative categories defined by NIST SP 800-30r1 results in a completely different risk matrix than the common 5x5 risk matrix.\n\n\n\n\n\nFigure 5: classic 5x5 risk matrix with equal squares\n\n\n\n\n\nFigure 6: 5x5 matrix with rectangles considering the unequal dimensions of the semi-quantitative categories according to NIST SP 800-30r1\nA classic risk matrix with equal squares means that risks with the same risk rating (Risk = Likelihood x Impact) lie on a hyperbola. A circumstance most people are hardly aware of.\n\n\n\n\n\nFigure 7: risk matrix with linear scales and risk isolines1)\nIn comparison a risk matrix with double-logarithmic scale, where the risk isolines are straight lines.\n\n\n\n\n\nFigure 8: risk matrix with logarithmic scale and risk isolines\n1) Isoline: A line of constant value on a map or chart.\n\n\nFalse assumption\nRisk matrices imply that the effort to reduce risk is the same for all risks in the same field of the risk matrix. This is incorrect for obvious reasons. Risk mitigation costs vary widely. Cost/benefit is a critical factor in risk treatment, and management must decide how best to spend limited resources to reduce overall risk.\n\n\nRisk appetite\nRisk appetite is the level of risk that an organization is willing and able to accept. The challenge with risk appetite is how to implement and enforce it and make it relevant to the business on a day-to-day basis, i.e. linking risk appetite to risk acceptance decisions.\nIt is common to visualize risk appetite using a staircase diagram. The picture suggests that all risks from the red, yellow and amber areas should be reduced, i.e. moved to the green area below the risk appetite line.\n\n\n\n\n\nFigure 9: 5x5 matrix with risk appetite line\nRisk acceptance should be a decision-making process at management or board level, not an automatic process triggered by a threshold. Risk acceptance should be a conscious decision based on risk/return, strategic objectives and risk capacity, not based on a baseline approach.\n\n\nBottom line\nIt is worth questioning common and widely used concepts such as the risk matrix. The risk matrix is a good example of how complex issues cannot be simplified as much as one would like.\n\nSimple scoring methods do not alleviate the fundamental problem of limited information.\nSimple risk presentation is in no way appropriate, although risk matrices work perfectly in a PowerPoint business culture.\n\nA double-logarithmic risk matrix is probably not appropriate for most audiences, but perhaps a risk matrix with rectangles is, given the uneven dimensions of the semi-quantitative categories. However, risk matrices should be used with caution, with careful interpretation, explanation, and sound background knowledge."
  },
  {
    "objectID": "posts/risk perception and assessment/risk perception and assessment.html",
    "href": "posts/risk perception and assessment/risk perception and assessment.html",
    "title": "Risk perception and assessment",
    "section": "",
    "text": "The following list includes some cognitive biases that can negatively affect risk perception and assessment:\n1. Rare events are underestimated and frequent events are overestimated.\n2. The greater the potential for harm, the greater the perceived risk.\n3. Risks to which one is exposed on a daily basis or risks that one believes one knows well are perceived as less threatening.\n4. Risks that have recently occurred are perceived as threatening.\n5. If a risk is taken voluntarily, it is perceived as less threatening.\n6. If the damage is reversible, it is perceived as less dangerous.\n7. If you are personally affected, a risk is perceived as more dangerous.\n8. If a benefit is not apparent, a risk is perceived as threatening.\nIn addition, risk perception and assessment are distorted by unrealistic optimism (“Nothing will happen to me!”) and illusions of control (“I can handle it!”).\nApart from the inherent shortcomings and weaknesses of qualitative risk management, cognitive biases are difficult to overcome and have a major impact on the results of qualitative risk assessments."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is my first post in the β Blog. Welcome!\n\nPicture from Rudy and Peter Skitterian on Pixaby\nProfessionally, I work in cyber security and risk management. As far as my time allows, I will publish blogs on these topics. The blogs will be based on my many years of experience and will focus on what has worked in practice. There will also be blogs on more theoretical topics that are of particular interest to me.\nThe blogs reflect my personal view and do not claim to be general. Different views are possible on any topic. I consider critical discussion to be especially important and valuable. The blog is published under the CC BY-SA 4.0 license. Knowledge should be shared freely and without restrictions."
  },
  {
    "objectID": "posts/system models.html",
    "href": "posts/system models.html",
    "title": "System Models",
    "section": "",
    "text": "Quantitative risk assessment relies on models. A model is an abstraction of reality used to gain clarity about a problem and its potential solutions by reducing the complexity.\nSystem models can be classified in relation to their randomness (deterministic or stochastic), time-dependence (static or dynamic), and number types (continuous or discrete).\nFigure 1: Taxonomy of model types"
  },
  {
    "objectID": "posts/system models.html#deterministic-models",
    "href": "posts/system models.html#deterministic-models",
    "title": "System Models",
    "section": "Deterministic models",
    "text": "Deterministic models\nDeterministic models are based on the assumption that the future can be predicted with certainty, and they use a fixed set of inputs to produce a single, deterministic output. These models are often simpler and easier to understand than stochastic models, and they can be useful in situations where the input data is well-defined and reliable. However, deterministic models may not capture the full range of potential outcomes and can be overly optimistic about risk. Therefore, deterministic models are suitable for operational risk management.\n\nFigure 2: Deterministic Model (reproduced after [1])"
  },
  {
    "objectID": "posts/system models.html#stochastic-models",
    "href": "posts/system models.html#stochastic-models",
    "title": "System Models",
    "section": "Stochastic models",
    "text": "Stochastic models\nStochastic models incorporate randomness and uncertainty into their calculations, allowing them to generate a range of possible outcomes and associated probabilities. This can be particularly useful in situations where the input data is incomplete or subject to fluctuations over time. Stochastic models are more complex than deterministic models, but they are often better suited to capturing the full range of potential risks and their likelihoods. They are also more adaptable to changing circumstances and can provide more robust risk assessments.\n\nFigure 3: Stochastic Model (reproduced after [2])"
  },
  {
    "objectID": "posts/system models.html#static-models",
    "href": "posts/system models.html#static-models",
    "title": "System Models",
    "section": "Static models",
    "text": "Static models\nStatic models are typically used to represent operational risks that do not change significantly over time, such as risks associated with a particular process, product, or service. For example, a static model could be used to identify and assess the operational risks associated with a specific product line or business unit. Static models can be useful for providing a high-level overview of operational risks and identifying areas for improvement."
  },
  {
    "objectID": "posts/system models.html#dynamic-models",
    "href": "posts/system models.html#dynamic-models",
    "title": "System Models",
    "section": "Dynamic models",
    "text": "Dynamic models\nDynamic models take into account the changing nature of operational risks over time, allowing for the modeling of complex interactions and feedback loops. These models can be useful in situations where the risk environment is constantly evolving or where there are multiple sources of risk that can interact with one another.\n\nFigure 4: Static vs. dynamic models"
  },
  {
    "objectID": "posts/system models.html#discrete-models",
    "href": "posts/system models.html#discrete-models",
    "title": "System Models",
    "section": "Discrete models",
    "text": "Discrete models\nDiscrete models use variables that change only at discrete set of points in time. Most operational risks are based on scenarios with a continuous range of values. However, discrete models can be used to analyze a specific events or scenarios that may lead to operational failures or losses. For example, a discrete model could be used to identify the specific events that could lead to a cyber attack or a supply chain disruption"
  },
  {
    "objectID": "posts/system models.html#continuous-models",
    "href": "posts/system models.html#continuous-models",
    "title": "System Models",
    "section": "Continuous models",
    "text": "Continuous models\nContinuous models are designed to model risks that occur over a continuous range of values, such as financial losses or process failures. These models can be useful for capturing the full distribution of possible outcomes and assessing the potential impact of different levels of risk exposure.\n\nFigure 5: Discrete vs. continuous models (reproduced after [3])"
  },
  {
    "objectID": "posts/system models.html#models-for-operational-risks",
    "href": "posts/system models.html#models-for-operational-risks",
    "title": "System Models",
    "section": "Models for operational risks",
    "text": "Models for operational risks\nStochastic, dynamic, continuous models are suitable for operational risk management because they are designed to capture the inherent complexity and variability of operational risks.\nDeterministic, static. discrete models are not appropriate for operational risk management because operational risks change over time (they are dynamic, not static) and operational risk parameters have continuous values.\nReferences\n[1] Kelton, W.D.; Law, A.M. 2000. Simulation Modeling and Analysis; Boston, MA, USA: McGraw Hill.\n[2] Platon, V.; Constantinescu, A. Monte Carlo Method in Risk Analysis for Investment Projects; Procedia Economics and Finance 15 (2014) 393 – 400. https://doi.org/10.1016/S2212-5671(14)00463-8.\n[3] R. Holzer, P. Wuchner, and H. de Meer, “Modeling of Self-Organizing Systems: An Overview” Electronic Communications of the EASST vol. 27 (2010). http://dx.doi.org/10.14279/tuj.eceasst.27.385."
  },
  {
    "objectID": "posts/system models.html#footnotes",
    "href": "posts/system models.html#footnotes",
    "title": "System Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMarkdown is a simple language.↩︎"
  },
  {
    "objectID": "posts/system models/system models.html",
    "href": "posts/system models/system models.html",
    "title": "System Models",
    "section": "",
    "text": "System models can be classified in relation to their randomness (deterministic or stochastic), time-dependence (static or dynamic), and number types (continuous or discrete).\nFigure 1: Taxonomy of model types"
  },
  {
    "objectID": "posts/system models/system models.html#deterministic-models",
    "href": "posts/system models/system models.html#deterministic-models",
    "title": "System Models",
    "section": "Deterministic models",
    "text": "Deterministic models\nDeterministic models are based on the assumption that the future can be predicted with certainty, and they use a fixed set of inputs to produce a single, deterministic output. These models are often simpler and easier to understand than stochastic models, and they can be useful in situations where the input data is well-defined and reliable. However, deterministic models may not capture the full range of potential outcomes and can be overly optimistic about risk. Therefore, deterministic models are suitable for operational risk management.\n\nFigure 2: Deterministic Model (reproduced after [1])"
  },
  {
    "objectID": "posts/system models/system models.html#stochastic-models",
    "href": "posts/system models/system models.html#stochastic-models",
    "title": "System Models",
    "section": "Stochastic models",
    "text": "Stochastic models\nStochastic models incorporate randomness and uncertainty into their calculations, allowing them to generate a range of possible outcomes and associated probabilities. This can be particularly useful in situations where the input data is incomplete or subject to fluctuations over time. Stochastic models are more complex than deterministic models, but they are often better suited to capturing the full range of potential risks and their likelihoods. They are also more adaptable to changing circumstances and can provide more robust risk assessments.\n\nFigure 3: Stochastic Model (reproduced after [2])"
  },
  {
    "objectID": "posts/system models/system models.html#static-models",
    "href": "posts/system models/system models.html#static-models",
    "title": "System Models",
    "section": "Static models",
    "text": "Static models\nStatic models are typically used to represent operational risks that do not change significantly over time, such as risks associated with a particular process, product, or service. For example, a static model could be used to identify and assess the operational risks associated with a specific product line or business unit. Static models can be useful for providing a high-level overview of operational risks and identifying areas for improvement."
  },
  {
    "objectID": "posts/system models/system models.html#dynamic-models",
    "href": "posts/system models/system models.html#dynamic-models",
    "title": "System Models",
    "section": "Dynamic models",
    "text": "Dynamic models\nDynamic models take into account the changing nature of operational risks over time, allowing for the modeling of complex interactions and feedback loops. These models can be useful in situations where the risk environment is constantly evolving or where there are multiple sources of risk that can interact with one another.\n\nFigure 4: Static vs. dynamic models"
  },
  {
    "objectID": "posts/system models/system models.html#discrete-models",
    "href": "posts/system models/system models.html#discrete-models",
    "title": "System Models",
    "section": "Discrete models",
    "text": "Discrete models\nDiscrete models use variables that change only at discrete set of points in time. Most operational risks are based on scenarios with a continuous range of values. However, discrete models can be used to analyze a specific events or scenarios that may lead to operational failures or losses. For example, a discrete model could be used to identify the specific events that could lead to a cyber attack or a supply chain disruption"
  },
  {
    "objectID": "posts/system models/system models.html#continuous-models",
    "href": "posts/system models/system models.html#continuous-models",
    "title": "System Models",
    "section": "Continuous models",
    "text": "Continuous models\nContinuous models are designed to model risks that occur over a continuous range of values, such as financial losses or process failures. These models can be useful for capturing the full distribution of possible outcomes and assessing the potential impact of different levels of risk exposure.\n\nFigure 5: Discrete vs. continuous models (reproduced after [3])"
  },
  {
    "objectID": "posts/system models/system models.html#models-for-operational-risks",
    "href": "posts/system models/system models.html#models-for-operational-risks",
    "title": "System Models",
    "section": "Models for operational risks",
    "text": "Models for operational risks\nStochastic, dynamic, continuous models are suitable for operational risk management because they are designed to capture the inherent complexity and variability of operational risks.\nDeterministic, static. discrete models are not appropriate for operational risk management because operational risks change over time (they are dynamic, not static) and operational risk parameters have continuous values.\nReferences\n[1] Kelton, W.D.; Law, A.M. 2000. Simulation Modeling and Analysis; Boston, MA, USA: McGraw Hill.\n[2] Platon, V.; Constantinescu, A. Monte Carlo Method in Risk Analysis for Investment Projects; Procedia Economics and Finance 15 (2014) 393 – 400. https://doi.org/10.1016/S2212-5671(14)00463-8.\n[3] R. Holzer, P. Wuchner, and H. de Meer, “Modeling of Self-Organizing Systems: An Overview” Electronic Communications of the EASST vol. 27 (2010). http://dx.doi.org/10.14279/tuj.eceasst.27.385."
  },
  {
    "objectID": "posts/simulation models/simulation models.html",
    "href": "posts/simulation models/simulation models.html",
    "title": "Simulation Models",
    "section": "",
    "text": "The statement “All models are wrong, but some are useful.” is usually attributed to the statistician George Box [1].\nA model is an approximation of reality. Reality is more complex than any model."
  },
  {
    "objectID": "posts/simulation models/simulation models.html#deterministic-simulation-model",
    "href": "posts/simulation models/simulation models.html#deterministic-simulation-model",
    "title": "Simulation Models",
    "section": "Deterministic simulation model",
    "text": "Deterministic simulation model\nA deterministic simulation model does not contain any random variables. Deterministic simulation models have a known set of inputs that result in a unique set of outputs.\nExamples:\n\nPatients arriving at the doctor’s office at the scheduled time.\nA production line with fixed processing times for each part\nTrain, bus, or plane schedules\nPricing structures for products or services\nLinear programming models for finding the best solution to a problem with linear constraints and a linear objective function (e.g., optimal delivery route)\n\nNote: The examples show that deterministic simulation models simplify reality (patients are delayed, production processes fail due to technical problems)."
  },
  {
    "objectID": "posts/simulation models/simulation models.html#stochastic-simulation-model",
    "href": "posts/simulation models/simulation models.html#stochastic-simulation-model",
    "title": "Simulation Models",
    "section": "Stochastic simulation model",
    "text": "Stochastic simulation model\nA stochastic simulation model has one or more random variables as inputs. Random inputs result in random outputs. Probability distributions such as Bernoulli distribution, Binominal distribution, Poisson distribution, etc. are used for stochastic simulation models.\nExamples:\n\nTurbulent flow over an airplane wing\nDynamic pricing\nWeather forecasting\nEarth climatology\nFinancial markets\nPopulation dynamics"
  },
  {
    "objectID": "posts/simulation models/simulation models.html#static-simulation-model",
    "href": "posts/simulation models/simulation models.html#static-simulation-model",
    "title": "Simulation Models",
    "section": "Static simulation model",
    "text": "Static simulation model\nA static simulation model or steady-state model is time-invariant, i.e. inputs and parameters do not change over time.\nStatic simulation models are (steatdy-state) memoryless systems because the output does not depend on past input signals \\(x(n-1)\\). It depends only on present input signals \\(x(n)\\).\nMathematical example:\n\\(y(n) = 6 \\cdot x(n)\\)\n\\(6\\) is a constant that multiplies the input \\(x(n)\\). The output \\(y(n)\\) is simultaneously dependent on the input \\(x(n)\\).\nPhysical example:\nThe load on a bridge. While this static model does not account for the effects of moving traffic, wind, earthquakes, or other factors that may change over time, it provides a useful first-order analysis of whether the bridge can support the weight it is designed to carry.\nNote: Static models are often used as an approximation of dynamic models to simplify the model and to reduce the complexity and the computational effort for the simulation."
  },
  {
    "objectID": "posts/simulation models/simulation models.html#dynamic-simulation-model",
    "href": "posts/simulation models/simulation models.html#dynamic-simulation-model",
    "title": "Simulation Models",
    "section": "Dynamic simulation model",
    "text": "Dynamic simulation model\nA dynamic simulation model is time-dependent, i.e. the state of the system changes over time. Dynamic simulation models require memory to store time-based values. Dynamic simulation models are used to evaluate scenarios that change over time or to analyze trends. Dynamic models are often described by differential equations where the state variables change continuously.\nMathematical example:\n\\(y(n) = x(n) + 6x \\cdot (n-2)\\)\nIf \\(x(n)\\) is the current input signal, then\n\n\\(x(n-t\\)) is the past signal\n\\(x(n+t)\\) is the future signal\n\nPhysical example:\nA dynamic simulation model of a bridge taking into account the effects of loads caused by moving traffic, wind, and earthquakes."
  },
  {
    "objectID": "posts/simulation models/simulation models.html#discrete-simulation-model",
    "href": "posts/simulation models/simulation models.html#discrete-simulation-model",
    "title": "Simulation Models",
    "section": "Discrete simulation model",
    "text": "Discrete simulation model\nA discrete model is one in which the state variables change only at a discrete set of times. For example, a store where the number of customers (state variable) changes only when a customer arrives or leaves.\nExample: Lifeboats and life jackets [2]\nA shipyard is designing a new ferry boat and wants to determine the number of lifeboats and life jackets it should have.\n\nEach life jacket holds 1 person and requires \\(0.05^\\,m3\\) of storage space.\nEach lifeboat holds 20 people and requires \\(2.1^\\,m3\\) of storage space.\nWe need to have capacity for 1,000 people in one or the other. - The total space to be devoted to this equipment is at most \\(85\\,m^3\\).\n\nHow many of each should be installed?\nYou could go through the following steps to determine a solution:\n\nFirst, see if it’s possible to put everyone in lifeboats, which would be best. This would require \\(\\frac{1000}{20} \\cdot 2.1 = 105\\,m^3\\) of space, so the answer is no, we must use some combination.\nMake sure there’s enough space for life jackets for everyone, otherwise the problem is impossible: \\(1000 \\cdot .05 = 50\\,m^3\\), so it would be possible to provide only life jackets and have space left over. But for the best solution, we want to use more of the available space and provide as many boats as possible.\nFigure out how many boats and life jackets will be used if we use exactly all the space available and provide exactly the capacity we need. We can frame this as a mathematical problem. There are two unknowns\n\n\n\\(x_1\\) = number of jackets\n\\(x_2\\) = number of boats\n\nand two equations to satisfy, one saying that the total capacity must be 1,000 people and the other saying that the total volume must be \\(85\\,m^3\\) . This is a linear system of equations:\n\\[\\begin{equation} \\tag{1.1}\n     \\displaylines{C_1x_1 + C_2x_2 = C\\\\V_1x_1 + V_2x_2 = V}\n\\end{equation}\\]\nwhere\n\n\\(C_1\\) = 1 (capacity of each jacket)\n\\(C_2\\) = 20 (capacity of each boat)\n\\(C\\) = 1000 (total capacity needed)\n\\(V_1\\) = 0.05 (volume of each jacket)\n\\(V_2\\) = 2.1 (volume of each boat)\n\\(V\\) = 85 (total available volume)\n\n\nSolve the mathematical problem. Equation (1.1) is just a 2 × 2 linear system, so it is easy to solve, obtaining\n\n\n\\(x_1\\) = 363.6364 (number of jackets)\n\\(x_2\\) = 31.8182 (number of boats)\n\nInterpreting the mathematical solution We cannot actually provide fractional jackets or boats, so we have to fix up these numbers. The only way to do this and keep the volume from increasing is to reduce the number of boats to 31 and increase the number of jackets to keep the capacity at 1,000. Since 31 × 20 = 620 this means we need 380 jackets. The total volume required is \\(84.1\\,m^3\\) .\nNote that the lifeboats are placed on symmetrical racks on both sides of the boat, so there must be an even number of lifeboats. Using this new information, revise the solution. There must be 30 boats and 400 life jackets.\nReconsider the solution to see if we have missed anything.\nNotice that the original mathematical solution had \\(x_2\\) almost equal to 32. This suggests that if we had just a little more space (\\(85.2\\,m^3\\) ), we could increase the number of boats to 32 instead of 30, a significant improvement. The shipyard could find another \\(0.2\\,m^3\\), which would allow two more boats to fit."
  },
  {
    "objectID": "posts/simulation models/simulation models.html#continuous-simulation-model",
    "href": "posts/simulation models/simulation models.html#continuous-simulation-model",
    "title": "Simulation Models",
    "section": "Continuous simulation model",
    "text": "Continuous simulation model\nA continuous model is one in which the state variables change continuously over time.\nContinuous, static model\nExample: Voltage drop across a resistor\n\\(V = I \\cdot R\\)\nThe voltage drop depends on the value of the current at that moment. Therefore, it is a static system. A continuous model is static (stationary or memoryless) if its output depends only on the current input.\n\nFigure 1: Electrical circuit no. 1\n\nFigure 2: Voltage curve as function of current and resistor\nContinuous, dynamic model\nExample: Charging of a capacitor\nA series circuit consisting of a resistor, a capacitor, a switch, and a constant DC voltage source $V_0$ is called a charging circuit.\nIf the capacitor is initially uncharged while the switch is open, and the switch is closed at $t=0$, Kirchhoff’s law of voltage states that:\n\\(V_S - R \\cdot i(t) - V_C(t) = 0\\)\n\nFigure 3: Electric circuit no 2\n\nFigure 4: Voltage curve of capacitor charging\nReferences:\n[1] George E. P. Box. “Science and Statistics.” Journal of the American Statistical Association 71, no. 356 (1976): 791–99.https://doi.org/10.2307/2286841.\n[2] Billey, S., Burke, J., Chartier, T., Greenbaum, A., & LeVequw R. (2010). Discrete Mathematical Modeling: Math 381 Course Notes. University of Washington. 381no&lt;tes_new.pdf (washington.edu)."
  },
  {
    "objectID": "posts/monte carlo simulation/monte carlo simulation.html",
    "href": "posts/monte carlo simulation/monte carlo simulation.html",
    "title": "Monte Carlo Simulation",
    "section": "",
    "text": "In Monte Carlo simulation, the behavior of a system is simulated using random numbers and probability distributions to replicate the uncertainties and variability that exist in the real world. It is particularly useful when the system or process being analyzed is too complex to solve analytically or when the underlying mathematical model is not well defined.\nMonte Carlo simulation is a stochastic method, which means that results can vary from simulation to simulation due to the random sampling involved. However, by using a large number of iterations, the simulation results converge to more accurate estimates.\nFigure 1: Schematic of Monte Carlo simulation"
  },
  {
    "objectID": "posts/monte carlo simulation/monte carlo simulation.html#process",
    "href": "posts/monte carlo simulation/monte carlo simulation.html#process",
    "title": "Monte Carlo Simulation",
    "section": "Process",
    "text": "Process\nThe process steps involved in performing a Monte Carlo simulation are as follows:\n\nDefine the problem: identify the system to be analyzed.\nIdentify the parameters: identify the parameters of the system and their probability distributions. This includes specifying the ranges or values that each parameter can take, as well as their respective probabilities.\nGenerate random samples: generate a set of random numbers that correspond to the probability distributions of the parameters.\nSimulate the system: calculate the output values based on the input parameters.\nAnalyze the results: analyze the probability distribution of the output variables.\nValidate the results: compare the results with observations or experimental data."
  },
  {
    "objectID": "posts/monte carlo simulation/monte carlo simulation.html#purpose",
    "href": "posts/monte carlo simulation/monte carlo simulation.html#purpose",
    "title": "Monte Carlo Simulation",
    "section": "Purpose",
    "text": "Purpose\nMonte Carlo simulation is a useful method for analyzing uncertain scenarios and providing probabilistic analysis of various situations. Monte Carlo simulation can be used for both static and dynamic simulations.\nIn a static simulation, the input variables are fixed and do not change over time. Monte Carlo simulation can be used to estimate the distribution of the output variable(s) under these fixed input conditions. For example, Monte Carlo simulation can be used to estimate the distribution of project costs under fixed assumptions about the duration, scope, and, resource requirements of the project.\nIn a dynamic simulation, the input variables change over time, and the simulation models the behavior of the system or process over time. Monte Carlo simulation can be used to estimate the distribution of the output variable(s) at different points in time under changing input conditions. For example, Monte Carlo simulation can be used to estimate the distribution of a portfolio’s returns over a period of time, taking into account changing market conditions and other factors that affect the portfolio’s performance.\nExample: Monte Carlo method applied to approximating the value of π\nThis is done by picking random points \\((x,y \\mid x \\in [-1..1] \\wedge y \\in [-1..1])\\) and checking (using Pythagoras’ theorem) if they are inside the unit circle:\n\\[x^2 + y^2 \\leq 1\\]\nThe ratio of the number of points inside and outside the circle can then be determined as follows:\n\\[\\frac{circle\\,area}{square\\,area} = \\frac{r^2 \\cdot \\pi}{(2 \\cdot r)^2} = \\frac {\\pi}{4} = \\frac {hits\\,in\\,circle\\,area}{generated\\,points\\,in\\,the\\,square} = P\\,{(in\\,the\\,circle)}\\]\nR code\n\nnum_darts &lt;- 1000\nnum_darts_in_circle &lt;- 0\n\nfor(i in 1:num_darts) {\n  x &lt;- runif(n = 1, min = -1, max = 1)\n  y &lt;- runif(n = 1, min = -1, max = 1)\n  if(x^2 + y^2 &lt;= 1) {\n    num_darts_in_circle = num_darts_in_circle + 1\n  }\n}\n\nprint(4 * num_darts_in_circle / num_darts)\n\n[1] 3.124\n\n\nR code for optimized runtime with vector operations\n\nlibrary(\"plotrix\")\n\nnum_darts &lt;- 1000000\nnum_darts_in_circle &lt;- 0\n\nx &lt;- runif(n = num_darts, min = -1, max = 1)\ny &lt;- runif(n = num_darts, min = -1, max = 1)\n\nsum_squares &lt;- x^2 + y^2\nindexes_darts_in_circle &lt;- which(sum_squares &lt;= 1)\nnum_darts_in_circle &lt;- length(indexes_darts_in_circle)\nprint(4 * num_darts_in_circle / num_darts)\n\n[1] 3.144388\n\n\n\n\n\n\n\nPlotting the results"
  },
  {
    "objectID": "posts/monte carlo simulation/monte carlo simulation (2).html",
    "href": "posts/monte carlo simulation/monte carlo simulation (2).html",
    "title": "Monte Carlo Simulation",
    "section": "",
    "text": "Monte Carlo simulation is a technique for estimating the probability distribution of a system using random sampling techniques.\nIn Monte Carlo simulation, the behavior of a system is simulated using random numbers and probability distributions to replicate the uncertainties and variability that exist in the real world. It is particularly useful when the system or process being analyzed is too complex to solve analytically or when the underlying mathematical model is not well defined.\nMonte Carlo simulation is a stochastic method, which means that results can vary from simulation to simulation due to the random sampling involved. However, by using a large number of iterations, the simulation results converge to more accurate estimates.\nFigure 1: Schematic of Monte Carlo simulation"
  },
  {
    "objectID": "posts/monte carlo simulation/monte carlo simulation (2).html#process",
    "href": "posts/monte carlo simulation/monte carlo simulation (2).html#process",
    "title": "Monte Carlo Simulation",
    "section": "Process",
    "text": "Process\nThe process steps involved in performing a Monte Carlo simulation are as follows:\n\nDefine the problem: identify the system to be analyzed.\nIdentify the parameters: identify the parameters of the system and their probability distributions. This includes specifying the ranges or values that each parameter can take, as well as their respective probabilities.\nGenerate random samples: generate a set of random numbers that correspond to the probability distributions of the parameters.\nSimulate the system: calculate the output values based on the input parameters.\nAnalyze the results: analyze the probability distribution of the output variables.\nValidate the results: compare the results with observations or experimental data."
  },
  {
    "objectID": "posts/monte carlo simulation/monte carlo simulation (2).html#purpose",
    "href": "posts/monte carlo simulation/monte carlo simulation (2).html#purpose",
    "title": "Monte Carlo Simulation",
    "section": "Purpose",
    "text": "Purpose\nMonte Carlo simulation is a useful method for analyzing uncertain scenarios and providing probabilistic analysis of various situations. Monte Carlo simulation can be used for both static and dynamic simulations.\nIn a static simulation, the input variables are fixed and do not change over time. Monte Carlo simulation can be used to estimate the distribution of the output variable(s) under these fixed input conditions. For example, Monte Carlo simulation can be used to estimate the distribution of project costs under fixed assumptions about the duration, scope, and, resource requirements of the project.\nIn a dynamic simulation, the input variables change over time, and the simulation models the behavior of the system or process over time. Monte Carlo simulation can be used to estimate the distribution of the output variable(s) at different points in time under changing input conditions. For example, Monte Carlo simulation can be used to estimate the distribution of a portfolio’s returns over a period of time, taking into account changing market conditions and other factors that affect the portfolio’s performance.\nExample: Monte Carlo method applied to approximating the value of π\nThis is done by picking random points \\((x,y \\mid x \\in [-1..1] \\wedge y \\in [-1..1])\\) and checking (using Pythagoras’ theorem) if they are inside the unit circle:\n\\[x^2 + y^2 \\leq 1\\]\nThe ratio of the number of points inside and outside the circle can then be determined as follows:\n\\[\\frac{circle\\,area}{square\\,area} = \\frac{r^2 \\cdot \\pi}{(2 \\cdot r)^2} = \\frac {\\pi}{4} = \\frac {hits\\,in\\,circle\\,area}{generated\\,points\\,in\\,the\\,square} = P\\,{(in\\,the\\,circle)}\\]\nR code\n\nnum_darts &lt;- 1000\nnum_darts_in_circle &lt;- 0\n\nfor(i in 1:num_darts) {\n  x &lt;- runif(n = 1, min = -1, max = 1)\n  y &lt;- runif(n = 1, min = -1, max = 1)\n  if(x^2 + y^2 &lt;= 1) {\n    num_darts_in_circle = num_darts_in_circle + 1\n  }\n}\n\nprint(4 * num_darts_in_circle / num_darts)\n\n[1] 3.048\n\n\nR code for optimized runtime with vector operations\n\nlibrary(\"plotrix\")\n\nnum_darts &lt;- 1000000\nnum_darts_in_circle &lt;- 0\n\nx &lt;- runif(n = num_darts, min = -1, max = 1)\ny &lt;- runif(n = num_darts, min = -1, max = 1)\n\nsum_squares &lt;- x^2 + y^2\nindexes_darts_in_circle &lt;- which(sum_squares &lt;= 1)\nnum_darts_in_circle &lt;- length(indexes_darts_in_circle)\nprint(4 * num_darts_in_circle / num_darts)\n\n[1] 3.141048\n\n\nR code with plotting the results\n\nlibrary(\"plotrix\")\n\nnum_darts &lt;- 100\nnum_darts_in_circle &lt;- 0\n\nx &lt;- runif(n = num_darts, min = -1, max = 1)\ny &lt;- runif(n = num_darts, min = -1, max = 1)\n\nsum_squares &lt;- x^2 + y^2\nindexes_darts_in_circle &lt;- which(sum_squares &lt;= 1)\nnum_darts_in_circle &lt;- length(indexes_darts_in_circle)\n\nplot(x[1:i], y[1:i], xlim = c(-1, 1), ylim = c(-1, 1), asp = 1)\nrect(-1, -1, 1, 1)\ndraw.circle(0, 0, 1, nv = 1000, border = \"red\", col = \"lightblue\", lty = 1, density = 10, lwd = 1)"
  },
  {
    "objectID": "posts/risk matrices/risk matrices.html",
    "href": "posts/risk matrices/risk matrices.html",
    "title": "Risk Matrices – Why they don’t work",
    "section": "",
    "text": "This post provides some insights that can help you use risk matrices more effectively. It adapted from an article I published on my website, B Advisory, on April 1, 2019.\n\nRisk matrices\nRisk matrices are used in risk assessments to define the level of risk by considering the likelihood of occurrence and impact. It is a simple mechanism for visualizing risk and supporting management decision making. Risk matrices are mentioned in several standards such as ISO and NIST. However, risk matrices have several shortcomings.\n\n\nRisk basics\nRisk is expressed as a combination of the likelihood of an event occurring and the impact on the business expressed in the equation:\n\n\n\nFigure 1: Risk Equation\n\n\n\n\nSelective representation of individual risks\nThe risk matrix is a presentation of individual risks, often arbitrarily selected. Dependencies, interactions and dynamics between individual risks are ignored.\nRisk matrices, by their nature, are not suitable for risk aggregation or risk portfolio presentation. Risks cannot simply be added up. Risk aggregation requires risk quantification based on probability distributions or stochastic processes. Monte Carlo simulation is one method used for risk aggregation.\n\n\n\n\n\nFigure 2: Aggregation scheme for a distribution-based determination of the overall risk\n\n\nPoor resolution\nA 5x5 risk matrix is often used for risk assessment.\n\n\n\n\n\nFigure 3: classic 5x5 risk matrix with linear scales*\nMost security risks do not have a linear risk distribution where five categories (rare, unlikely, possible, likely, certain) or (negligible, minor, moderate, critical, catastrophic) would be appropriate. A logarithmic representation for both likelihood and impact provides a much better resolution with a wider spread. A risk matrix with a double-logarithmic scale allows to illustrate even so-called tail risks with extremely low probability and very high impact to be represented.\n\n\n\n\n\nFigure 4: risk matrix with logarithmic scales\n\n\nAmbiguous inputs and outputs\nInputs to risk matrices (likelihood and impact) are often subjective and biased. Different assessors may arrive at opposite ratings for the same risks. Sometimes the likelihood ratings are underscored with frequencies for the occurrence of an event, e.g., three times a year, and the impact categories are defined by financial losses, e.g., $ 500,000, to guide risk assessors and avoid subjectivity and bias 1). However, the actual ratings are still based on the judgment of individuals and are not supported or derived from reliable data. In addition, most risk assessors rely on their experience based on past events and incidents. In fact, risk assessors should be trying to anticipate the likelihood and impact of future events, which is obviously a challenge.\n1) The National Institute of Standards & Technology (NIST) standard SP 800-30r1 “Guide for Conducting Risk Assessments” provides a starting point for rating scales in Appendix G, which should be tailored to fit any organization-specific conditions. For both likelihood and impact, the qualitative values of very low, low, moderate, high, and high are converted to semi-quantitative values of 0-4, 5-20, 21-79, 80-95, and 96-100, respectively.\n\n\nMisleading risk representation\nA correct representation of the semi-quantitative categories defined by NIST SP 800-30r1 results in a completely different risk matrix than the common 5x5 risk matrix.\n\n\n\n\n\nFigure 5: classic 5x5 risk matrix with equal squares\n\n\n\n\n\nFigure 6: 5x5 matrix with rectangles considering the unequal dimensions of the semi-quantitative categories according to NIST SP 800-30r1\nA classic risk matrix with equal squares means that risks with the same risk rating (Risk = Likelihood x Impact) lie on a hyperbola. A circumstance most people are hardly aware of.\n\n\n\n\n\nFigure 7: risk matrix with linear scales and risk isolines1)\nIn comparison a risk matrix with double-logarithmic scale, where the risk isolines are straight lines.\n\n\n\n\n\nFigure 8: risk matrix with logarithmic scale and risk isolines\n1) Isoline: A line of constant value on a map or chart.\n\n\nFalse assumption\nRisk matrices imply that the effort to reduce risk is the same for all risks in the same field of the risk matrix. This is incorrect for obvious reasons. Risk mitigation costs vary widely. Cost/benefit is a critical factor in risk treatment, and management must decide how best to spend limited resources to reduce overall risk.\n\n\nRisk appetite\nRisk appetite is the level of risk that an organization is willing and able to accept. The challenge with risk appetite is how to implement and enforce it and make it relevant to the business on a day-to-day basis, i.e. linking risk appetite to risk acceptance decisions.\nIt is common to visualize risk appetite using a staircase diagram. The picture suggests that all risks from the red, yellow and amber areas should be reduced, i.e. moved to the green area below the risk appetite line.\n\n\n\n\n\nFigure 9: 5x5 matrix with risk appetite line\nRisk acceptance should be a decision-making process at management or board level, not an automatic process triggered by a threshold. Risk acceptance should be a conscious decision based on risk/return, strategic objectives and risk capacity, not based on a baseline approach.\n\n\nBottom line\nIt is worth questioning common and widely used concepts such as the risk matrix. The risk matrix is a good example of how complex issues cannot be simplified as much as one would like.\n\nSimple scoring methods do not alleviate the fundamental problem of limited information.\nSimple risk presentation is in no way appropriate, although risk matrices work perfectly in a PowerPoint business culture.\n\nA double-logarithmic risk matrix is probably not appropriate for most audiences, but perhaps a risk matrix with rectangles is, given the uneven dimensions of the semi-quantitative categories. However, risk matrices should be used with caution, with careful interpretation, explanation, and sound background knowledge."
  },
  {
    "objectID": "posts/triangle distribution/triangle distribution.html",
    "href": "posts/triangle distribution/triangle distribution.html",
    "title": "Triangle Distribution",
    "section": "",
    "text": "The triangular distribution (also known as the Simpson distribution) is a continuous probability distribution. The graph of the probability density function looks like a triangle and gives this distribution its name.\nThe probability density function moves between a minimum \\(a\\), a maximum \\(b\\) and the mode \\(c\\) (the value with the highest probability). The \\(y\\) axis shows the density of the respective probability for a value \\(x \\in [a, b]\\).\nFor a triangular distribution, the maximum of the probability density is at the value of the mode. For the lower and upper limit, the value of the probability density is zero.\nWhen very little concrete data is available to determine the distribution function \\(f(x)\\) of a random variable \\(x\\), the triangular distribution is often used in practice. The triangular distribution is also a popular distribution in Monte Carlo simulation.\nTriangular distributions can take the following forms: symmetric distribution, right-skewed distribution, and left-skewed distribution."
  },
  {
    "objectID": "posts/triangle distribution/triangle distribution.html#probability-density-function-pdf",
    "href": "posts/triangle distribution/triangle distribution.html#probability-density-function-pdf",
    "title": "Triangle Distribution",
    "section": "Probability Density Function (PDF)",
    "text": "Probability Density Function (PDF)\nThe probability density function of the triangular distribution in the interval \\(x \\in [a, b]\\) is defined by\n\\[\\begin{equation} \\tag{1.1}\n    f(x) = \\begin{cases}\n        \\dfrac{2(x-a)}{(b-a)(c-a)} & \\text{for } x \\in [a, c] \\\\\n        \\dfrac{2(b-x)}{(b-a)(b-c)} & \\text{for } x \\in (c, b] \\\\\n        {0} & \\text{else}\n    \\end{cases}\n\\end{equation}\\]\n\n\n\n\n\n\nPDF (probability density function)\n\n\n\nThe PDF (probability density function) in the triangular distribution shows how the probability is distributed across different values within a given interval. The triangular distribution is a continuous probability distribution in which the probability first increases linearly, then peaks, and then decreases linearly. The shape of the triangular distribution is determined by three parameters: the left tail value \\(a\\), the right tail value \\(b\\), and the mode \\(c\\) (the value with the highest probability)."
  },
  {
    "objectID": "posts/triangle distribution/triangle distribution.html#title",
    "href": "posts/triangle distribution/triangle distribution.html#title",
    "title": "Triangle Distribution",
    "section": "Title",
    "text": "Title\n…\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "posts/triangle distribution/triangle distribution.html#cumulative-distribution-function-cdf",
    "href": "posts/triangle distribution/triangle distribution.html#cumulative-distribution-function-cdf",
    "title": "Triangle Distribution",
    "section": "Cumulative Distribution Function (CDF)",
    "text": "Cumulative Distribution Function (CDF)\nBy integration, the cumulative distribution function \\(F(x)\\) from equation (1.1) becomes\n\\[\\begin{equation} \\tag{1.2}\n    F(x) = \\begin{cases}\n        \\dfrac{(x-a)^2}{(b-a)(c-a)} & \\text{for } x \\in [a, c] \\\\\n        1-\\dfrac{(b-x)^2}{(b-a)(b-c)} & \\text{for } x \\in (c, b] \\\\\n        {0} & \\text{else}\n    \\end{cases}\n\\end{equation}\\]\n\n\n\n\n\n\nNote\n\n\n\nThe cumulative distribution function (CDF) of a probability distribution indicates the probability that a random variable will take a value less than or equal to a given value. The CDF of a triangular distribution increases slowly at first, peaks at \\(c\\), and then decreases until it reaches the value 1 when the random variable reaches the maximum value \\(b\\). The meaning of the CDF is that it indicates the cumulative probability that the random variable will take \\(a\\) value less than or equal to a certain value."
  },
  {
    "objectID": "posts/triangle distribution/triangle distribution.html#mode",
    "href": "posts/triangle distribution/triangle distribution.html#mode",
    "title": "Triangle Distribution",
    "section": "Mode",
    "text": "Mode\nThe mode is the value that occurs most frequently. The mode of a triangular distribution is the value of \\(x\\) at which the probability density function (PDF) of the reaches its maximum value.\nAt the point \\(c\\) in \\([a,b]\\) (mode), the probability distribution \\(f(x)\\) has its maximum, which is given by \\[ \\begin{equation} \\tag{1.3}\n    F(\\infty) = \\int_{- \\infty}^\\infty {f(x)}{dx} = \\frac{b-a}{2} \\cdot f(c) = 1\n\\end{equation} \\]\nresults to \\[ \\begin{equation} \\tag{1.4}\n    f(c) = \\frac{2}{b-a}\n\\end{equation} \\]\nThe mean and median of a triangular distribution are not necessarily the same as the mode. The mean and median depend on the specific parameters of the distribution, while the mode depends only on the range \\([a, b]\\)."
  },
  {
    "objectID": "posts/triangle distribution/triangle distribution.html#expected-value",
    "href": "posts/triangle distribution/triangle distribution.html#expected-value",
    "title": "Triangle Distribution",
    "section": "Expected Value",
    "text": "Expected Value\nThe expected value is the weighted average of all possible outcomes, taking into account the probabilities of the outcomes. For a triangular distribution, the expected value is calculated by adding the values of the three parameters of the distribution (minimum, maximum and mode) and dividing by 3. \\[ \\begin{equation} \\tag{1.6}\n    E(x) = \\frac{a+b+c}3\n\\end{equation} \\]"
  },
  {
    "objectID": "posts/triangle distribution/triangle distribution.html#mean",
    "href": "posts/triangle distribution/triangle distribution.html#mean",
    "title": "Triangle Distribution",
    "section": "Mean",
    "text": "Mean\nThe mean is the arithmetic average of all observed values. In the case of a triangular distribution, the mean depends on the position of the mode in the distribution. If the mode is exactly halfway between the minimum and maximum, the mean is equal to the expected value. \\[ \\begin{equation} \\tag{1.7}\n    \\bar{x} = \\frac{b-a}2\n\\end{equation} \\]\n\n\n\n\n\n\nNote\n\n\n\nIn an asymmetric triangular distribution, the expected value and the mean are not identical. This is only the case with a symmetric triangular distribution. In general, the expected value of a triangular distribution is a better measure than the mean to indicate the average value of the distribution."
  },
  {
    "objectID": "posts/triangle distribution/triangle distribution.html#interpretation-and-use",
    "href": "posts/triangle distribution/triangle distribution.html#interpretation-and-use",
    "title": "Triangle Distribution",
    "section": "Interpretation and Use",
    "text": "Interpretation and Use\nIn many real-world applications, sparse data is available to estimate a concrete distribution of the population.\nIn management decision making where the Best Case, the Worst Case and the Most Likely Case are known or can be estimated. In this case, the triangular distribution can be used.\nThe triangular distribution can also be used when absolute bounds are to be set for the minimum and maximum, but the distribution is similar to a lognormal distribution."
  },
  {
    "objectID": "posts/triangle distribution/triangle distribution.html#example",
    "href": "posts/triangle distribution/triangle distribution.html#example",
    "title": "Triangle Distribution",
    "section": "Example",
    "text": "Example\nA software developer estimates that it will take an average of 60 hours to program the application. In the best case, he estimates a processing time of 45 hours, but if problems occur, the effort can increase to 150 hours. This results in a triangular distribution with \\(a=45\\) (minimum), \\(b=150\\) (maximum), and \\(c=60\\) (most likely value).\nThe expected value for the project is: \\[E = \\frac{a+b+c}{3} = \\frac{45+150+60}{3} h = 85\\,h\\]\nThe maximum (mode) of the triangular distribution is: \\[f(c) = \\frac{2}{b-a} = \\frac{2}{150-45} = 0.019\\]\nThe probability for the programmer’s estimate is: \\[\nF(x=c) =  \\frac{(c-a)^2}{(b-a)(c-a)} = \\frac{c-a}{b-a} = \\frac{60-45}{150-45} = 0.143\n\\]\nThe probability of the expected value is: \\[\nF(x=E) = 1- \\frac{(b-E)^2}{(b-a)(b-c)} = 1- \\frac{(150-85)^2}{(150-45)(150-60)} = 0.553\n\\]\nConclusion\nThe probability of the developer’s estimate of the project duration (60 h) is with 14% less likely than the probability of the expected value (85 h) at 55%.\n\n\n\n\n\n\n\n\n\nR Code\n# Triangular Distribution\n\nlibrary(EnvStats)\n# Probability density function (PDF): dtri(q, min, max, mode)\n# Cumulative distribution function (CDF): ptri(q, min, max, mode)\n# q: vector of quantiles, missing values (NA) are allowed\n\n# calculate probability density function (PDF)\ndtri(x = 60, min = 45, max = 150, mode = 60) \n\n# Cumulative distribution function (CDF)\nptri(q = 60, min = 45, max = 150, mode = 60)\n\n# calculate probability density function (PDF)\ndtri(x = 85, min = 45, max = 150, mode = 60) \n\n# Cumulative distribution function (CDF)\nptri(q = 85, min = 45, max = 150, mode = 60) \nConsole output:\n[1] 0.01904762\n\n[1] 0.1428571\n \n[1] 0.01375661\n\n[1] 0.5529101"
  },
  {
    "objectID": "posts/triangle distribution/triangle distribution.html#interpretation",
    "href": "posts/triangle distribution/triangle distribution.html#interpretation",
    "title": "Triangle Distribution",
    "section": "Interpretation",
    "text": "Interpretation\nIn many real-world applications, sparse data is available to estimate a concrete distribution of the population.\nIn management decision making where the Best Case, the Worst Case and the Most Likely Case are known or can be estimated. In this case, the triangular distribution can be used.\nThe triangular distribution can also be used when absolute bounds are to be set for the minimum and maximum, but the distribution is similar to a lognormal distribution."
  },
  {
    "objectID": "posts/triangle distribution/triangle distribution.html#using-the-triangular-distribution-in-practice",
    "href": "posts/triangle distribution/triangle distribution.html#using-the-triangular-distribution-in-practice",
    "title": "Triangle Distribution",
    "section": "Using the triangular distribution in practice",
    "text": "Using the triangular distribution in practice\nThe triangular distribution is used for management decisions where the Best Case, the Worst Case and the Most Likely Case are known or can be estimated."
  },
  {
    "objectID": "posts/confidence interval/confidence interval.html",
    "href": "posts/confidence interval/confidence interval.html",
    "title": "Confidence Intervall",
    "section": "",
    "text": "A confidence interval is a statistical concept used to provide an estimate for an unknown parameter in a population. It indicates how certain or trustworthy that estimate is. Confidence intervals are often used in the context of hypothesis testing and estimation in statistics.\nIn statistics, a confidence interval is usually defined as a range of values within which the true value of a parameter lies with a certain probability. For example, a 95% confidence interval indicates that there is a 95% probability that the true value of the parameter lies within that interval.\nThe calculation of a confidence interval depends on the underlying statistical model and the distribution of the data. Typically, the standard normal distribution or the t-distribution is used, depending on the size of the sample.\nConfidence intervals are useful to account for uncertainty in estimates and to better interpret the results of studies or surveys."
  },
  {
    "objectID": "posts/confidence interval/confidence interval.html#parameter-estimation",
    "href": "posts/confidence interval/confidence interval.html#parameter-estimation",
    "title": "Confidence Intervall",
    "section": "Parameter estimation",
    "text": "Parameter estimation\nThere are two estimation methods in inferential statistics:\n\npoint estimation\ninterval estimation\n\n\nPoint estimation\nIn point estimation, properties about the population are determined as exact values and are usually given a measure of the estimation error.\n\n\nInterval estimation\nIn interval estimation, an interval is specified for the characteristic of the population, and this is then given a confidence interval (e.g., 95%).\n\n\nConclusion\nPoint estimation pretends “pseudo-accuracy”. The estimated parameter value is given with point accuracy, although the true parameter value is very rarely hit exactly.\n\nYou can compare a point estimate to trying to hit a fly (the true parameter value) with a pin (point estimate). With a fly swatter (interval), the probability of hitting the fly is much higher."
  },
  {
    "objectID": "posts/confidence interval/confidence interval.html#confidence-interval-definition",
    "href": "posts/confidence interval/confidence interval.html#confidence-interval-definition",
    "title": "Confidence Intervall",
    "section": "Confidence Interval Definition",
    "text": "Confidence Interval Definition\nA confidence interval is a statistical interval that contains the location of a true parameter of a population with a certain probability. A confidence level of 95% contains the true value with 95% probability. However, a confidence level of 95% does not mean that 95% of the data lies within this interval.\n\nThe intervals \\(\\hat{\\theta}_A\\), \\(\\hat{\\theta}_B\\) and \\(\\hat{\\theta}_E\\) do not contain the true value \\(\\theta\\).\nThe intervals \\(\\hat{\\theta}_A\\) and \\(\\hat{\\theta}_E\\) belong to the 5% of the “least likely” (farthest from the true value) sample values.\nA confidence interval has a lower bound \\(x_u\\) and an upper bound \\(x_o\\).\n\\(\\bar{x}\\) denotes the mean of the data set, \\(z_u\\) and \\(z_o\\) are the z-transformed interval limits.\nThe standard error is the fraction of the standard deviation \\(s_x\\) and the square root of the sample size \\(n\\).\n\\[ \\begin{equation} \\tag{1.0}\n    \\large x_u = \\bar{x} + z_u \\cdot \\frac{s_x}{\\sqrt{n}}\n\\end{equation} \\]\n\\[ \\begin{equation} \\tag{1.1}\n    \\large x_o = \\bar{x} + z_o \\cdot \\frac{s_x}{\\sqrt{n}}\n\\end{equation} \\]"
  },
  {
    "objectID": "r_scripts/r basic plots.html",
    "href": "r_scripts/r basic plots.html",
    "title": "R Basic Plots",
    "section": "",
    "text": "…\nR Code\n\n1 + 1\n\n[1] 2\n\n\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code, only the output is displayed."
  },
  {
    "objectID": "r_scripts/r basic plots.html#title",
    "href": "r_scripts/r basic plots.html#title",
    "title": "R Basic Plots",
    "section": "",
    "text": "…\nR Code\n\n1 + 1\n\n[1] 2\n\n\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code, only the output is displayed."
  },
  {
    "objectID": "posts/bootstrapping/bootstrapping.html",
    "href": "posts/bootstrapping/bootstrapping.html",
    "title": "Bootstrapping",
    "section": "",
    "text": "Bootstrapping is a statistical method that resamples a single data set to create many samples. This process allows to calculate standard errors, construct confidence intervals, and perform hypothesis testing for numerous types of sample statistics.\nThe basic idea of bootstrapping is that inference about a population from sample data (sample → population) can be modeled by resampling the sample data and performing inference about a sample from resampled data (resampled → sample).\n\nFigure 1: Traditional statistical method\n\nFigure 2: Bootstrapping method\nSimple Random Samples (SRS)\nA simple random sample (SRS) of size \\(n\\) consists of \\(n\\) individuals from the population, selected so that each set of \\(n\\) individuals has an equal chance of being the sample actually selected.\nPopulations vs. Samples\nParameters describe populations, statistics describe samples.\n\nThe mean (\\(\\mu\\)) and standard deviation (\\(\\sigma\\)) of a population are parameters.\nThe mean (\\(\\bar{x}\\)) and standard deviation (\\(s\\)) of a sample are statistics."
  },
  {
    "objectID": "posts/bootstrapping/bootstrapping.html#bootstrapping-method",
    "href": "posts/bootstrapping/bootstrapping.html#bootstrapping-method",
    "title": "Bootstrapping",
    "section": "",
    "text": "Bootstrapping is a statistical method that resamples a single data set to create many samples. This process allows to calculate standard errors, construct confidence intervals, and perform hypothesis testing for numerous types of sample statistics.\nThe basic idea of bootstrapping is that inference about a population from sample data (sample → population) can be modeled by resampling the sample data and performing inference about a sample from resampled data (resampled → sample).\n\nFigure 1: Traditional statistical method\n\nFigure 2: Bootstrapping method\nSimple Random Samples (SRS)\nA simple random sample (SRS) of size \\(n\\) consists of \\(n\\) individuals from the population, selected so that each set of \\(n\\) individuals has an equal chance of being the sample actually selected.\nPopulations vs. Samples\nParameters describe populations, statistics describe samples.\n\nThe mean (\\(\\mu\\)) and standard deviation (\\(\\sigma\\)) of a population are parameters.\nThe mean (\\(\\bar{x}\\)) and standard deviation (\\(s\\)) of a sample are statistics."
  },
  {
    "objectID": "posts/bootstrapping/bootstrapping.html#types-of-bootstrapping",
    "href": "posts/bootstrapping/bootstrapping.html#types-of-bootstrapping",
    "title": "Bootstrapping",
    "section": "Types of bootstrapping",
    "text": "Types of bootstrapping\n\nParametric Bootstrapping\nParametric bootstraps resample a known distribution function $F$ (e.g., normal) whose parameters (e.g., mean, variance) are estimated from the sample data.\n\n\nNonparametric Bootstraps\nNonparametric bootstraps make no assumptions \\(\\hat{F}\\) about the underlying data distribution \\(F\\).\nProcess steps:\n\nEstimate the parameters of the hypothesized parametric model (parametric bootstrapping only).\nTake a random sample from the original data by sampling with replacement, with the same number of items as in the original data set.\nCompute the statistic of interest (e.g., mean, median, standard deviation) from the bootstrapped sample.\nRepeat steps \\((2)\\) and \\((3)\\) a large number of times.\nAnalyze the distribution of the bootstrap statistics or statistics of interest (e.g., mean, standard deviation, confidence intervals).\n\n\n\n\n\n\n\nTypes of bootstrapping\n\n\n\nParametric bootstrapping can be efficient when the underlying data distribution is well known. However, it can lead to biased results if the assumed model is incorrect.\nNonparametric bootstrapping is more flexible because it does not rely on specific distribution assumptions. It can be particularly useful when the underlying data distribution is unknown or complex. However, it may require a larger number of bootstrap samples to obtain accurate estimates compared to parametric bootstrapping.\n\\(\\rightarrow\\) Parametric bootstrapping assumes a specific distribution and estimates parameters from the data, while nonparametric bootstrapping makes no assumption about the distribution and estimates the sampling distribution directly through resampling."
  },
  {
    "objectID": "posts/bootstrapping/bootstrapping.html#bootstrapping-vs.-monte-carlo-simulation",
    "href": "posts/bootstrapping/bootstrapping.html#bootstrapping-vs.-monte-carlo-simulation",
    "title": "Bootstrapping",
    "section": "Bootstrapping vs. Monte Carlo Simulation",
    "text": "Bootstrapping vs. Monte Carlo Simulation\nBootstrapping and Monte Carlo simulation are two different statistical techniques that are used for different purposes. Bootstrapping can be seen as a specific application of Monte Carlo methods, where random resampling is used to estimate sampling distributions and perform statistical inference.\n\nAbout Bootstrapping\nBootstrapping is a resampling technique used to estimate the sampling distribution of a statistic by repeatedly sampling with replacement from the original data.\nAdvantages\n\nNonparametric: Bootstrapping does not assume a specific underlying distribution, making it useful when the distribution of the data is unknown or non-standard.\nEasy to implement: Bootstrapping is conceptually simple and easy to implement.\nUseful with small sample sizes: It works with limited data where traditional statistical tests may be less reliable.\n\nDisadvantages\n\nComputationally intensive: Bootstrapping involves many resampling iterations, which can be computationally expensive for large datasets or complex models.\nData quality dependence: Bootstrapping results are highly dependent on the quality and representativeness of the original data.\nNot suitable for all scenarios: While powerful, bootstrapping may not be the best approach for certain statistical problems, especially those involving complex models or non-standard scenarios."
  },
  {
    "objectID": "posts/bootstrapping/bootstrapping.html#monte-carlo-simulation",
    "href": "posts/bootstrapping/bootstrapping.html#monte-carlo-simulation",
    "title": "Bootstrapping",
    "section": "Monte Carlo Simulation",
    "text": "Monte Carlo Simulation\nMonte Carlo simulation is a technique used to model and analyze the probability of different outcomes in a process that involves uncertainty and randomness. It relies on random sampling to estimate complex mathematical functions or to simulate different scenarios.\nAdvantages\n\nFlexibility: Monte Carlo simulation is applicable to a wide range of problems, including optimization, probability estimation, and risk analysis, making it a versatile tool.\nProvides a range of outcomes: It provides a distribution of possible outcomes, giving insight into the likelihood of different scenarios occurring.\nWorks well with optimization problems: Monte Carlo simulation can be integrated with optimization algorithms to find optimal solutions in complex systems.\n\nDisadvantages\n\nResource intensive: Monte Carlo simulation requires a large number of iterations to produce reliable results, which can be computationally expensive and time consuming.\nConvergence issues: In some cases, Monte Carlo simulation may converge slowly or require special techniques to ensure accurate results.\n\nKey Differences Between Monte Carlo Simulation and Bootstrapping\n\n\n\n\n\n\n\n\nFeature\nMote Carlo simulation\nBootstrapping method\n\n\n\n\nGenerality\nCan be used to estimate the uncertainty of any statistic\nOnly used to estimate the uncertainty of statistics from a sample of data\n\n\nEfficiency\nLess efficient than bootstrapping\nMore efficient than Monte Carlo simulation\n\n\nEase of use\nMore difficult to use than bootstrapping\nEasier to use than Monte Carlo simulation"
  },
  {
    "objectID": "posts/confidence interval/confidence interval.html#r-code",
    "href": "posts/confidence interval/confidence interval.html#r-code",
    "title": "Confidence Intervall",
    "section": "R code",
    "text": "R code\nExample 1 – Confidence interval of dataset\n\n# Calculate confidence interval with function t.test from core package stats\n\n# Sample data\ndata &lt;- c(23, 28, 32, 27, 25, 30, 31, 29, 26, 24)\n\n# Calculate the confidence interval\n# The function t.test performs the Student's t-Tests on vectors of data\n# By default conf.level=0.95\nresult &lt;- t.test(data)\n\n# Extract the confidence interval\nconfidence_interval &lt;- result$conf.int\n\n# Get the lower bound and the upper bound of the confidence interval\nlower_bound &lt;- result$conf.int[1]\nupper_bound &lt;- result$conf.int[2]\n\n# Print lower bound and uppder bound of the confidence interval\ncat(\"Lower bound of the 95% confidence interval:\", lower_bound, \"\\n\")\n\nLower bound of the 95% confidence interval: 25.33415 \n\ncat(\"Upper bound of the 95% confidence interval:\", upper_bound, \"\\n\")\n\nUpper bound of the 95% confidence interval: 29.66585 \n\n# Plot histogram\nplot(data, main = \"Histogram\", xlab = \"Data\", ylab = \"Value\")\n\n# Draw lower bound and upper bound of the confidence interval\nabline(h = lower_bound, col = 'red', lwd = 1, lty = 2)\ntext(x = 1.5, y = lower_bound+0.3, labels = \"lower bound\", col = \"#4D4D4D\")\nabline(h = upper_bound, col = 'red', lwd = 1, lty = 2)\ntext(x = 1.5, y = upper_bound+0.3, labels = \"upper bound\", col = \"#4D4D4D\")\n\n\n\n\n\n\n\n\nThe 95% confidence interval is [25.3, 29.7].\nExample 2 – Univariate Statistic (Median)\nFor this example, we will generate \\(n = 100\\) observations from a standard normal distribution, and use the median as the parameter/statistic of interest. Note that the true (population) median is zero. Since the median is a univariate statistic, the bootstrap distribution will be a vector of length \\(R + 1\\) containing the bootstrap replicates of the median.\n\n# Univariate Statistic (Median)\n\nlibrary(nptest)\n\nPackage 'nptest' version 1.1\nType 'citation(\"nptest\")' to cite this package.\n\n# generate 100 standard normal observations\nset.seed(1)\nn &lt;- 100\nx &lt;- rnorm(n)\n\n# nonparametric bootstrap\nnpbs &lt;- np.boot(x = x, statistic = median)\nnpbs\n\n\nNonparametric Bootstrap of Univariate Statistic\nusing R = 9999 bootstrap replicates\n\n  t0: 0.1139\n  SE: 0.1394\nBias: 0.0185 \n\nBCa Confidence Intervals:\n      lower  upper\n90% -0.0566 0.3411\n95% -0.0811 0.3673\n99% -0.1351 0.3940\n\n# check t0, SE, and bias\nmedian(x)                          # t0\n\n[1] 0.1139092\n\nsd(npbs$boot.dist)                 # SE\n\n[1] 0.1393567\n\nmean(npbs$boot.dist) - npbs$t0     # Bias\n\n[1] 0.01845341\n\n# bootstrap distribution\nhist(npbs$boot.dist,\n     xlab = \"t*\",\n     ylab = \"Density\",\n     main = \"Bootstrap Distribution\")\n\n# Observed median\nabline(v = npbs$t0,\n       lty = 2,\n       col = \"red\")\n\n# CI 95% lower boundary\nabline(v = npbs$bca[2, 1],\n       lty = 2,\n       col = \"blue\")\n\n# CI 95% upper boundary\nabline(v = npbs$bca[2, 2],\n       lty = 2,\n       col = \"blue\")\n\nlegend(\n  \"topleft\",\n  legend = c(\"t0\", \"CI 95%\"),\n  col = c(\"red\", \"blue\"),\n  lty = 2,\n  bty = \"n\"\n)"
  },
  {
    "objectID": "posts/bootstrapping/bootstrapping.html#r-code",
    "href": "posts/bootstrapping/bootstrapping.html#r-code",
    "title": "Bootstrapping",
    "section": "R code",
    "text": "R code\nThere are several R packages that can be used for bootstrapping calculations with R:\n\nboot\nbootstrap\nnptest\n\n\nExample 1 – Multivariate Statistic (Median)\nFor this example, we will generate \\(n = 100\\) observations from a standard normal distribution, and use the quartiles as the parameters/statistics of interest. Note that the true (population) quartiles are \\(Q1 = qnorm(0.25) = -0.6744898\\), \\(Q2 = qnorm(0.5) = 0\\), and \\(Q3 = qnorm(0.75) = 0.6744898\\). Since the quartiles are a multivariate statistic, the bootstrap distribution will be a matrix of dimension \\(R + 1 × 3\\), where each column contains the bootstrap replicates of the corresponding quartile.\n\n# Multivariate Statistic (Quartiles)\n\nlibrary(nptest)\n\nPackage 'nptest' version 1.1\nType 'citation(\"nptest\")' to cite this package.\n\n# generate 100 standard normal observations\nset.seed(1)\nn &lt;- 100\nx &lt;- rnorm(n)\n\n# nonparametric bootstrap (using ... to enter 'probs' argument)\nnpbs &lt;- np.boot(x = x,\n                statistic = quantile,\n                probs = c(0.25, 0.5, 0.75))\nnpbs\n\n\nNonparametric Bootstrap of Multivariate Statistic\nusing R = 9999 bootstrap replicates\n\n          25%    50%     75%\n  t0: -0.4942 0.1139  0.6915\n  SE:  0.1172 0.1394  0.0933\nBias:  0.0058 0.0185 -0.0170\n\n95% BCa Confidence Intervals:\n          25%     50%    75%\nlower -0.6941 -0.0811 0.5047\nupper -0.2534  0.3673 0.8811\n\n# bootstrap distribution\npar(mfrow = c(1, 3))\nfor (j in 1:3) {\n  hist(\n    npbs$boot.dist[, j],\n    xlab = \"t*\",\n    ylab = \"Density\",\n    main = paste0(\"Bootstrap Distribution\", \": Q\", j)\n  )\n  abline(v = npbs$t0[j],\n         lty = 2,\n         col = \"red\")\n  legend(\n    \"topright\",\n    paste0(\"t0[\", j, \"]\"),\n    lty = 2,\n    col = \"red\",\n    bty = \"n\"\n  )\n}"
  },
  {
    "objectID": "posts/confidence interval/confidence interval.html#example",
    "href": "posts/confidence interval/confidence interval.html#example",
    "title": "Confidence Intervall",
    "section": "Example",
    "text": "Example\nCalculate the 95% confidence interval for the monthly snack spending of a group of people.\nStep 1: Calculate the estimated parameter\nCalculate the mean \\(\\bar{x}\\) from the existing data set.\n\n\n\n# participants\nMonthly snack spending USD\n\n\n\n\n20\n8\n\n\n30\n32\n\n\n10\n0\n\n\n30\n48\n\n\n10\n16\n\n\n\nSample size \\(\\large n = 20+30+10+30+10 = 100\\)\n\\(\\large \\bar{x}=\\frac{20\\cdot8+30\\cdot32+10\\cdot0+30\\cdot48+10\\cdot16}{100}= \\text{USD } 27.2\\)\nStep 2: Transform the interval limits into standard normally distributed values\nWith a confidence level of 95%, we obtain the following interval limits: the lower limit is 2.5% \\(= x_u = 0.025\\) and the upper limit is 97.5% \\(x_o = 0.975\\).\nWe now need to standardize these values by looking at the z-distribution table.\n\nFor the upper bound \\(x_o\\), the z-value is easy to read: \\(x_o = 0.975\\) corresponds to \\(z_o = 1.960\\).\nSince the normal distribution is symmetric and mirrored on the x-axis, the lower limit \\(z_u = -1.960\\) results in\n\n\nWith a mean of \\(\\bar{x} = 27.20\\) USD, a lower limit of \\(z_u= -1.960\\), an upper limit of \\(z_o= 1.960\\) and a sample size of \\(n = 100\\), all required parameters for the formula are given except for the standard deviation \\(s_x\\).\nStep 3: Calculation of variance and standard deviation\nThe variance can be calculated using the values from the table as follows:\n\\(\\large s_x^2=\\ \\frac{\\left(8-27.2\\right)^2\\cdot20+\\left(32-27.2\\right)^2\\cdot30+\\left(0-27.2\\right)^2\\cdot10+\\left(48-27.2\\right)^2\\cdot30+\\left(16-27.2\\right)^2\\cdot10}{100}\\)\n\\(\\large s_x^2=296.96\\)\nThe standard deviation is the square root of the variance:\n\\(\\large s_x=\\sqrt{s_x^2}=\\sqrt{296.96}=17.23\\)\nStep 4: Calculation of lower and upper limit\n\\(\\large x_u=\\ \\bar{x}+\\ z_u\\ \\cdot\\ \\frac{s_x}{\\sqrt n}=27.2+\\left(-1.960\\right)\\cdot\\frac{17.23}{\\sqrt{100}}=23.82\\)\n\\(\\large x_o=\\ \\bar{x}+\\ z_o\\ \\cdot\\ \\frac{s_x}{\\sqrt n}=27.2+1.960\\cdot\\frac{17.23}{\\sqrt{100}}=30.58\\)\nThus, the limits of the contingency interval are at points 23.82 and 30.58. The conclusion is: with 95% confidence, the true mean for the group of people’s monthly snack spending is between USD 23.82 and USDF 30.58."
  },
  {
    "objectID": "posts/confidence interval/confidence interval.html#z-distribution",
    "href": "posts/confidence interval/confidence interval.html#z-distribution",
    "title": "Confidence Intervall",
    "section": "Z-Distribution",
    "text": "Z-Distribution\nBy performing a z-standardization, we can transform the normal distribution into a standard normal distribution. This can be done with any normal distribution (but only with normal distributions). This gives us a distribution that is always the same, with mean \\(µ = 0\\) and standard deviation \\(σ = 1\\). Just like the normal distribution, the standard normal distribution tells you what percentage of values fall within a given range. Now, this range no longer depends on \\(µ\\) and \\(σ\\), but is standardized.\n\n\nIn the standard normal distribution, approximately 68.2% of the values always lie in the range between \\(z = -1\\) and \\(z = +1\\). Or put another way: There is a 68.2% probability that the value is between -1 and +1.\nUsing a z-transformation or z-standardization, you subtract the arithmetic mean from each measured value, divide the resulting difference by the standard deviation, and you get the so-called z-scores.\nAs formula (with \\(x\\) for the respective value, \\(μ\\) for the arithmetic mean and \\(σ\\) for the standard deviation) \\[z=\\frac{x-μ}σ\\]\nAfter the z-transformation\n\nthe arithmetic mean of the transformed series is always zero, and\nthe variance and standard deviation are always 1.\n\nIf a normal distribution is present, the corresponding probabilities can be read from a standard normal distribution table based on the z-values. The standard normal table contains only positive z-values. Since the normal distribution is symmetric and mirrored on the x-axis, the z-value for the lower limit results from the negative z-value for the upper limit \\(z_u = -z_o\\)."
  },
  {
    "objectID": "posts/confidence interval/confidence interval.html#three-sigma-rule",
    "href": "posts/confidence interval/confidence interval.html#three-sigma-rule",
    "title": "Confidence Intervall",
    "section": "Three Sigma Rule",
    "text": "Three Sigma Rule\nThe “Three Sigma Rule” is a statistical rule used to quantify the probability of events within a normally distributed data set.\n\nThe three sigma rule states that\na) 68.3% of the data are within one standard deviation of the mean (\\(\\mu \\pm \\sigma\\)).\nb) 95.4% of the data are within two standard deviations of the mean (\\(\\mu \\pm 2\\sigma\\)).\nc) 99.7% of the data are within 3 standard deviations of the mean (\\(\\mu \\pm 3\\sigma\\)).\n\n\n\n\n\n\nThree-sigma rule\n\n\n\nThe three-sigma rule applies only to normally distributed data sets. The three sigma rule does not apply to skewed triangular distributions."
  },
  {
    "objectID": "posts/standard deviation - standard error - confidence interval/standard deviation - standard error - confidence interval.html",
    "href": "posts/standard deviation - standard error - confidence interval/standard deviation - standard error - confidence interval.html",
    "title": "Standard Deviation, Standard Error, and Confidence Interval",
    "section": "",
    "text": "Standard deviation measures the amount of variation or dispersion within a single dataset.\nIt is used to describe how individual data points deviate from the mean (average) of the dataset.\nStandard deviation is typically used when you have the entire population data or when you want to analyze the variability within a single sample.\n\nThe formula for the population standard deviation (denoted as \\(σ\\)) is:\n\\[\\sigma = \\sqrt{\\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}{n}}\\]\nwhere:\n\n\\(x_i\\) represents each individual data point in the dataset\n\\(\\bar{x}\\) is the mean (average) of all data points\n\\(n\\) is the number of data points in the dataset (sample size)\n\nThe formula for the sample standard deviation (denoted as \\(s\\)) is:\n\\[s = \\sqrt{\\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}{n-1}}\\]\nwhere:\n\n\\(x_i\\) represents each individual data point in the dataset\n\\(\\bar{x}\\) is the mean (average) of all data points\n\\(n\\) is the number of data points in the dataset (sample size)\n\nThe standard deviation is a statistics tool in statistics that allows to understand the spread or dispersion of data points, the consistency of the data, and the degree of uncertainty associated with the data.\nA smaller standard deviation indicates that data points tend to be closer to the mean, while a larger standard deviation suggests more significant variability or spread within the dataset.\n\n\n\n# Data set\ndata &lt;- c(10, 15, 20, 25, 30)\n\n# Calculate sample standard deviation\nsample_standard_deviation &lt;- sd(data)\n\n# Print the result\ncat(\"Sample standard deviation:\", sample_standard_deviation)\n\nSample standard deviation: 7.905694\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe R function sd calculates the sample standard deviation."
  },
  {
    "objectID": "posts/standard deviation - standard error - confidence interval/standard deviation - standard error - confidence interval.html#standard-deviation",
    "href": "posts/standard deviation - standard error - confidence interval/standard deviation - standard error - confidence interval.html#standard-deviation",
    "title": "Standard Deviation, Standard Error, and Confidence Interval",
    "section": "",
    "text": "Standard deviation measures the amount of variation or dispersion within a single dataset.\nIt is used to describe how individual data points deviate from the mean (average) of the dataset.\nStandard deviation is typically used when you have the entire population data or when you want to analyze the variability within a single sample.\n\nThe formula for the population standard deviation (denoted as \\(σ\\)) is:\n\\[\\sigma = \\sqrt{\\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}{n}}\\]\nwhere:\n\n\\(x_i\\) represents each individual data point in the dataset\n\\(\\bar{x}\\) is the mean (average) of all data points\n\\(n\\) is the number of data points in the dataset (sample size)\n\nThe formula for the sample standard deviation (denoted as \\(s\\)) is:\n\\[s = \\sqrt{\\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}{n-1}}\\]\nwhere:\n\n\\(x_i\\) represents each individual data point in the dataset\n\\(\\bar{x}\\) is the mean (average) of all data points\n\\(n\\) is the number of data points in the dataset (sample size)\n\nThe standard deviation is a statistics tool in statistics that allows to understand the spread or dispersion of data points, the consistency of the data, and the degree of uncertainty associated with the data.\nA smaller standard deviation indicates that data points tend to be closer to the mean, while a larger standard deviation suggests more significant variability or spread within the dataset.\n\n\n\n# Data set\ndata &lt;- c(10, 15, 20, 25, 30)\n\n# Calculate sample standard deviation\nsample_standard_deviation &lt;- sd(data)\n\n# Print the result\ncat(\"Sample standard deviation:\", sample_standard_deviation)\n\nSample standard deviation: 7.905694\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe R function sd calculates the sample standard deviation."
  },
  {
    "objectID": "posts/standard deviation - standard error - confidence interval/standard deviation - standard error - confidence interval.html#standard-error",
    "href": "posts/standard deviation - standard error - confidence interval/standard deviation - standard error - confidence interval.html#standard-error",
    "title": "Standard Deviation, Standard Error, and Confidence Interval",
    "section": "Standard error",
    "text": "Standard error\n\nStandard error measures the variability of a sample statistic (e.g., sample mean) from one sample to another, assuming that multiple samples are drawn from the same population.\nIt provides an indication of the precision or accuracy of the sample statistic in estimating the population parameter.\nStandard error is typically used in inferential statistics to make inferences about the population based on a sample.\n\nThe formula for the standard error (denoted as \\(SE\\) is:\n\\[SE = \\frac{\\sigma}{\\sqrt{n}}\\]\nwhere:\n\n\\(σ\\) represents the population standard deviation\n\\(n\\) is the sample size\n\nIf the population standard deviation (\\(σ\\)) is unknown and needs to be estimated from the sample, the formula becomes:\n\\[SE = \\frac{s}{\\sqrt{n}}\\]\nwhere:\n\n\\(s\\) is the sample standard deviation\n\nThe standard error is used in hypothesis testing, confidence intervals, and other statistical analyses.\nA smaller standard error indicates that the sample statistic is more precise and likely to be closer to the true population parameter. Conversely, a larger standard error suggests that the sample statistic has more uncertainty and could vary widely in different samples."
  },
  {
    "objectID": "posts/standard deviation - standard error - confidence interval/standard deviation - standard error - confidence interval.html#confidence-interval",
    "href": "posts/standard deviation - standard error - confidence interval/standard deviation - standard error - confidence interval.html#confidence-interval",
    "title": "Standard Deviation, Standard Error, and Confidence Interval",
    "section": "Confidence interval",
    "text": "Confidence interval\nA confidence interval is a statistical range that is constructed around a sample statistic, such as the sample mean or sample proportion, to estimate the likely range of values of the corresponding population parameter. It provides a measure of uncertainty about the true value of the population parameter based on the information available from a sample.\n\nA confidence interval is a range of values around a sample statistic (e.g., sample mean) to estimate the likely range of values of the corresponding population parameter.\nIt provides a measure of the uncertainty about the true value the population parameter.\nThe confidence level is typically expressed as a percentage (e.g., 95% confidence interval), and it represents the probability that the interval contains the true population parameter in repeated sampling.\n\nThe formula for the confidence interval (denoted as \\(CI\\)) is:\n\\[CI = \\bar{x} \\pm z \\frac{s}{\\sqrt{n}}\\]\nwhere:\n\n\\(\\bar{x}\\) is the sample mean\n\\(z\\) is the confidence level value\n\\(s\\) is the sample standard deviation\n\\(n\\) is the sample size (number of data points in the dataset)\n\nConfidence intervals are used in statistical inference as they provide a range of plausible values for the population parameter and help in making more informed decisions and drawing conclusions from sample data.\nThe wider the confidence interval, the less precise the estimate, while a narrower confidence interval indicates a more precise estimate.\n\n\n\n\n\n\nNote\n\n\n\n\nThe standard deviation quantifies the variation within a single dataset, while standard error quantifies the variability of a sample statistic across different samples from the same population.\nThe standard deviation describes the spread of the data points, while standard error describes the precision of a sample statistic in estimating the population parameter.\nThe standard error is a measure of precision or accuracy of the sample estimate, while a confidence interval is a range of values that provides an estimate of the likely range of the population parameter with a specified level of confidence.\nThe standard error helps in understanding the variability of the sample statistic, while confidence intervals help in making inferences about the population parameter based on the sample data."
  },
  {
    "objectID": "posts/standard deviation - standard error - confidence interval/standard deviation - standard error - confidence interval.html#r-code",
    "href": "posts/standard deviation - standard error - confidence interval/standard deviation - standard error - confidence interval.html#r-code",
    "title": "Standard Deviation, Standard Error, and Confidence Interval",
    "section": "R code",
    "text": "R code\n\nrandom_normal &lt;- rnorm(n = 10,\n                       mean = 10,\n                       sd = 2)\nm &lt;- lm(random_normal ~ 1)\nsummary(m)\n\n\nCall:\nlm(formula = random_normal ~ 1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.1112 -1.3216 -0.0333  1.6658  4.0395 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   9.5703     0.9386    10.2 3.04e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.968 on 9 degrees of freedom\n\n\nPlease note, that CI is set up to 95% which is wider that estimated mean 10.7324 +/- 0.7154. But they can be the same, if we’ll set confidence interval to 68% level (instead of 95%), we’ll get virtually the same answer, as 10.7324 +/-0.7154.\nSide note: regressing with syntax random_normal ~ 1 is the same as estimating the mean. It’s just for convenience to quickly obtain standard errors and CI in this particular case. Reporting both won’t hurt, to my opinion. But CI’s, in general, are more versatile."
  },
  {
    "objectID": "posts/standard deviation - standard error - confidence interval/standard deviation - standard error - confidence interval.html#r-code-1",
    "href": "posts/standard deviation - standard error - confidence interval/standard deviation - standard error - confidence interval.html#r-code-1",
    "title": "Standard Deviation, Standard Error, and Confidence Interval",
    "section": "R code",
    "text": "R code\n\nrandom_normal &lt;- rnorm(n = 10,\n                       mean = 10,\n                       sd = 2)\nm &lt;- lm(random_normal ~ 1)\nsummary(m)\n\n\nCall:\nlm(formula = random_normal ~ 1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.73025 -1.72644 -0.09643  1.75533  3.00881 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  10.4914     0.6415   16.36 5.31e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.029 on 9 degrees of freedom\n\n\nPlease note, that CI is set up to 95% which is wider that estimated mean 10.7324 +/- 0.7154. But they can be the same, if we’ll set confidence interval to 68% level (instead of 95%), we’ll get virtually the same answer, as 10.7324 +/-0.7154.\nSide note: regressing with syntax random_normal ~ 1 is the same as estimating the mean. It’s just for convenience to quickly obtain standard errors and CI in this particular case. Reporting both won’t hurt, to my opinion. But CI’s, in general, are more versatile.\n\nExample\nCalculate the normal distributions with different number of observations (\\(n = 100, 1000, and \\, 10000\\)) and different standard deviation (\\(sd = \\sqrt(1), \\sqrt(1.1)\\)) and compare the confidence intervals."
  },
  {
    "objectID": "posts/lognormal distribution/lognormal distribution.html",
    "href": "posts/lognormal distribution/lognormal distribution.html",
    "title": "Lognormal Distribution",
    "section": "",
    "text": "The lognormal distribution is a continuous probability distribution that can only take positive values. It describes the distribution of a random variable \\(X\\) if the random variable \\(Y = ln(X)\\) transformed by the logarithm is normally distributed.\nThe lognormal distribution is often used to model phenomena in which the logarithmized values of the data follow a normal distribution, while the actual data itself has a non-normal distribution."
  },
  {
    "objectID": "posts/lognormal distribution/lognormal distribution.html#probability-density-function-pdf",
    "href": "posts/lognormal distribution/lognormal distribution.html#probability-density-function-pdf",
    "title": "Lognormal Distribution",
    "section": "Probability Density Function (PDF)",
    "text": "Probability Density Function (PDF)\nThe probability density function of the lognormal distribution is defined by\n\\[\\begin{equation} \\tag{1.1} f(x) = \\frac{1}{x\\sigma \\sqrt{2\\pi}} e\\left(-\\frac{(\\ln(x) - \\mu)^2}{2\\sigma^2}\\right) \\end{equation}\\]\nwhere \\(x\\) is the random variable, \\(\\mu\\) is the mean of the logarithmic distribution, and \\(\\sigma\\) is the standard deviation of the logarithmic distribution.\n\n\n\n\n\nFigure 1: Lognormal Probability Density Functions (PDF) with varying standard deviations"
  },
  {
    "objectID": "posts/lognormal distribution/lognormal distribution.html#cumulative-distribution-function-cdf",
    "href": "posts/lognormal distribution/lognormal distribution.html#cumulative-distribution-function-cdf",
    "title": "Lognormal Distribution",
    "section": "Cumulative Distribution Function (CDF)",
    "text": "Cumulative Distribution Function (CDF)\nThe Cumulative Distribution Function (CDF) of the lognormal distribution is as follows:\n\\[\\begin{equation} \\tag{1.2} F(x) = \\int_{0}^{x} f(t) dt = \\Phi (\\frac{ln(x)-\\mu}{\\sigma}) \\end{equation}\\]\nwhere \\(\\Phi\\) denotes the distribution function of the standard normal distribution.\n\n\n\n\n\nFigure 2: Lognormal Cumulative Density Functions (CDF) with varying standard deviations"
  },
  {
    "objectID": "posts/lognormal distribution/lognormal distribution.html#relationship-between-normal-and-lognormal-distribution",
    "href": "posts/lognormal distribution/lognormal distribution.html#relationship-between-normal-and-lognormal-distribution",
    "title": "Lognormal Distribution",
    "section": "Relationship between normal and lognormal distribution",
    "text": "Relationship between normal and lognormal distribution\nIf \\(Y = \\mu + \\sigma Z\\) is normal distributed, then \\(X\\sim e^{Y}\\) is lognormal distributed.\n\nFigure 3: Normal distribution vs. lognormal distribution (inspired by figure from Wikipedia)"
  },
  {
    "objectID": "posts/lognormal distribution/lognormal distribution.html#median",
    "href": "posts/lognormal distribution/lognormal distribution.html#median",
    "title": "Lognormal Distribution",
    "section": "Median",
    "text": "Median\nThe median of the lognormal distribution is:\n\\(\\large x_{med}=e^{\\mu}\\)\nThe median is the middle value when a data set is ordered from least to greatest."
  },
  {
    "objectID": "posts/lognormal distribution/lognormal distribution.html#mean",
    "href": "posts/lognormal distribution/lognormal distribution.html#mean",
    "title": "Lognormal Distribution",
    "section": "Mean",
    "text": "Mean\nThe mean of the lognormal distribution is:\n\\(\\large m = e^{\\mu + {\\frac {\\sigma ^{2}}{2}}}\\)\nThe mean (average) of a data set is found by adding all numbers in the data set and then dividing by the number of values in the set. The mean of the lognormal distribution is equal to the expected value."
  },
  {
    "objectID": "posts/lognormal distribution/lognormal distribution.html#mode",
    "href": "posts/lognormal distribution/lognormal distribution.html#mode",
    "title": "Lognormal Distribution",
    "section": "Mode",
    "text": "Mode\nThe mode (the most frequent value of the distribution) of the lognormal distribution is:\n\\(\\large Modus(X) = x_{D} = e^{\\mu -\\sigma ^{2}}\\)\nThe mode is the number that occurs most often in a data set."
  },
  {
    "objectID": "posts/lognormal distribution/lognormal distribution.html#expected-value",
    "href": "posts/lognormal distribution/lognormal distribution.html#expected-value",
    "title": "Lognormal Distribution",
    "section": "Expected Value",
    "text": "Expected Value\nThe expected value of the lognormal distribution is:\n\\(\\large E(X) = e^{\\mu + {\\frac {\\sigma ^{2}}{2}}}\\)\nWhere \\(μ\\) is the mean value and \\(σ\\) is the standard deviation of the lognormal distribution.\nThe expected value of a lognormal distribution is a key figure that indicates the central tendency value of the distribution. It is important to note that the expected value of a lognormal distribution is generally greater than the median (the 50th percentile value) of the distribution. This is because the lognormal distribution has a positive skewness (right-skewed), which means that it tends to have higher values."
  },
  {
    "objectID": "posts/lognormal distribution/lognormal distribution.html#example",
    "href": "posts/lognormal distribution/lognormal distribution.html#example",
    "title": "Lognormal Distribution",
    "section": "Example",
    "text": "Example\n…"
  },
  {
    "objectID": "r_scripts/embedded R codes.html",
    "href": "r_scripts/embedded R codes.html",
    "title": "Embedded R code",
    "section": "",
    "text": "R Code\n\n1 + 1\n\n[1] 2\n\n\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code, only the output is displayed."
  },
  {
    "objectID": "posts/lognormal distribution/lognormal distribution.html#variance",
    "href": "posts/lognormal distribution/lognormal distribution.html#variance",
    "title": "Lognormal Distribution",
    "section": "Variance",
    "text": "Variance\nThe variance of the lognormal distribution is:\n\\(\\large Var(X) = e^{2\\mu +\\sigma^2} (e^{\\sigma^2}-1)\\)"
  },
  {
    "objectID": "posts/lognormal distribution/lognormal distribution.html#standard-deviation",
    "href": "posts/lognormal distribution/lognormal distribution.html#standard-deviation",
    "title": "Lognormal Distribution",
    "section": "Standard deviation",
    "text": "Standard deviation\nThe standard deviation of the lognormal distribution is:\n\\(\\large \\sqrt Var(X)\\)"
  },
  {
    "objectID": "posts/lognormal distribution/lognormal distribution.html#skewness",
    "href": "posts/lognormal distribution/lognormal distribution.html#skewness",
    "title": "Lognormal Distribution",
    "section": "Skewness",
    "text": "Skewness\nThe lognormal distribution is skewed to the right. The greater the difference between the expected value and the median, the more pronounced the skewness of the lognormal distribution.\n\n\n\n\n\nFigure 4: Comparison of mean value, median, and mode"
  },
  {
    "objectID": "posts/lognormal distribution/lognormal distribution.html#example-1-us-income-2006",
    "href": "posts/lognormal distribution/lognormal distribution.html#example-1-us-income-2006",
    "title": "Lognormal Distribution",
    "section": "Example 1: US Income 2006",
    "text": "Example 1: US Income 2006\nThe graph below shows the distribution of US income in 2006. It illustrates how the location parameter is the median of this distribution. The shaded area represents 50% of the distribution, which corresponds to the median value of 28,788. The value is calculated by taking e and increasing it by the location value. In this case, \\(e^{10.2677} = 28,788\\).\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe 2009 paper by Pinkovskiy and Sala-i-Martin is entitled “Parametric Estimations of the World Distribution of Income”. It is an econometric study in which the authors attempt to analyze the global distribution of income using parametric estimation.\nThe authors use a method based on the assumption that the distribution of income follows a lognormal distribution. They estimate the parameters of this distribution and use the results to quantify the global distribution of income."
  },
  {
    "objectID": "posts/lognormal distribution/lognormal distribution.html#example-2-lifetime-of-an-engine",
    "href": "posts/lognormal distribution/lognormal distribution.html#example-2-lifetime-of-an-engine",
    "title": "Lognormal Distribution",
    "section": "Example 2: Lifetime of an engine",
    "text": "Example 2: Lifetime of an engine\nSuppose the lifetime of an engine has a lognormal distribution. What is the probability that the lifetime will exceed 12,000 hours if the mean and standard deviation of the underlying normal distribution are 11,000 hours and 1.3 hours respectively.\n\n# Lifetime of an engine\n\n# Parameters\nmean_normal &lt;- 11 # mean of the underlying normal distribution\nsd_normal &lt;- 1.3  # standard deviation of the underlying normal distribution\nthreshold_lifetime &lt;- 12000  # threshold value\n\n# Calculate the probability that the lifetime exceeds the threshold\nprobability_exceeds_threshold &lt;- 1 - pnorm(log(threshold_lifetime), mean = mean_normal, sd = sd_normal)\n\n# Print the result\ncat(\"Probability that the lifetime exceeds 12,000 hours:\", probability_exceeds_threshold, \"\\n\")\n\nProbability that the lifetime exceeds 12,000 hours: 0.8918476"
  },
  {
    "objectID": "posts/probability density function (PDF) - cumulative distribution function (CDF)/PDF - CDF.html",
    "href": "posts/probability density function (PDF) - cumulative distribution function (CDF)/PDF - CDF.html",
    "title": "Probability Density Function (PDF) - Cumulative Distribution function (CDF)",
    "section": "",
    "text": "The probability density function (PDF) and the cumulative distribution function (CDF) are both concepts from probability theory and statistics. They describe different aspects of a random variable. The main differences are in what they represent and how they are interpreted.\n\nProbability Density Function (PDF)\nDescription: The PDF describes the probability of a random variable taking on a particular value. It is a non-negative function that integrates to 1 over its entire range.\nRepresentation: The PDF, denoted as \\(f(x)\\) for a random variable \\(X\\), represents the probability distribution of the variable.\nInterpretation: The area under the PDF curve over a range gives the probability that the random variable falls within that range.\nIntegral Interpretation: The integral of the PDF over a certain interval gives the probability that the random variable falls within that interval. Mathematically, for an interval \\([a,b]\\):\n\\[\\large P(a≤X≤b) = \\int_{a}^{b} f(x)dx\\]\nNormalization: The total area under the entire PDF curve is 1.\n\nExample\n\nRoll a perfect dice one time. Let \\(x\\) denote the number that the dice lands on, then the probability density function for the outcome can be described as follows:\nP(x &lt; 1) : 0\nP(x = 1) : 1/6\nP(x = 2) : 1/6\nP(x = 3) : 1/6\nP(x = 4) : 1/6\nP(x = 5) : 1/6\nP(x = 6) : 1/6\nP(x &gt; 6) : 0\nNote: This is an example of a discrete random variable, since \\(x\\) can only take on integer values. For a continuous random variable, the PDF cannot be used, since the probability that \\(x\\) takes on any exact value is zero.\n\n\n\nCumulative Distribution Function (CDF)\nDescription: The CDF describes the probability that a random variable will be less than or equal to a particular value. It is an increasing function that starts at 0 and approaches 1 as the value of x approaches the upper bound of the PDF. The value of the CDF at a particular point represents the probability that the random variable will take on a value less than or equal to that point.\nRepresentation: The CDF, denoted as \\(F(x)\\), represents the cumulative probability that a random variable \\(X\\) is less than or equal to a specific value \\(x\\).\nInterpretation: \\(F(x)\\) gives the probability that \\(X\\) is less than or equal to \\(x\\). The value \\(F(x)\\) at a point \\(x\\) is a cumulative probability.\nIntegral Interpretation: The CDF is related to the PDF through integration. Specifically, \\(F(x)\\) is the integral of the PDF from \\(−∞\\) to \\(x\\):\n\\[\\large F(x) = \\int_{-∞}^{x} f(t) dt\\]\nRange: The CDF ranges from 0 to 1, with \\(F(−∞)=0\\) and \\(F(∞)=1\\).\nMonotonicity: The CDF is a non-decreasing function.\n\nExample:\n\nRoll a perfect dice one time. Let \\(x\\) denote the number that the dice lands on, then the cumulative distribution function for the outcome can be described as follows:\nP(x ≤ 0) : 0\nP(x ≤ 1) : 1/6\nP(x ≤ 2) : 2/6\nP(x ≤ 3) : 3/6\nP(x ≤ 4) : 4/6\nP(x ≤ 5) : 5/6\nP(x ≤ 6) : 6/6\nP(x &gt; 6) : 0\nNote: The probability that \\(x\\) is less than or equal to 6 is 6/6, which is equal to 1. This is because the dice will land on either 1, 2, 3, 4, 5, or 6 with 100% probability. This example uses a discrete random variable, but a continuous density function can also be used for a continuous random variable.\n\n\n\n\n\n\nNote\n\n\n\nPDF → probability at a given point\nCDF → total probability up to a given point"
  },
  {
    "objectID": "posts/Cyber security risk assessments with triangular distributions/CS RA w TD.html",
    "href": "posts/Cyber security risk assessments with triangular distributions/CS RA w TD.html",
    "title": "Cyber security risk assessments with triangular distributions",
    "section": "",
    "text": "In a quantitative risk calculation, a triangle distribution is a probability distribution that assumes a minimum, maximum, and most likely value for a variable. It is often used when the available data is limited, and we have some knowledge about the range of possible values."
  },
  {
    "objectID": "posts/Cyber security risk assessments with triangular distributions/CS RA w TD.html#example",
    "href": "posts/Cyber security risk assessments with triangular distributions/CS RA w TD.html#example",
    "title": "Cyber security risk assessments with triangular distributions",
    "section": "Example",
    "text": "Example\nCompany XYZ has assessed the potential financial impact of a data breach. Based on historical data and expert judgment, they estimate the following values for the financial impact (in USD) with a triangle distribution:\n\nMinimum financial impact: a = USD 40,000\nMost likely financial impact (mode): c = USD 150,000\nMaximum financial impact: b = USD 60,000\n\n\n\n\n\n\nResults\nThe triangle distribution parameters are:\n\nExpected value: USD 85,000\nProbability of the expected value: 0.553 (55%)\nConfidence Interval [lower bound]: USD 51,000\nConfidence Interval [upper band]: USD 135,000\n\nInterpretation\n\nThe 95% confidence interval covers almost the entire range of the triangular distribution.\nThe probability of the expected value is only 55%.\n\nThese two conclusions tell us that the expected value is characterized by a high degree of uncertainty which is typical of a skewed triangular distribution.\n\n\n\n\n\n\nNote\n\n\n\nConfidence Interval Various methods can be used to calculate a confidence interval for the triangular distribution. It is important to note that these methods are based on assumptions and may not always be accurate, especially if the triangular distribution is highly asymmetric. In such cases, more advanced statistical methods may be required."
  },
  {
    "objectID": "posts/Cyber security risk assessments with triangular distributions/CS RA w TD.html#example-1",
    "href": "posts/Cyber security risk assessments with triangular distributions/CS RA w TD.html#example-1",
    "title": "Cyber security risk assessments with triangular distributions",
    "section": "Example",
    "text": "Example\n…\n\nFigure 3: Linear pooling vs. logarithmic pooling"
  },
  {
    "objectID": "posts/Cyber security risk assessments with triangular distributions/CS RA w TD.html#linear-opinion-pooling-lop",
    "href": "posts/Cyber security risk assessments with triangular distributions/CS RA w TD.html#linear-opinion-pooling-lop",
    "title": "Cyber security risk assessments with triangular distributions",
    "section": "Linear opinion pooling (LOP)",
    "text": "Linear opinion pooling (LOP)\nThe results of triangular distributions can be aggregated by combining multiple triangular distributions to represent the overall uncertainty. This can be done by taking the minimum of the minimum values, the maximum of the maximum values, and aggregating the modes to form a new triangular distribution that represents the combined uncertainty. One approach to this is opinion pooling.\nThe following formula can be used for linear opinion pooling: \\[LOP = \\sum_{i=1}^{N} (w_i \\times o_i)\\] where:\n\n\\(N\\) is the number of experts resp. opinions\n\\(o_i\\) is the opinion of expert \\(i\\)\n\\(w_i\\) is the weight of expert \\(i\\) (used to reflect the trustworthiness of expert \\(i\\))\n\nNote: \\(w_i\\) are the non-negative weights such that \\(\\sum_{i} w_i = 1\\), with \\(w_1= w_2= \\ldots = w_k = 1/k\\) if all the opinions as equally trustworthy.\nIf no weighting is used: \\[LOP = \\frac {1}{N} \\sum_{i=1}^{N} o_i\\]"
  },
  {
    "objectID": "posts/Cyber security risk assessments with triangular distributions/CS RA w TD.html#logarithmic-opinion-pooling-logop",
    "href": "posts/Cyber security risk assessments with triangular distributions/CS RA w TD.html#logarithmic-opinion-pooling-logop",
    "title": "Cyber security risk assessments with triangular distributions",
    "section": "Logarithmic opinion pooling (LogOP)",
    "text": "Logarithmic opinion pooling (LogOP)\nLogarithmic opinion pooling is a method used to combine opinions from multiple experts. The basic idea is to take the logarithm of the individual probabilities and then average them. This helps prevent extreme opinions from dominating the final result.\nThe following formula can be used for logarithmic opinion pooling: \\[LogOp = \\exp \\left(\\sum_{i=1}^{N} w_i \\times \\ln(o_i) \\right)\\] where:\n\n\\(N\\) is the number of experts resp. opinions\n\\(o_i\\) is the opinion of expert \\(i\\)\n\\(w_i\\) is the weight of expert \\(i\\) (used to reflect the trustworthiness of expert \\(i\\))\n\nIf no weighting is used: \\[LogOp = \\frac{1}{N} \\exp \\left(\\sum_{i=1}^{N} \\ln(o_i) \\right)\\]"
  },
  {
    "objectID": "posts/Cyber security risk assessments with triangular distributions/CS RA w TD.html#linear-opinion-pooling-vs.-logarithmic-opinion-pooling",
    "href": "posts/Cyber security risk assessments with triangular distributions/CS RA w TD.html#linear-opinion-pooling-vs.-logarithmic-opinion-pooling",
    "title": "Cyber security risk assessments with triangular distributions",
    "section": "Linear opinion pooling vs. logarithmic opinion pooling",
    "text": "Linear opinion pooling vs. logarithmic opinion pooling\nLinear opinion pooling and logarithmic opinion pooling are two methods used in combining multiple opinions to form a collective opinion. The choice between these methods depends on the characteristics of the underlying beliefs and the desired properties of the aggregated opinion.\n1. Linear Opinion Pooling (LOP)\n\nApplicability: Linear pooling is often suitable when the underlying opinions are assumed to be independent and equally weighted.\nCharacteristics: This method assumes that each opinion contributes equally to the collective belief. It is straightforward and easy to implement.\nUse Cases: Linear pooling may be appropriate when there is no reason to believe that any source of information is more reliable or more influential than others.\n\n2. Logarithmic Opinion Pooling (LogOP)\n\nApplicability: Logarithmic pooling is more suitable when there is a need to underweight extreme opinions and give more importance to the middle-ground opinions.\nCharacteristics: It is based on the assumption that extreme opinions (outliners) are more likely to be incorrect or biased, and thus they should have less impact on the final aggregated opinion. Note that the logarithmic pooling can produce an aggregated probability that is zero, even if none of the experts assign a probability of zero to the outcome. This is because the logarithmic function is unbounded below.\nUse Cases: Logarithmic pooling might be more appropriate in situations where there is a possibility of extreme opinions being outliers or influenced by noise, and a more conservative approach to combining opinions is desired.\n\nThe choice between linear and logarithmic opinion pooling depends on the context and the specific characteristics of the data. Linear and logarithmic opinion pooling are not the only methods available. Other approaches such as Bayesian aggregation, may also be considered.\n\n[!NOTE] Probabilistic Opinion Pooling Probabilistic Opinion Pooling is a concept developed by Franz Dietrich. It refers to methods for combining the opinions or judgments of multiple individuals and generating a joint probability distribution for a given statement or hypothesis. Dietrich is a researcher in the field of decision theory and formal social choice theory.\n\n\nLOP Example\nScenario\nDDoS attack on the organization, where three experts estimate the probability of occurrence and potential impact.\nEstimations\n\nExpert A: probability = 0.7; impact = USD 100,000 =&gt; risk = USD 70,000\nExpert B: probability = 0.6; impact = USD 300,000 =&gt; risk = USD 180,000\nExpert C: probability = 0.8; impact = USD 200,000 =&gt; risk = USD 160,000\n\nTrustworthiness\nThe trustworthiness or expertise of the experts can be weighted:\n\nExpert A: \\(w_A\\) = 0.3\nExpert B: \\(w_B\\) = 0.4\nExpert C: \\(w_C\\) = 0.3\n\nNote: The sum of all weighting factors should be 1.0.\nOverall Assessment\nCalculate the overall assessment by applying the linear pooling method: \\[\\text{Overall assessment} = w_A \\times \\text{risk}_A + w_B \\times \\text{risk}_B + w_C \\times \\text{risk}_C\\] \\[\\text{Overall assessment} = 0.3 \\times \\text{USD 70,000} + 0.4 \\times \\text{USD 180,000} + 0.3 \\times \\text{USD 160,000} = \\text{USD 141,000}\\]\n\n\nLogOP Example\nUsed the same scenario and the same values for the expert estimations and trustworthiness as in the LOP example.\nThe logarithmic opinion pooling formula is: \\[LogOp(r_1,r_2,r_3)= \\exp⁡(w_1 \\times ln⁡(r_1)+w_2 \\times ln⁡(r_2)+w_3 \\times ln⁡(r_3))\\]\n\\[LogOp(70,180,160)= \\exp⁡(0.3 \\times ln⁡(70)+0.4 \\times ln⁡(180)+ 0.3 \\times ln⁡(160))\\]\n\\[LogOp(70,180,160)= \\text{USD 1k } 130.880\\]\nOverall Assessment\nThe overall assessment calculated with logarithmic pooling is USD 130,880. The result of the logarithmic pooling is compared with the linear opinion pooling lower (USD 130,880 &lt; USD 141,000) because of the effect to correct outliners (risk estimation of expert A which is significantly lower then the risks estimated by the experts B and C)."
  },
  {
    "objectID": "posts/cyber security risk assessments wit lognormal distributions/CS RA w LND.html",
    "href": "posts/cyber security risk assessments wit lognormal distributions/CS RA w LND.html",
    "title": "Cyber security risk assessments with lognormal distributions",
    "section": "",
    "text": "The lognormal distribution is a continuous probability distribution that can only take positive values. It describes the distribution of a random variable \\(X\\) if the random variable \\(Y = ln(X)\\) transformed by the logarithm is normally distributed.\n\n\n\n\n\n\n\n\n\n\n\nThe lognormal distribution is often used for operational risk management due to its ability to model the right-skewed and heavy-tailed nature of operational risks such as cyber security risks.\nHere are some of the reasons why the lognormal distribution is popular in operational risk modeling:\n\nSkewness: It captures the fat-tailed nature of operational losses.\nNon-negative values: Operational risk events are typically associated with losses, and the lognormal distribution ensures that the values generated are always positive.\nMultiplicative nature: Operational risks are often caused by multiple factors that interact in a multiplicative way. The lognormal distribution is consistent with this multiplicative model of risk, because it is a product of independent random variables.\nEmpirical evidence: The parameters of the lognormal distribution can be estimated from historical operational loss data using standard statistical techniques.\nWell accepted and understood: The lognormal distribution is an established distribution in statistics and risk management, and it is widely accepted by regulators.\nProven reliability: Studies have shown that the lognormal distribution can be used to predict operational losses with reasonable accuracy.\nPercentile Range Estimation: The lognormal distribution is suitable for estimating the percentile range of the loss distribution, which is essential for risk assessment.\n\nDespite its advantages, the lognormal distribution is not always the best choice for operational risk modeling. In some cases, other distributions, such as the Gamma or Pareto distributions, may be more appropriate.\n\n\n\nConsider a cyber risk with a lognormal distribution (parameters see below).\nWhat is the risk that the loss associated with the cyber risk will be greater than the threshold?\n\n\n\n\n\n\n\n\n\nLognormal distribution parameters: \n\n\nMean: 0  \n\n\nSigma: 1  \n\n\nThreshold: 10  \n\n\nCyber security risk: 0.0106511 \n\n\n95% confidence interval (lower boundary): 0.1408584 \n\n\n95% confidence interval (upper boundary): 7.099327"
  },
  {
    "objectID": "posts/cyber security risk assessments wit lognormal distributions/CS RA w LND.html#suitability-for-operational-risk-management",
    "href": "posts/cyber security risk assessments wit lognormal distributions/CS RA w LND.html#suitability-for-operational-risk-management",
    "title": "Cyber security risk assessments with lognormal distributions",
    "section": "",
    "text": "The lognormal distribution is often used for operational risk management due to its ability to model the right-skewed and heavy-tailed nature of operational risks such as cyber security risks.\nHere are some of the reasons why the lognormal distribution is popular in operational risk modeling:\n\nSkewness: It captures the fat-tailed nature of operational losses.\nNon-negative values: Operational risk events are typically associated with losses, and the lognormal distribution ensures that the values generated are always positive.\nMultiplicative nature: Operational risks are often caused by multiple factors that interact in a multiplicative way. The lognormal distribution is consistent with this multiplicative model of risk, because it is a product of independent random variables.\nEmpirical evidence: The parameters of the lognormal distribution can be estimated from historical operational loss data using standard statistical techniques.\nWell accepted and understood: The lognormal distribution is an established distribution in statistics and risk management, and it is widely accepted by regulators.\nProven reliability: Studies have shown that the lognormal distribution can be used to predict operational losses with reasonable accuracy.\nPercentile Range Estimation: The lognormal distribution is suitable for estimating the percentile range of the loss distribution, which is essential for risk assessment.\n\nDespite its advantages, the lognormal distribution is not always the best choice for operational risk modeling. In some cases, other distributions, such as the Gamma or Pareto distributions, may be more appropriate."
  },
  {
    "objectID": "posts/cyber security risk assessments wit lognormal distributions/CS RA w LND.html#example-1",
    "href": "posts/cyber security risk assessments wit lognormal distributions/CS RA w LND.html#example-1",
    "title": "Cyber security risk assessments with lognormal distributions",
    "section": "",
    "text": "Consider a cyber risk with a lognormal distribution (parameters see below).\nWhat is the risk that the loss associated with the cyber risk will be greater than the threshold?\n\n\n\n\n\n\n\n\n\nLognormal distribution parameters: \n\n\nMean: 0  \n\n\nSigma: 1  \n\n\nThreshold: 10  \n\n\nCyber security risk: 0.0106511 \n\n\n95% confidence interval (lower boundary): 0.1408584 \n\n\n95% confidence interval (upper boundary): 7.099327"
  },
  {
    "objectID": "posts/cyber security risk assessments with triangular distributions/CS RA w TD.html",
    "href": "posts/cyber security risk assessments with triangular distributions/CS RA w TD.html",
    "title": "Cyber security risk assessments with triangular distributions",
    "section": "",
    "text": "In a quantitative risk calculation, a triangle distribution is a probability distribution that assumes a minimum, maximum, and most likely value for a variable. It is often used when the available data is limited, and we have some knowledge about the range of possible values."
  },
  {
    "objectID": "posts/cyber security risk assessments with triangular distributions/CS RA w TD.html#example",
    "href": "posts/cyber security risk assessments with triangular distributions/CS RA w TD.html#example",
    "title": "Cyber security risk assessments with triangular distributions",
    "section": "Example",
    "text": "Example\nCompany XYZ has assessed the potential financial impact of a data breach. Based on historical data and expert judgment, they estimate the following values for the financial impact (in USD) with a triangle distribution:\n\nMinimum financial impact: a = USD 40,000\nMost likely financial impact (mode): c = USD 150,000\nMaximum financial impact: b = USD 60,000\n\n\n\n\n\n\n\n\n\n\nResults\nThe triangle distribution parameters are:\n\nExpected value: USD 85,000\nProbability of the expected value: 0.553 (55%)\nConfidence Interval [lower bound]: USD 51,000\nConfidence Interval [upper band]: USD 135,000\n\nInterpretation\n\nThe 95% confidence interval covers almost the entire range of the triangular distribution.\nThe probability of the expected value is only 55%.\n\nThese two conclusions tell us that the expected value is characterized by a high degree of uncertainty which is typical of a skewed triangular distribution.\n\n\n\n\n\n\nConfidence interval\n\n\n\nVarious methods can be used to calculate a confidence interval for the triangular distribution. It is important to note that these methods are based on assumptions and may not always be accurate, especially if the triangular distribution is highly asymmetric. In such cases, more advanced statistical methods may be required.\n\n\n\n\n\n\n\n\nExpected value\n\n\n\nThe expected value is a commonly used measurement in quantitative risk management. The expected value provides a single “numerical summary” of a probability distribution. However, it is important to note that relying solely on the expected value may have limitations. The probability of the expected value, the confidence interval, and a good understanding of the underlying probability distribution help to better understand the risk profile and make more comprehensive risk management decisions."
  },
  {
    "objectID": "posts/cyber security risk assessments with triangular distributions/CS RA w TD.html#linear-opinion-pooling-lop",
    "href": "posts/cyber security risk assessments with triangular distributions/CS RA w TD.html#linear-opinion-pooling-lop",
    "title": "Cyber security risk assessments with triangular distributions",
    "section": "Linear opinion pooling (LOP)",
    "text": "Linear opinion pooling (LOP)\nThe results of triangular distributions can be aggregated by combining multiple triangular distributions to represent the overall uncertainty. This can be done by taking the minimum of the minimum values, the maximum of the maximum values, and aggregating the modes to form a new triangular distribution that represents the combined uncertainty. One approach to this is opinion pooling.\nThe following formula can be used for linear opinion pooling: \\[LOP = \\sum_{i=1}^{N} (w_i \\times o_i)\\] where:\n\n\\(N\\) is the number of experts resp. opinions\n\\(o_i\\) is the opinion of expert \\(i\\)\n\\(w_i\\) is the weight of expert \\(i\\) (used to reflect the trustworthiness of expert \\(i\\))\n\nNote: \\(w_i\\) are the non-negative weights such that \\(\\sum_{i} w_i = 1\\), with \\(w_1= w_2= \\ldots = w_k = 1/k\\) if all the opinions as equally trustworthy.\nIf no weighting is used: \\[LOP = \\frac {1}{N} \\sum_{i=1}^{N} o_i\\]"
  },
  {
    "objectID": "posts/cyber security risk assessments with triangular distributions/CS RA w TD.html#logarithmic-opinion-pooling-logop",
    "href": "posts/cyber security risk assessments with triangular distributions/CS RA w TD.html#logarithmic-opinion-pooling-logop",
    "title": "Cyber security risk assessments with triangular distributions",
    "section": "Logarithmic opinion pooling (LogOP)",
    "text": "Logarithmic opinion pooling (LogOP)\nLogarithmic opinion pooling is a method used to combine opinions from multiple experts. The basic idea is to take the logarithm of the individual probabilities and then average them. This helps prevent extreme opinions from dominating the final result.\nThe following formula can be used for logarithmic opinion pooling: \\[LogOp = \\exp \\left(\\sum_{i=1}^{N} w_i \\times \\ln(o_i) \\right)\\] where:\n\n\\(N\\) is the number of experts resp. opinions\n\\(o_i\\) is the opinion of expert \\(i\\)\n\\(w_i\\) is the weight of expert \\(i\\) (used to reflect the trustworthiness of expert \\(i\\))\n\nIf no weighting is used: \\[LogOp = \\frac{1}{N} \\exp \\left(\\sum_{i=1}^{N} \\ln(o_i) \\right)\\]"
  },
  {
    "objectID": "posts/cyber security risk assessments with triangular distributions/CS RA w TD.html#linear-opinion-pooling-vs.-logarithmic-opinion-pooling",
    "href": "posts/cyber security risk assessments with triangular distributions/CS RA w TD.html#linear-opinion-pooling-vs.-logarithmic-opinion-pooling",
    "title": "Cyber security risk assessments with triangular distributions",
    "section": "Linear opinion pooling vs. logarithmic opinion pooling",
    "text": "Linear opinion pooling vs. logarithmic opinion pooling\nLinear opinion pooling and logarithmic opinion pooling are two methods used in combining multiple opinions to form a collective opinion. The choice between these methods depends on the characteristics of the underlying beliefs and the desired properties of the aggregated opinion.\n1. Linear Opinion Pooling (LOP)\n\nApplicability: Linear pooling is often suitable when the underlying opinions are assumed to be independent and equally weighted.\nCharacteristics: This method assumes that each opinion contributes equally to the collective belief. It is straightforward and easy to implement.\nUse Cases: Linear pooling may be appropriate when there is no reason to believe that any source of information is more reliable or more influential than others.\n\n2. Logarithmic Opinion Pooling (LogOP)\n\nApplicability: Logarithmic pooling is more suitable when there is a need to underweight extreme opinions and give more importance to the middle-ground opinions.\nCharacteristics: It is based on the assumption that extreme opinions (outliners) are more likely to be incorrect or biased, and thus they should have less impact on the final aggregated opinion. Note that the logarithmic pooling can produce an aggregated probability that is zero, even if none of the experts assign a probability of zero to the outcome. This is because the logarithmic function is unbounded below.\nUse Cases: Logarithmic pooling might be more appropriate in situations where there is a possibility of extreme opinions being outliers or influenced by noise, and a more conservative approach to combining opinions is desired.\n\nThe choice between linear and logarithmic opinion pooling depends on the context and the specific characteristics of the data. Linear and logarithmic opinion pooling are not the only methods available. Other approaches such as Bayesian aggregation, may also be considered.\n\n\n\n\n\n\nProbabilistic opinion\n\n\n\nPooling is a concept developed by Franz Dietrich. It refers to methods for combining the opinions or judgments of multiple individuals and generating a joint probability distribution for a given statement or hypothesis. Dietrich is a researcher in the field of decision theory and formal social choice theory.\n\n\n\nLOP Example\nScenario\nDDoS attack on the organization, where three experts estimate the probability of occurrence and potential impact.\nEstimations\n\nExpert A: probability = 0.7; impact = USD 100,000 =&gt; risk = USD 70,000\nExpert B: probability = 0.6; impact = USD 300,000 =&gt; risk = USD 180,000\nExpert C: probability = 0.8; impact = USD 200,000 =&gt; risk = USD 160,000\n\nTrustworthiness\nThe trustworthiness or expertise of the experts can be weighted:\n\nExpert A: \\(w_A\\) = 0.3\nExpert B: \\(w_B\\) = 0.4\nExpert C: \\(w_C\\) = 0.3\n\nNote: The sum of all weighting factors should be 1.0.\nOverall Assessment\nCalculate the overall assessment by applying the linear pooling method: \\[\\text{Overall assessment} = w_A \\times \\text{risk}_A + w_B \\times \\text{risk}_B + w_C \\times \\text{risk}_C\\] \\[\\text{Overall assessment} = 0.3 \\times \\text{USD 70,000} + 0.4 \\times \\text{USD 180,000} + 0.3 \\times \\text{USD 160,000} = \\text{USD 141,000}\\]\n\n\nLogOP Example\nUsed the same scenario and the same values for the expert estimations and trustworthiness as in the LOP example.\nThe logarithmic opinion pooling formula is: \\[LogOp(r_1,r_2,r_3)= \\exp⁡(w_1 \\times ln⁡(r_1)+w_2 \\times ln⁡(r_2)+w_3 \\times ln⁡(r_3))\\]\n\\[LogOp(70,180,160)= \\exp⁡(0.3 \\times ln⁡(70)+0.4 \\times ln⁡(180)+ 0.3 \\times ln⁡(160))\\]\n\\[LogOp(70,180,160)= \\text{USD 1k } 130.880\\]\nOverall Assessment\nThe overall assessment calculated with logarithmic pooling is USD 130,880. The result of the logarithmic pooling is compared with the linear opinion pooling lower (USD 130,880 &lt; USD 141,000) because of the effect to correct outliners (risk estimation of expert A which is significantly lower then the risks estimated by the experts B and C)."
  },
  {
    "objectID": "posts/return on security investment (ROSI)/ROSI.html",
    "href": "posts/return on security investment (ROSI)/ROSI.html",
    "title": "Return on Security Investment (ROSI)",
    "section": "",
    "text": "Return on Security Investment (ROSI) is used to measure the effectiveness of security investments.\nROSI is the ratio of the monetary benefit of a security investment to its cost. A positive ROSI indicates that the investment has generated a positive net return, while a negative ROSI indicates that the investment has resulted in a net loss.\nROSI can help organizations determine which security investments are worth the cost and which are not and, allocate resources to effective security solutions.\n\nFigure 1: Balance between costs, agility/innovation and risk\n\n\nThe return on investment (ROI) equation only works for investments that produce positive results, such as cost savings or increased revenue. Cyber security spending does not increase revenue, nor does it provide a payback. Rather, it is risk management that contributes to loss prevention and risk mitigation. The ROSI therefore aims to calculate how much loss an organization can avoid by investing in its security. The formula for this is therefore different from the ROI calculation.\n\\[ROI = \\frac{Return}{Investment}\\]\n\\[ROSI = \\frac{\\text{Financial Loss Reduction - Cost of Security Solution}}{\\text{Cost of Security Solution}}\\]\nImplementing a security solution reduces the ALE expressed by the mitigation ratio.\n\\[ROSI = \\frac{\\text{ALE * Mitigation Ratio - Cost of Security Solution}}{\\text{Cost of Security Solution}}\\]\nWhere:\n\nSingle Loss Expectancy (SLE): expected financial loss when a risk occurs\nAnnual Rate of Occurrence (ARO): probability that a risk occurs in a year\nAnnualized Loss Expectancy (ALE): total financial loss per year resulting from a specific risks \\[ALE = SLE * ARO\\]\n\nProcess steps of ROSI calculation\n\nDetermine the cost of security solution\nEstimate the single loss expectancy (SLE)\nEstimate the annual rate of occurrence (ARO)\nCalculate the financial loss reduction based on ALE and mitigation ratio\nCalculate the ROSI value\n\n\n\n\n\nEstimating both single loss expectancy (SLE) and annual occurrence rate (ARO) is challenging and often subject to bias.\nCalculating the cost of a security solution requires a solid understanding of the one-time cost of implementation and the recurring cost of operation.\nROSI does not support multi-year budget view.\nROSI considers only the financial aspects of risk mitigation. In practice, other factors such as reputation damage are also relevant.\n\n\n\n\nExample 1\nA company is considering investing in an anti-malware solution. Each year, the company suffers 5 malware attacks (ARO=5). The CISO estimates that each attacks cost approximately USD 15,000 in loss of data and productivity (SLE=15,000). The anti-malware solution is expected to block 80% of the malware attacks (Mitigation ratio=80%) and costs USD 25,000 per year (license fees USD 15,000 + USD 10,000 for operations). The return on security investment (ROSI) for this solution is then calculated as follow:\n\\[ROSI = \\frac{(5 * 15,000)*0.8-25,000}{25,000}= 1.4\\]\nConclusion The investment in the anti-malware solution will save the company USD 35,000 per year, which corresponds to an amortization of 140%. The ROSI calculation shows that the anti-malware solution is a cost-effective solution.\nExample 2\nA company has 500 employees. They all have use notebooks for their daily work. The company protects the notebooks with an end-point security solution.\nAssumptions\n\nThe number of notebooks infected with malware per year is 6%, or 30 notebooks.\nFinancial loss due to malware infection is 30 * USD 10,000 = USD 300,000\nThe end-point security software prevents from 80% of the malware attacks\nOne-time license cost for end-point security software is 500 * USD 110 = USD 55,000\nAnnual operating cost is USD 10,000\n\nWhere:\n\nSingle Loss Expectancy (SLE): USD 10,000\nAnnual Rate of Occurrence (ARO): 6%\nAnnualized Loss Expectancy (ALE): \\(ALE = SLE * ARO = \\text{USD 300,000}\\)\nCost of Security Solution: USD 55,000 + USD 10,000 = USD 65,000\nMitigation Ratio: 80%\n\n\\[ROSI = \\frac{\\text{ALE * Mitigation Ratio - Cost of Security Solution}}{\\text{Cost of Security Solution}}\\]\n\\[ROSI = \\frac{\\text{USD 300,000 * 0.8} - \\text{USD 65,000}}{\\text{USD 65.000}} = 2.69\\]\nConclusion\nThe investment in the end-point security solution will save the company USD 174,850 per year, which corresponds to an amortization of 269%. The ROSI calculation shows that the end-point protection is a cost-effective solution.\n\n\n\n\n\nThe balanced scorecard is a method that considers multiple perspectives, such as financial, customer, internal process, and learning and growth for strategic management. The balanced scorecard can help organizations to align their security investments with their overall business strategy.\nKey performance indicators (KPI) are can be used to measure the effectiveness of each perspective. ROSI can be used as key performance indicators (KPI) to measure the financial effectiveness (i.e., contribution to the overall business strategy) of security investments.\nExample\nSuppose an organization uses a balanced scorecard to evaluate the effectiveness of its email security investment using the following key performance indicators (KPI):\n\nFinancial perspective: ROSI of email security solution\nCustomer perspective: Customer satisfaction with email security measures\nInternal process perspective: Time to detect and respond to email security incidents\nLearning and growth perspective: Employees participation in training and awareness of email security best practices\n\nThese KPI provide a comprehensive view of the effectiveness of the email security investment, and can help the organization to identify areas for improvement and optimize its security strategy over time. The balanced scorecard method is a valuable extension of the ROSI calculation, providing a holistic view at the strategic level.\n\n\n\nThe Net Present Value (NPV) calculates the present value of the net benefits of an investment over time. NPV takes into account the time value of money and the changing benefits and costs of a security investment over time. \\[NPV = \\sum_{t=0}^{T} \\frac{CF_t}{(1+r)^t} - C_0\\] Where:\n\n\\(NPV\\) is the Net Present Value\n\\(T\\) is the number of periods (usually years)\n\\(CF_t\\) is the net cash flow during the period ( t )\n\\(r\\) is the discount rate (the rate of return required by the investor)\n\\(C_0\\) is the initial investment cost\n\nIf the NPV is positive, it suggests that the investment is expected to generate more cash inflows than outflows, indicating a potentially profitable opportunity. Conversely, if the NPV is negative, it suggests that the investment may not meet the required rate of return and may not be economically viable.\nConclusion\n\nNPV provides a time-based analysis of the profitability of an investment.\nROSI is a simple ratio and does not consider the timing of cash flows or the cost of capital.\nSecurity investments do not generate revenue or losses. They prevent from losses. Therefore, the NPV method cannot be applied to security investments.\n\n\n\n\nThe Gordon-Loeb method is a mathematical economic model used to determine the optimal level of investment in information security.\nThe basic components of the Gordon-Loeb model are as follows:\n\nAssets that are vulnerable to cyber attacks. This vulnerability \\(v\\) (0 ≤ v ≤ 1) corresponds to the probability of a successful cyber attack.\nThe potential loss is denoted as \\(L\\). Thus, \\(vL\\) is the expected loss prior to an investment in additional cyber security measures.\nThe cyber security investment \\(z\\), will reduce \\(v\\). \\[z(v) ≤ \\frac{1}{e} vL\\]\n\nGordon and Loeb were able to show that the optimal level of cyber security investments would not exceed \\(\\frac{vL}{e}\\) or approx. 37% of the expected loss from a security attack (\\(vL\\)).\n\nFigure 2: The optimal level of cyber security investment at \\(z^*\\)\nExample\nSuppose an estimated data value of  USD 1,000,000, with a cyber attack probability of 15%, and an 80% chance that an attack would be successful.\nIn this case, the potential loss is given by the product USD 1,000,000  ×  0.15  ×  0.8 = USD 120,000.\nAccording to Gordon and Loeb, the company’s investment in security should not exceed USD 120,000  ×  0.37 = USD 44,000.\nLimitations of the Gordon-Loeb model\nWhile the Gordon-Loeb model is a valuable contribution to the field of cyber security economics, it is important to understand its limitations.\n\nSimplifying assumptions: The model relies on simplifying assumptions, such as the linearity of the relationship between investment and risk reduction. The optimum of security investments is determined by the mathematical derivative of the investment function. In reality, the relationship may be more complex and context-dependent.\nRisk appetite and industry differences: Organizations vary in their risk appetite, and different industries may have distinct risk profiles. What is an appropriate investment level for one organization or industry may not be suitable for another. Factors such as legal and regulatory requirements and the organization’s risk appetite need to be considered.\nDynamic nature of cyber threats: Cyber threats evolve rapidly, and the threat landscape is dynamic. The model assumes a static environment, which may not capture the changing nature of cyber risks. Organizations need to adapt their security investments to address emerging threats.\nIncomplete information: Calculating the expected loss is challenging, as it requires accurate data on potential losses from security incidents. In practice, organizations may face difficulties in obtaining comprehensive and reliable data for such calculations.\nBeyond financial considerations: The model primarily focuses on financial aspects and may not fully capture non-financial factors, such as reputational damage, and customer trust.\n\nThe limitations of points 3, 4, and 5 apply not only to the Gordon-Loeb model, but also to ROSI. The Gordon-Loeb model can be used as a starting point rather than a universal rule.\n\n\n\n\n\n\nDependence on mathematical approach\n\n\n\nResearch showed that even within the initial assumptions of the model, some security incident probability functions should be fixed with no less than \\(\\frac{1}{2}\\) the expected loss, contradicting the hypothesis that the \\(\\frac{1}{e}\\) factor was universal. Using a different mathematical approach (the second derivative of the loss function need not to be continuous), one can create loss functions whose optimal fixing costs 100% of the estimated loss."
  },
  {
    "objectID": "posts/return on security investment (ROSI)/ROSI.html#roi-vs.-rosi",
    "href": "posts/return on security investment (ROSI)/ROSI.html#roi-vs.-rosi",
    "title": "Return on Security Investment (ROSI)",
    "section": "",
    "text": "The return on investment (ROI) equation only works for investments that produce positive results, such as cost savings or increased revenue. Cyber security spending does not increase revenue, nor does it provide a payback. Rather, it is risk management that contributes to loss prevention and risk mitigation. The ROSI therefore aims to calculate how much loss an organization can avoid by investing in its security. The formula for this is therefore different from the ROI calculation.\n\\[ROI = \\frac{Return}{Investment}\\]\n\\[ROSI = \\frac{\\text{Financial Loss Reduction - Cost of Security Solution}}{\\text{Cost of Security Solution}}\\]\nImplementing a security solution reduces the ALE expressed by the mitigation ratio.\n\\[ROSI = \\frac{\\text{ALE * Mitigation Ratio - Cost of Security Solution}}{\\text{Cost of Security Solution}}\\]\nWhere:\n\nSingle Loss Expectancy (SLE): expected financial loss when a risk occurs\nAnnual Rate of Occurrence (ARO): probability that a risk occurs in a year\nAnnualized Loss Expectancy (ALE): total financial loss per year resulting from a specific risks \\[ALE = SLE * ARO\\]\n\nProcess steps of ROSI calculation\n\nDetermine the cost of security solution\nEstimate the single loss expectancy (SLE)\nEstimate the annual rate of occurrence (ARO)\nCalculate the financial loss reduction based on ALE and mitigation ratio\nCalculate the ROSI value"
  },
  {
    "objectID": "posts/return on security investment (ROSI)/ROSI.html#examples",
    "href": "posts/return on security investment (ROSI)/ROSI.html#examples",
    "title": "Return on Security Investment (ROSI)",
    "section": "",
    "text": "Example 1\nA company is considering investing in an anti-malware solution. Each year, the company suffers 5 malware attacks (ARO=5). The CISO estimates that each attacks cost approximately USD 15,000 in loss of data and productivity (SLE=15,000). The anti-malware solution is expected to block 80% of the malware attacks (Mitigation ratio=80%) and costs USD 25,000 per year (license fees USD 15,000 + USD 10,000 for operations). The return on security investment (ROSI) for this solution is then calculated as follow:\n\\[ROSI = \\frac{(5 * 15,000)*0.8-25,000}{25,000}= 1.4\\]\nConclusion The investment in the anti-malware solution will save the company USD 35,000 per year, which corresponds to an amortization of 140%. The ROSI calculation shows that the anti-malware solution is a cost-effective solution.\nExample 2\nA company has 500 employees. They all have use notebooks for their daily work. The company protects the notebooks with an end-point security solution.\nAssumptions\n\nThe number of notebooks infected with malware per year is 6%, or 30 notebooks.\nFinancial loss due to malware infection is 30 * USD 10,000 = USD 300,000\nThe end-point security software prevents from 80% of the malware attacks\nOne-time license cost for end-point security software is 500 * USD 110 = USD 55,000\nAnnual operating cost is USD 10,000\n\nWhere:\n\nSingle Loss Expectancy (SLE): USD 10,000\nAnnual Rate of Occurrence (ARO): 6%\nAnnualized Loss Expectancy (ALE): \\(ALE = SLE * ARO = \\text{USD 300,000}\\)\nCost of Security Solution: USD 55,000 + USD 10,000 = USD 65,000\nMitigation Ratio: 80%\n\n\\[ROSI = \\frac{\\text{ALE * Mitigation Ratio - Cost of Security Solution}}{\\text{Cost of Security Solution}}\\]\n\\[ROSI = \\frac{\\text{USD 300,000 * 0.8} - \\text{USD 65,000}}{\\text{USD 65.000}} = 2.69\\]\nConclusion\nThe investment in the end-point security solution will save the company USD 174,850 per year, which corresponds to an amortization of 269%. The ROSI calculation shows that the end-point protection is a cost-effective solution."
  },
  {
    "objectID": "posts/return on security investment (ROSI)/ROSI.html#alternative-methods",
    "href": "posts/return on security investment (ROSI)/ROSI.html#alternative-methods",
    "title": "Return on Security Investment (ROSI)",
    "section": "",
    "text": "The balanced scorecard is a method that considers multiple perspectives, such as financial, customer, internal process, and learning and growth for strategic management. The balanced scorecard can help organizations to align their security investments with their overall business strategy.\nKey performance indicators (KPI) are can be used to measure the effectiveness of each perspective. ROSI can be used as key performance indicators (KPI) to measure the financial effectiveness (i.e., contribution to the overall business strategy) of security investments.\nExample\nSuppose an organization uses a balanced scorecard to evaluate the effectiveness of its email security investment using the following key performance indicators (KPI):\n\nFinancial perspective: ROSI of email security solution\nCustomer perspective: Customer satisfaction with email security measures\nInternal process perspective: Time to detect and respond to email security incidents\nLearning and growth perspective: Employees participation in training and awareness of email security best practices\n\nThese KPI provide a comprehensive view of the effectiveness of the email security investment, and can help the organization to identify areas for improvement and optimize its security strategy over time. The balanced scorecard method is a valuable extension of the ROSI calculation, providing a holistic view at the strategic level.\n\n\n\nThe Net Present Value (NPV) calculates the present value of the net benefits of an investment over time. NPV takes into account the time value of money and the changing benefits and costs of a security investment over time. \\[NPV = \\sum_{t=0}^{T} \\frac{CF_t}{(1+r)^t} - C_0\\] Where:\n\n\\(NPV\\) is the Net Present Value\n\\(T\\) is the number of periods (usually years)\n\\(CF_t\\) is the net cash flow during the period ( t )\n\\(r\\) is the discount rate (the rate of return required by the investor)\n\\(C_0\\) is the initial investment cost\n\nIf the NPV is positive, it suggests that the investment is expected to generate more cash inflows than outflows, indicating a potentially profitable opportunity. Conversely, if the NPV is negative, it suggests that the investment may not meet the required rate of return and may not be economically viable.\nConclusion\n\nNPV provides a time-based analysis of the profitability of an investment.\nROSI is a simple ratio and does not consider the timing of cash flows or the cost of capital.\nSecurity investments do not generate revenue or losses. They prevent from losses. Therefore, the NPV method cannot be applied to security investments.\n\n\n\n\nThe Gordon-Loeb method is a mathematical economic model used to determine the optimal level of investment in information security.\nThe basic components of the Gordon-Loeb model are as follows:\n\nAssets that are vulnerable to cyber attacks. This vulnerability \\(v\\) (0 ≤ v ≤ 1) corresponds to the probability of a successful cyber attack.\nThe potential loss is denoted as \\(L\\). Thus, \\(vL\\) is the expected loss prior to an investment in additional cyber security measures.\nThe cyber security investment \\(z\\), will reduce \\(v\\). \\[z(v) ≤ \\frac{1}{e} vL\\]\n\nGordon and Loeb were able to show that the optimal level of cyber security investments would not exceed \\(\\frac{vL}{e}\\) or approx. 37% of the expected loss from a security attack (\\(vL\\)).\n\nFigure 2: The optimal level of cyber security investment at \\(z^*\\)\nExample\nSuppose an estimated data value of  USD 1,000,000, with a cyber attack probability of 15%, and an 80% chance that an attack would be successful.\nIn this case, the potential loss is given by the product USD 1,000,000  ×  0.15  ×  0.8 = USD 120,000.\nAccording to Gordon and Loeb, the company’s investment in security should not exceed USD 120,000  ×  0.37 = USD 44,000.\nLimitations of the Gordon-Loeb model\nWhile the Gordon-Loeb model is a valuable contribution to the field of cyber security economics, it is important to understand its limitations.\n\nSimplifying assumptions: The model relies on simplifying assumptions, such as the linearity of the relationship between investment and risk reduction. The optimum of security investments is determined by the mathematical derivative of the investment function. In reality, the relationship may be more complex and context-dependent.\nRisk appetite and industry differences: Organizations vary in their risk appetite, and different industries may have distinct risk profiles. What is an appropriate investment level for one organization or industry may not be suitable for another. Factors such as legal and regulatory requirements and the organization’s risk appetite need to be considered.\nDynamic nature of cyber threats: Cyber threats evolve rapidly, and the threat landscape is dynamic. The model assumes a static environment, which may not capture the changing nature of cyber risks. Organizations need to adapt their security investments to address emerging threats.\nIncomplete information: Calculating the expected loss is challenging, as it requires accurate data on potential losses from security incidents. In practice, organizations may face difficulties in obtaining comprehensive and reliable data for such calculations.\nBeyond financial considerations: The model primarily focuses on financial aspects and may not fully capture non-financial factors, such as reputational damage, and customer trust.\n\nThe limitations of points 3, 4, and 5 apply not only to the Gordon-Loeb model, but also to ROSI. The Gordon-Loeb model can be used as a starting point rather than a universal rule.\n\n\n\n\n\n\nDependence on mathematical approach\n\n\n\nResearch showed that even within the initial assumptions of the model, some security incident probability functions should be fixed with no less than \\(\\frac{1}{2}\\) the expected loss, contradicting the hypothesis that the \\(\\frac{1}{e}\\) factor was universal. Using a different mathematical approach (the second derivative of the loss function need not to be continuous), one can create loss functions whose optimal fixing costs 100% of the estimated loss."
  },
  {
    "objectID": "posts/return on security investment (ROSI)/ROSI.html#limitations-of-rosi",
    "href": "posts/return on security investment (ROSI)/ROSI.html#limitations-of-rosi",
    "title": "Return on Security Investment (ROSI)",
    "section": "",
    "text": "Estimating both single loss expectancy (SLE) and annual occurrence rate (ARO) is challenging and often subject to bias.\nCalculating the cost of a security solution requires a solid understanding of the one-time cost of implementation and the recurring cost of operation.\nROSI does not support multi-year budget view.\nROSI considers only the financial aspects of risk mitigation. In practice, other factors such as reputation damage are also relevant."
  },
  {
    "objectID": "posts/risk reporting/risk reporting.html",
    "href": "posts/risk reporting/risk reporting.html",
    "title": "Risk reporting",
    "section": "",
    "text": "Risk reporting\n…\n\n\nRisk aggregation\nThere are a several methods to aggregate methods:\n\nOpinion pooling: Calculates the weighted average of the individual distributions with linear pooling or logarithmic pooling.\nMonte Carlo simulation: Generate a large number of random samples from each triangular distribution and combining them to obtain an aggregated result.\nConvolution: Combines probability density functions (PDF) of individual distributions to obtain a distribution representing the sum of them.\nBayesian inference: Bayesian inference can be used when the distributions have different levels of uncertainty.\n\n\n\n\n\n\n\n\n\n\nMethod\nDescription\nAdvantages\nDisadvantages\n\n\n\n\nOpinion pooling\nCalculates the average of teach triangular distribution.\nSimple and straightforward\nMay not be accurate if distributions have different shapes\n\n\nMonte Carlo simulation\nSimulates random samples from each triangular distribution and combines them to form an aggregated distribution.\nSimple method, but requires computer power\nComputationally intensive\n\n\nConvolution\nMathematical operation on two functions (\\(f\\) and \\(g\\)) that produces a third function (\\(f*g\\)).\nComplex method which requires higher mathematical knowledge\nComputational complexity, sensitivity to assumptions\n\n\nBayesian inference\nCombine the information from multiple triangular distributions into a single posterior distribution.\nFlexibility, can handle small sample sizes\nIn many cases, analytical solutions for Bayesian models may not exist\n\n\n\nTable 1: Key characteristics of each aggregation method\nIn a subsequent posts, convolution and Bayesian inference are discussed as methods for aggregating cyber security risks."
  },
  {
    "objectID": "posts/risk reporting/risk reporting.html#expected-value",
    "href": "posts/risk reporting/risk reporting.html#expected-value",
    "title": "Risk reporting",
    "section": "",
    "text": "Q: Is the expected value a useful, appropriate measurement for quantitative risk management?\nA: Yes, the expected value is a commonly used measurement in quantitative risk management. The expected value provides a single “numerical summary” of a probability distribution. However, it is important to note that relying solely on the expected value may have limitations. The probability of the expected value, the confidence interval, and a good understanding of the underlying probability distribution help to better understand the risk profile and make more comprehensive risk management decisions."
  }
]