[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog is about cyber security and risk management. Blog posts will be published in a loose series.\nThe following tools were used to create this blog:\n\nQuarto (open source publishing system)\nRStudio for web authoring\nGitHub for version control\nNetlify for web hosting\n\nThe initial setup took place on June 2, 2023\n\nDisclaimer of liability\nThe author reserves the right not to be responsible for the topicality, correctness, completeness or quality of the information provided. Liability claims regarding damage caused by the use of any information provided, including any kind of information which is incomplete or incorrect, will therefore be rejected.\nParts of the pages or the complete publication including all offers and information might be extended, changed or partly or completely deleted by the author without separate announcement.\n\n\nReferrals and links\nLinks to third party websites are provided for convenience only and do not imply any responsibility for or approval of the information published on those other websites.\n\n\nCopyright Notice\nThe entire content of this website is licensed under a Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0).\n\n\nPrivacy Policy\nThis website does not use any cookies. By visiting this website, information about the access (date, time, page views) may be stored on the server. Personal data may be provided voluntarily (e.g. by comments)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Beta Blog",
    "section": "",
    "text": "Bootstrapping\n\n\n\n\n\n\n\nstatistics\n\n\nrisk management\n\n\n\n\nThe bootstrap method is a statistical resampling technique used to estimate the properties of a population by repeatedly sampling with replacement from a given data set.\n\n\n\n\n\n\nAug 1, 2023\n\n\nJohannes Buck\n\n\n\n\n\n\n  \n\n\n\n\nConfidence Intervall\n\n\n\n\n\n\n\nstatistics\n\n\nrisk management\n\n\n\n\nA confidence interval is a statistical interval that contains the location of a true parameter of a population with a certain probability.\n\n\n\n\n\n\nJul 30, 2023\n\n\nJohannes Buck\n\n\n\n\n\n\n  \n\n\n\n\nTriangle Distribution\n\n\n\n\n\n\n\nstatistics\n\n\nrisk management\n\n\n\n\nThe triangular distribution (also known as the Simpson distribution) is a continuous probability distribution. The graph of the probability density function looks like a triangle and gives this distribution its name.\n\n\n\n\n\n\nJul 18, 2023\n\n\nJohannes Buck\n\n\n\n\n\n\n  \n\n\n\n\nMonte Carlo Simulation\n\n\n\n\n\n\n\nstatistics\n\n\nrisk management\n\n\n\n\nMonte Carlo simulation is a technique for estimating the probability distribution of a system using random sampling techniques.\n\n\n\n\n\n\nJun 27, 2023\n\n\nJohannes Buck\n\n\n\n\n\n\n  \n\n\n\n\nSimulation Models\n\n\n\n\n\n\n\nstatistics\n\n\nrisk management\n\n\n\n\nAll models are wrong, but some are useful.\n\n\n\n\n\n\nJun 17, 2023\n\n\nJohannes Buck\n\n\n\n\n\n\n  \n\n\n\n\nSystem Models\n\n\n\n\n\n\n\nstatistics\n\n\nrisk management\n\n\n\n\nQuantitative risk assessment relies on models. A model is an abstraction of reality used to gain clarity about a problem and its potential solutions by reducing the complexity.\n\n\n\n\n\n\nJun 16, 2023\n\n\nJohannes Buck\n\n\n\n\n\n\n  \n\n\n\n\nRisk perception and assessment\n\n\n\n\n\n\n\ncyber security\n\n\nrisk management\n\n\n\n\nCognitive biases are systematic errors in the way individuals view the world based on subjective perceptions of reality.\n\n\n\n\n\n\nJun 8, 2023\n\n\nJohannes Buck\n\n\n\n\n\n\n  \n\n\n\n\nRisk Matrices – Why they don’t work\n\n\n\n\n\n\n\ncyber security\n\n\nrisk management\n\n\n\n\nDo we use risk matrices even though we know the limitations and shortcomings of this popular risk assessment tool?\n\n\n\n\n\n\nJun 4, 2023\n\n\nJohannes Buck\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJun 2, 2023\n\n\nJohannes Buck\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/risk matrices/index.html",
    "href": "posts/risk matrices/index.html",
    "title": "Risk Matrices – Why they don’t work",
    "section": "",
    "text": "This post provides some insights that can help you use risk matrices more effectively. It adapted from an article I published on my website, B Advisory, on April 1, 2019.\n\nRisk matrices\nRisk matrices are used in risk assessments to define the level of risk by considering the likelihood of occurrence and impact. It is a simple mechanism for visualizing risk and supporting management decision making. Risk matrices are mentioned in several standards such as ISO and NIST. However, risk matrices have several shortcomings.\n\n\nRisk basics\nRisk is expressed as a combination of the likelihood of an event occurring and the impact on the business expressed in the equation:\n\n\n\nFigure 1: Risk Equation\n\n\n\n\nSelective representation of individual risks\nThe risk matrix is a presentation of individual risks, often arbitrarily selected. Dependencies, interactions and dynamics between individual risks are ignored.\nRisk matrices, by their nature, are not suitable for risk aggregation or risk portfolio presentation. Risks cannot simply be added up. Risk aggregation requires risk quantification based on probability distributions or stochastic processes. Monte Carlo simulation is one method used for risk aggregation.\n\n\n\n\n\nFigure 2: Aggregation scheme for a distribution-based determination of the overall risk\n\n\nPoor resolution\nA 5x5 risk matrix is often used for risk assessment.\n\n\n\n\n\nFigure 3: classic 5x5 risk matrix with linear scales*\nMost security risks do not have a linear risk distribution where five categories (rare, unlikely, possible, likely, certain) or (negligible, minor, moderate, critical, catastrophic) would be appropriate. A logarithmic representation for both likelihood and impact provides a much better resolution with a wider spread. A risk matrix with a double-logarithmic scale allows to illustrate even so-called tail risks with extremely low probability and very high impact to be represented.\n\n\n\n\n\nFigure 4: risk matrix with logarithmic scales\n\n\nAmbiguous inputs and outputs\nInputs to risk matrices (likelihood and impact) are often subjective and biased. Different assessors may arrive at opposite ratings for the same risks. Sometimes the likelihood ratings are underscored with frequencies for the occurrence of an event, e.g., three times a year, and the impact categories are defined by financial losses, e.g., $ 500,000, to guide risk assessors and avoid subjectivity and bias 1). However, the actual ratings are still based on the judgment of individuals and are not supported or derived from reliable data. In addition, most risk assessors rely on their experience based on past events and incidents. In fact, risk assessors should be trying to anticipate the likelihood and impact of future events, which is obviously a challenge.\n1) The National Institute of Standards & Technology (NIST) standard SP 800-30r1 “Guide for Conducting Risk Assessments” provides a starting point for rating scales in Appendix G, which should be tailored to fit any organization-specific conditions. For both likelihood and impact, the qualitative values of very low, low, moderate, high, and high are converted to semi-quantitative values of 0-4, 5-20, 21-79, 80-95, and 96-100, respectively.\n\n\nMisleading risk representation\nA correct representation of the semi-quantitative categories defined by NIST SP 800-30r1 results in a completely different risk matrix than the common 5x5 risk matrix.\n\n\n\n\n\nFigure 5: classic 5x5 risk matrix with equal squares\n\n\n\n\n\nFigure 6: 5x5 matrix with rectangles considering the unequal dimensions of the semi-quantitative categories according to NIST SP 800-30r1\nA classic risk matrix with equal squares means that risks with the same risk rating (Risk = Likelihood x Impact) lie on a hyperbola. A circumstance most people are hardly aware of.\n\n\n\n\n\nFigure 7: risk matrix with linear scales and risk isolines1)\nIn comparison a risk matrix with double-logarithmic scale, where the risk isolines are straight lines.\n\n\n\n\n\nFigure 8: risk matrix with logarithmic scale and risk isolines\n1) Isoline: A line of constant value on a map or chart.\n\n\nFalse assumption\nRisk matrices imply that the effort to reduce risk is the same for all risks in the same field of the risk matrix. This is incorrect for obvious reasons. Risk mitigation costs vary widely. Cost/benefit is a critical factor in risk treatment, and management must decide how best to spend limited resources to reduce overall risk.\n\n\nRisk appetite\nRisk appetite is the level of risk that an organization is willing and able to accept. The challenge with risk appetite is how to implement and enforce it and make it relevant to the business on a day-to-day basis, i.e. linking risk appetite to risk acceptance decisions.\nIt is common to visualize risk appetite using a staircase diagram. The picture suggests that all risks from the red, yellow and amber areas should be reduced, i.e. moved to the green area below the risk appetite line.\n\n\n\n\n\nFigure 9: 5x5 matrix with risk appetite line\nRisk acceptance should be a decision-making process at management or board level, not an automatic process triggered by a threshold. Risk acceptance should be a conscious decision based on risk/return, strategic objectives and risk capacity, not based on a baseline approach.\n\n\nBottom line\nIt is worth questioning common and widely used concepts such as the risk matrix. The risk matrix is a good example of how complex issues cannot be simplified as much as one would like.\n\nSimple scoring methods do not alleviate the fundamental problem of limited information.\nSimple risk presentation is in no way appropriate, although risk matrices work perfectly in a PowerPoint business culture.\n\nA double-logarithmic risk matrix is probably not appropriate for most audiences, but perhaps a risk matrix with rectangles is, given the uneven dimensions of the semi-quantitative categories. However, risk matrices should be used with caution, with careful interpretation, explanation, and sound background knowledge."
  },
  {
    "objectID": "posts/risk perception and assessment/risk perception and assessment.html",
    "href": "posts/risk perception and assessment/risk perception and assessment.html",
    "title": "Risk perception and assessment",
    "section": "",
    "text": "The following list includes some cognitive biases that can negatively affect risk perception and assessment:\n1. Rare events are underestimated and frequent events are overestimated.\n2. The greater the potential for harm, the greater the perceived risk.\n3. Risks to which one is exposed on a daily basis or risks that one believes one knows well are perceived as less threatening.\n4. Risks that have recently occurred are perceived as threatening.\n5. If a risk is taken voluntarily, it is perceived as less threatening.\n6. If the damage is reversible, it is perceived as less dangerous.\n7. If you are personally affected, a risk is perceived as more dangerous.\n8. If a benefit is not apparent, a risk is perceived as threatening.\nIn addition, risk perception and assessment are distorted by unrealistic optimism (“Nothing will happen to me!”) and illusions of control (“I can handle it!”).\nApart from the inherent shortcomings and weaknesses of qualitative risk management, cognitive biases are difficult to overcome and have a major impact on the results of qualitative risk assessments."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is my first post in the β Blog. Welcome!\n\nPicture from Rudy and Peter Skitterian on Pixaby\nProfessionally, I work in cyber security and risk management. As far as my time allows, I will publish blogs on these topics. The blogs will be based on my many years of experience and will focus on what has worked in practice. There will also be blogs on more theoretical topics that are of particular interest to me.\nThe blogs reflect my personal view and do not claim to be general. Different views are possible on any topic. I consider critical discussion to be especially important and valuable. The blog is published under the CC BY-SA 4.0 license. Knowledge should be shared freely and without restrictions."
  },
  {
    "objectID": "posts/system models.html",
    "href": "posts/system models.html",
    "title": "System Models",
    "section": "",
    "text": "Quantitative risk assessment relies on models. A model is an abstraction of reality used to gain clarity about a problem and its potential solutions by reducing the complexity.\nSystem models can be classified in relation to their randomness (deterministic or stochastic), time-dependence (static or dynamic), and number types (continuous or discrete).\nFigure 1: Taxonomy of model types"
  },
  {
    "objectID": "posts/system models.html#deterministic-models",
    "href": "posts/system models.html#deterministic-models",
    "title": "System Models",
    "section": "Deterministic models",
    "text": "Deterministic models\nDeterministic models are based on the assumption that the future can be predicted with certainty, and they use a fixed set of inputs to produce a single, deterministic output. These models are often simpler and easier to understand than stochastic models, and they can be useful in situations where the input data is well-defined and reliable. However, deterministic models may not capture the full range of potential outcomes and can be overly optimistic about risk. Therefore, deterministic models are suitable for operational risk management.\n\nFigure 2: Deterministic Model (reproduced after [1])"
  },
  {
    "objectID": "posts/system models.html#stochastic-models",
    "href": "posts/system models.html#stochastic-models",
    "title": "System Models",
    "section": "Stochastic models",
    "text": "Stochastic models\nStochastic models incorporate randomness and uncertainty into their calculations, allowing them to generate a range of possible outcomes and associated probabilities. This can be particularly useful in situations where the input data is incomplete or subject to fluctuations over time. Stochastic models are more complex than deterministic models, but they are often better suited to capturing the full range of potential risks and their likelihoods. They are also more adaptable to changing circumstances and can provide more robust risk assessments.\n\nFigure 3: Stochastic Model (reproduced after [2])"
  },
  {
    "objectID": "posts/system models.html#static-models",
    "href": "posts/system models.html#static-models",
    "title": "System Models",
    "section": "Static models",
    "text": "Static models\nStatic models are typically used to represent operational risks that do not change significantly over time, such as risks associated with a particular process, product, or service. For example, a static model could be used to identify and assess the operational risks associated with a specific product line or business unit. Static models can be useful for providing a high-level overview of operational risks and identifying areas for improvement."
  },
  {
    "objectID": "posts/system models.html#dynamic-models",
    "href": "posts/system models.html#dynamic-models",
    "title": "System Models",
    "section": "Dynamic models",
    "text": "Dynamic models\nDynamic models take into account the changing nature of operational risks over time, allowing for the modeling of complex interactions and feedback loops. These models can be useful in situations where the risk environment is constantly evolving or where there are multiple sources of risk that can interact with one another.\n\nFigure 4: Static vs. dynamic models"
  },
  {
    "objectID": "posts/system models.html#discrete-models",
    "href": "posts/system models.html#discrete-models",
    "title": "System Models",
    "section": "Discrete models",
    "text": "Discrete models\nDiscrete models use variables that change only at discrete set of points in time. Most operational risks are based on scenarios with a continuous range of values. However, discrete models can be used to analyze a specific events or scenarios that may lead to operational failures or losses. For example, a discrete model could be used to identify the specific events that could lead to a cyber attack or a supply chain disruption"
  },
  {
    "objectID": "posts/system models.html#continuous-models",
    "href": "posts/system models.html#continuous-models",
    "title": "System Models",
    "section": "Continuous models",
    "text": "Continuous models\nContinuous models are designed to model risks that occur over a continuous range of values, such as financial losses or process failures. These models can be useful for capturing the full distribution of possible outcomes and assessing the potential impact of different levels of risk exposure.\n\nFigure 5: Discrete vs. continuous models (reproduced after [3])"
  },
  {
    "objectID": "posts/system models.html#models-for-operational-risks",
    "href": "posts/system models.html#models-for-operational-risks",
    "title": "System Models",
    "section": "Models for operational risks",
    "text": "Models for operational risks\nStochastic, dynamic, continuous models are suitable for operational risk management because they are designed to capture the inherent complexity and variability of operational risks.\nDeterministic, static. discrete models are not appropriate for operational risk management because operational risks change over time (they are dynamic, not static) and operational risk parameters have continuous values.\nReferences\n[1] Kelton, W.D.; Law, A.M. 2000. Simulation Modeling and Analysis; Boston, MA, USA: McGraw Hill.\n[2] Platon, V.; Constantinescu, A. Monte Carlo Method in Risk Analysis for Investment Projects; Procedia Economics and Finance 15 (2014) 393 – 400. https://doi.org/10.1016/S2212-5671(14)00463-8.\n[3] R. Holzer, P. Wuchner, and H. de Meer, “Modeling of Self-Organizing Systems: An Overview” Electronic Communications of the EASST vol. 27 (2010). http://dx.doi.org/10.14279/tuj.eceasst.27.385."
  },
  {
    "objectID": "posts/system models.html#footnotes",
    "href": "posts/system models.html#footnotes",
    "title": "System Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMarkdown is a simple language.↩︎"
  },
  {
    "objectID": "posts/system models/system models.html",
    "href": "posts/system models/system models.html",
    "title": "System Models",
    "section": "",
    "text": "System models can be classified in relation to their randomness (deterministic or stochastic), time-dependence (static or dynamic), and number types (continuous or discrete).\nFigure 1: Taxonomy of model types"
  },
  {
    "objectID": "posts/system models/system models.html#deterministic-models",
    "href": "posts/system models/system models.html#deterministic-models",
    "title": "System Models",
    "section": "Deterministic models",
    "text": "Deterministic models\nDeterministic models are based on the assumption that the future can be predicted with certainty, and they use a fixed set of inputs to produce a single, deterministic output. These models are often simpler and easier to understand than stochastic models, and they can be useful in situations where the input data is well-defined and reliable. However, deterministic models may not capture the full range of potential outcomes and can be overly optimistic about risk. Therefore, deterministic models are suitable for operational risk management.\n\nFigure 2: Deterministic Model (reproduced after [1])"
  },
  {
    "objectID": "posts/system models/system models.html#stochastic-models",
    "href": "posts/system models/system models.html#stochastic-models",
    "title": "System Models",
    "section": "Stochastic models",
    "text": "Stochastic models\nStochastic models incorporate randomness and uncertainty into their calculations, allowing them to generate a range of possible outcomes and associated probabilities. This can be particularly useful in situations where the input data is incomplete or subject to fluctuations over time. Stochastic models are more complex than deterministic models, but they are often better suited to capturing the full range of potential risks and their likelihoods. They are also more adaptable to changing circumstances and can provide more robust risk assessments.\n\nFigure 3: Stochastic Model (reproduced after [2])"
  },
  {
    "objectID": "posts/system models/system models.html#static-models",
    "href": "posts/system models/system models.html#static-models",
    "title": "System Models",
    "section": "Static models",
    "text": "Static models\nStatic models are typically used to represent operational risks that do not change significantly over time, such as risks associated with a particular process, product, or service. For example, a static model could be used to identify and assess the operational risks associated with a specific product line or business unit. Static models can be useful for providing a high-level overview of operational risks and identifying areas for improvement."
  },
  {
    "objectID": "posts/system models/system models.html#dynamic-models",
    "href": "posts/system models/system models.html#dynamic-models",
    "title": "System Models",
    "section": "Dynamic models",
    "text": "Dynamic models\nDynamic models take into account the changing nature of operational risks over time, allowing for the modeling of complex interactions and feedback loops. These models can be useful in situations where the risk environment is constantly evolving or where there are multiple sources of risk that can interact with one another.\n\nFigure 4: Static vs. dynamic models"
  },
  {
    "objectID": "posts/system models/system models.html#discrete-models",
    "href": "posts/system models/system models.html#discrete-models",
    "title": "System Models",
    "section": "Discrete models",
    "text": "Discrete models\nDiscrete models use variables that change only at discrete set of points in time. Most operational risks are based on scenarios with a continuous range of values. However, discrete models can be used to analyze a specific events or scenarios that may lead to operational failures or losses. For example, a discrete model could be used to identify the specific events that could lead to a cyber attack or a supply chain disruption"
  },
  {
    "objectID": "posts/system models/system models.html#continuous-models",
    "href": "posts/system models/system models.html#continuous-models",
    "title": "System Models",
    "section": "Continuous models",
    "text": "Continuous models\nContinuous models are designed to model risks that occur over a continuous range of values, such as financial losses or process failures. These models can be useful for capturing the full distribution of possible outcomes and assessing the potential impact of different levels of risk exposure.\n\nFigure 5: Discrete vs. continuous models (reproduced after [3])"
  },
  {
    "objectID": "posts/system models/system models.html#models-for-operational-risks",
    "href": "posts/system models/system models.html#models-for-operational-risks",
    "title": "System Models",
    "section": "Models for operational risks",
    "text": "Models for operational risks\nStochastic, dynamic, continuous models are suitable for operational risk management because they are designed to capture the inherent complexity and variability of operational risks.\nDeterministic, static. discrete models are not appropriate for operational risk management because operational risks change over time (they are dynamic, not static) and operational risk parameters have continuous values.\nReferences\n[1] Kelton, W.D.; Law, A.M. 2000. Simulation Modeling and Analysis; Boston, MA, USA: McGraw Hill.\n[2] Platon, V.; Constantinescu, A. Monte Carlo Method in Risk Analysis for Investment Projects; Procedia Economics and Finance 15 (2014) 393 – 400. https://doi.org/10.1016/S2212-5671(14)00463-8.\n[3] R. Holzer, P. Wuchner, and H. de Meer, “Modeling of Self-Organizing Systems: An Overview” Electronic Communications of the EASST vol. 27 (2010). http://dx.doi.org/10.14279/tuj.eceasst.27.385."
  },
  {
    "objectID": "posts/simulation models/simulation models.html",
    "href": "posts/simulation models/simulation models.html",
    "title": "Simulation Models",
    "section": "",
    "text": "The statement “All models are wrong, but some are useful.” is usually attributed to the statistician George Box [1].\nA model is an approximation of reality. Reality is more complex than any model."
  },
  {
    "objectID": "posts/simulation models/simulation models.html#deterministic-simulation-model",
    "href": "posts/simulation models/simulation models.html#deterministic-simulation-model",
    "title": "Simulation Models",
    "section": "Deterministic simulation model",
    "text": "Deterministic simulation model\nA deterministic simulation model does not contain any random variables. Deterministic simulation models have a known set of inputs that result in a unique set of outputs.\nExamples:\n\nPatients arriving at the doctor’s office at the scheduled time.\nA production line with fixed processing times for each part\nTrain, bus, or plane schedules\nPricing structures for products or services\nLinear programming models for finding the best solution to a problem with linear constraints and a linear objective function (e.g., optimal delivery route)\n\nNote: The examples show that deterministic simulation models simplify reality (patients are delayed, production processes fail due to technical problems)."
  },
  {
    "objectID": "posts/simulation models/simulation models.html#stochastic-simulation-model",
    "href": "posts/simulation models/simulation models.html#stochastic-simulation-model",
    "title": "Simulation Models",
    "section": "Stochastic simulation model",
    "text": "Stochastic simulation model\nA stochastic simulation model has one or more random variables as inputs. Random inputs result in random outputs. Probability distributions such as Bernoulli distribution, Binominal distribution, Poisson distribution, etc. are used for stochastic simulation models.\nExamples:\n\nTurbulent flow over an airplane wing\nDynamic pricing\nWeather forecasting\nEarth climatology\nFinancial markets\nPopulation dynamics"
  },
  {
    "objectID": "posts/simulation models/simulation models.html#static-simulation-model",
    "href": "posts/simulation models/simulation models.html#static-simulation-model",
    "title": "Simulation Models",
    "section": "Static simulation model",
    "text": "Static simulation model\nA static simulation model or steady-state model is time-invariant, i.e. inputs and parameters do not change over time.\nStatic simulation models are (steatdy-state) memoryless systems because the output does not depend on past input signals \\(x(n-1)\\). It depends only on present input signals \\(x(n)\\).\nMathematical example:\n\\(y(n) = 6 \\cdot x(n)\\)\n\\(6\\) is a constant that multiplies the input \\(x(n)\\). The output \\(y(n)\\) is simultaneously dependent on the input \\(x(n)\\).\nPhysical example:\nThe load on a bridge. While this static model does not account for the effects of moving traffic, wind, earthquakes, or other factors that may change over time, it provides a useful first-order analysis of whether the bridge can support the weight it is designed to carry.\nNote: Static models are often used as an approximation of dynamic models to simplify the model and to reduce the complexity and the computational effort for the simulation."
  },
  {
    "objectID": "posts/simulation models/simulation models.html#dynamic-simulation-model",
    "href": "posts/simulation models/simulation models.html#dynamic-simulation-model",
    "title": "Simulation Models",
    "section": "Dynamic simulation model",
    "text": "Dynamic simulation model\nA dynamic simulation model is time-dependent, i.e. the state of the system changes over time. Dynamic simulation models require memory to store time-based values. Dynamic simulation models are used to evaluate scenarios that change over time or to analyze trends. Dynamic models are often described by differential equations where the state variables change continuously.\nMathematical example:\n\\(y(n) = x(n) + 6x \\cdot (n-2)\\)\nIf \\(x(n)\\) is the current input signal, then\n\n\\(x(n-t\\)) is the past signal\n\\(x(n+t)\\) is the future signal\n\nPhysical example:\nA dynamic simulation model of a bridge taking into account the effects of loads caused by moving traffic, wind, and earthquakes."
  },
  {
    "objectID": "posts/simulation models/simulation models.html#discrete-simulation-model",
    "href": "posts/simulation models/simulation models.html#discrete-simulation-model",
    "title": "Simulation Models",
    "section": "Discrete simulation model",
    "text": "Discrete simulation model\nA discrete model is one in which the state variables change only at a discrete set of times. For example, a store where the number of customers (state variable) changes only when a customer arrives or leaves.\nExample: Lifeboats and life jackets [2]\nA shipyard is designing a new ferry boat and wants to determine the number of lifeboats and life jackets it should have.\n\nEach life jacket holds 1 person and requires \\(0.05^\\,m3\\) of storage space.\nEach lifeboat holds 20 people and requires \\(2.1^\\,m3\\) of storage space.\nWe need to have capacity for 1,000 people in one or the other. - The total space to be devoted to this equipment is at most \\(85\\,m^3\\).\n\nHow many of each should be installed?\nYou could go through the following steps to determine a solution:\n\nFirst, see if it’s possible to put everyone in lifeboats, which would be best. This would require \\(\\frac{1000}{20} \\cdot 2.1 = 105\\,m^3\\) of space, so the answer is no, we must use some combination.\nMake sure there’s enough space for life jackets for everyone, otherwise the problem is impossible: \\(1000 \\cdot .05 = 50\\,m^3\\), so it would be possible to provide only life jackets and have space left over. But for the best solution, we want to use more of the available space and provide as many boats as possible.\nFigure out how many boats and life jackets will be used if we use exactly all the space available and provide exactly the capacity we need. We can frame this as a mathematical problem. There are two unknowns\n\n\n\\(x_1\\) = number of jackets\n\\(x_2\\) = number of boats\n\nand two equations to satisfy, one saying that the total capacity must be 1,000 people and the other saying that the total volume must be \\(85\\,m^3\\) . This is a linear system of equations:\n\\[\\begin{equation} \\tag{1.1}\n     \\displaylines{C_1x_1 + C_2x_2 = C\\\\V_1x_1 + V_2x_2 = V}\n\\end{equation}\\]\nwhere\n\n\\(C_1\\) = 1 (capacity of each jacket)\n\\(C_2\\) = 20 (capacity of each boat)\n\\(C\\) = 1000 (total capacity needed)\n\\(V_1\\) = 0.05 (volume of each jacket)\n\\(V_2\\) = 2.1 (volume of each boat)\n\\(V\\) = 85 (total available volume)\n\n\nSolve the mathematical problem. Equation (1.1) is just a 2 × 2 linear system, so it is easy to solve, obtaining\n\n\n\\(x_1\\) = 363.6364 (number of jackets)\n\\(x_2\\) = 31.8182 (number of boats)\n\nInterpreting the mathematical solution We cannot actually provide fractional jackets or boats, so we have to fix up these numbers. The only way to do this and keep the volume from increasing is to reduce the number of boats to 31 and increase the number of jackets to keep the capacity at 1,000. Since 31 × 20 = 620 this means we need 380 jackets. The total volume required is \\(84.1\\,m^3\\) .\nNote that the lifeboats are placed on symmetrical racks on both sides of the boat, so there must be an even number of lifeboats. Using this new information, revise the solution. There must be 30 boats and 400 life jackets.\nReconsider the solution to see if we have missed anything.\nNotice that the original mathematical solution had \\(x_2\\) almost equal to 32. This suggests that if we had just a little more space (\\(85.2\\,m^3\\) ), we could increase the number of boats to 32 instead of 30, a significant improvement. The shipyard could find another \\(0.2\\,m^3\\), which would allow two more boats to fit."
  },
  {
    "objectID": "posts/simulation models/simulation models.html#continuous-simulation-model",
    "href": "posts/simulation models/simulation models.html#continuous-simulation-model",
    "title": "Simulation Models",
    "section": "Continuous simulation model",
    "text": "Continuous simulation model\nA continuous model is one in which the state variables change continuously over time.\nContinuous, static model\nExample: Voltage drop across a resistor\n\\(V = I \\cdot R\\)\nThe voltage drop depends on the value of the current at that moment. Therefore, it is a static system. A continuous model is static (stationary or memoryless) if its output depends only on the current input.\n\nFigure 1: Electrical circuit no. 1\n\nFigure 2: Voltage curve as function of current and resistor\nContinuous, dynamic model\nExample: Charging of a capacitor\nA series circuit consisting of a resistor, a capacitor, a switch, and a constant DC voltage source $V_0$ is called a charging circuit.\nIf the capacitor is initially uncharged while the switch is open, and the switch is closed at $t=0$, Kirchhoff’s law of voltage states that:\n\\(V_S - R \\cdot i(t) - V_C(t) = 0\\)\n\nFigure 3: Electric circuit no 2\n\nFigure 4: Voltage curve of capacitor charging\nReferences:\n[1] George E. P. Box. “Science and Statistics.” Journal of the American Statistical Association 71, no. 356 (1976): 791–99.https://doi.org/10.2307/2286841.\n[2] Billey, S., Burke, J., Chartier, T., Greenbaum, A., & LeVequw R. (2010). Discrete Mathematical Modeling: Math 381 Course Notes. University of Washington. 381no&lt;tes_new.pdf (washington.edu)."
  },
  {
    "objectID": "posts/monte carlo simulation/monte carlo simulation.html",
    "href": "posts/monte carlo simulation/monte carlo simulation.html",
    "title": "Monte Carlo Simulation",
    "section": "",
    "text": "In Monte Carlo simulation, the behavior of a system is simulated using random numbers and probability distributions to replicate the uncertainties and variability that exist in the real world. It is particularly useful when the system or process being analyzed is too complex to solve analytically or when the underlying mathematical model is not well defined.\nMonte Carlo simulation is a stochastic method, which means that results can vary from simulation to simulation due to the random sampling involved. However, by using a large number of iterations, the simulation results converge to more accurate estimates.\nFigure 1: Schematic of Monte Carlo simulation"
  },
  {
    "objectID": "posts/monte carlo simulation/monte carlo simulation.html#process",
    "href": "posts/monte carlo simulation/monte carlo simulation.html#process",
    "title": "Monte Carlo Simulation",
    "section": "Process",
    "text": "Process\nThe process steps involved in performing a Monte Carlo simulation are as follows:\n\nDefine the problem: identify the system to be analyzed.\nIdentify the parameters: identify the parameters of the system and their probability distributions. This includes specifying the ranges or values that each parameter can take, as well as their respective probabilities.\nGenerate random samples: generate a set of random numbers that correspond to the probability distributions of the parameters.\nSimulate the system: calculate the output values based on the input parameters.\nAnalyze the results: analyze the probability distribution of the output variables.\nValidate the results: compare the results with observations or experimental data."
  },
  {
    "objectID": "posts/monte carlo simulation/monte carlo simulation.html#purpose",
    "href": "posts/monte carlo simulation/monte carlo simulation.html#purpose",
    "title": "Monte Carlo Simulation",
    "section": "Purpose",
    "text": "Purpose\nMonte Carlo simulation is a useful method for analyzing uncertain scenarios and providing probabilistic analysis of various situations. Monte Carlo simulation can be used for both static and dynamic simulations.\nIn a static simulation, the input variables are fixed and do not change over time. Monte Carlo simulation can be used to estimate the distribution of the output variable(s) under these fixed input conditions. For example, Monte Carlo simulation can be used to estimate the distribution of project costs under fixed assumptions about the duration, scope, and, resource requirements of the project.\nIn a dynamic simulation, the input variables change over time, and the simulation models the behavior of the system or process over time. Monte Carlo simulation can be used to estimate the distribution of the output variable(s) at different points in time under changing input conditions. For example, Monte Carlo simulation can be used to estimate the distribution of a portfolio’s returns over a period of time, taking into account changing market conditions and other factors that affect the portfolio’s performance.\nExample: Monte Carlo method applied to approximating the value of π\nThis is done by picking random points \\((x,y \\mid x \\in [-1..1] \\wedge y \\in [-1..1])\\) and checking (using Pythagoras’ theorem) if they are inside the unit circle:\n\\[x^2 + y^2 \\leq 1\\]\nThe ratio of the number of points inside and outside the circle can then be determined as follows:\n\\[\\frac{circle\\,area}{square\\,area} = \\frac{r^2 \\cdot \\pi}{(2 \\cdot r)^2} = \\frac {\\pi}{4} = \\frac {hits\\,in\\,circle\\,area}{generated\\,points\\,in\\,the\\,square} = P\\,{(in\\,the\\,circle)}\\]\nR code\n\nnum_darts &lt;- 1000\nnum_darts_in_circle &lt;- 0\n\nfor(i in 1:num_darts) {\n  x &lt;- runif(n = 1, min = -1, max = 1)\n  y &lt;- runif(n = 1, min = -1, max = 1)\n  if(x^2 + y^2 &lt;= 1) {\n    num_darts_in_circle = num_darts_in_circle + 1\n  }\n}\n\nprint(4 * num_darts_in_circle / num_darts)\n\n[1] 3.124\n\n\nR code for optimized runtime with vector operations\n\nlibrary(\"plotrix\")\n\nnum_darts &lt;- 1000000\nnum_darts_in_circle &lt;- 0\n\nx &lt;- runif(n = num_darts, min = -1, max = 1)\ny &lt;- runif(n = num_darts, min = -1, max = 1)\n\nsum_squares &lt;- x^2 + y^2\nindexes_darts_in_circle &lt;- which(sum_squares &lt;= 1)\nnum_darts_in_circle &lt;- length(indexes_darts_in_circle)\nprint(4 * num_darts_in_circle / num_darts)\n\n[1] 3.144388\n\n\n\n\n\n\n\nPlotting the results"
  },
  {
    "objectID": "posts/monte carlo simulation/monte carlo simulation (2).html",
    "href": "posts/monte carlo simulation/monte carlo simulation (2).html",
    "title": "Monte Carlo Simulation",
    "section": "",
    "text": "Monte Carlo simulation is a technique for estimating the probability distribution of a system using random sampling techniques.\nIn Monte Carlo simulation, the behavior of a system is simulated using random numbers and probability distributions to replicate the uncertainties and variability that exist in the real world. It is particularly useful when the system or process being analyzed is too complex to solve analytically or when the underlying mathematical model is not well defined.\nMonte Carlo simulation is a stochastic method, which means that results can vary from simulation to simulation due to the random sampling involved. However, by using a large number of iterations, the simulation results converge to more accurate estimates.\nFigure 1: Schematic of Monte Carlo simulation"
  },
  {
    "objectID": "posts/monte carlo simulation/monte carlo simulation (2).html#process",
    "href": "posts/monte carlo simulation/monte carlo simulation (2).html#process",
    "title": "Monte Carlo Simulation",
    "section": "Process",
    "text": "Process\nThe process steps involved in performing a Monte Carlo simulation are as follows:\n\nDefine the problem: identify the system to be analyzed.\nIdentify the parameters: identify the parameters of the system and their probability distributions. This includes specifying the ranges or values that each parameter can take, as well as their respective probabilities.\nGenerate random samples: generate a set of random numbers that correspond to the probability distributions of the parameters.\nSimulate the system: calculate the output values based on the input parameters.\nAnalyze the results: analyze the probability distribution of the output variables.\nValidate the results: compare the results with observations or experimental data."
  },
  {
    "objectID": "posts/monte carlo simulation/monte carlo simulation (2).html#purpose",
    "href": "posts/monte carlo simulation/monte carlo simulation (2).html#purpose",
    "title": "Monte Carlo Simulation",
    "section": "Purpose",
    "text": "Purpose\nMonte Carlo simulation is a useful method for analyzing uncertain scenarios and providing probabilistic analysis of various situations. Monte Carlo simulation can be used for both static and dynamic simulations.\nIn a static simulation, the input variables are fixed and do not change over time. Monte Carlo simulation can be used to estimate the distribution of the output variable(s) under these fixed input conditions. For example, Monte Carlo simulation can be used to estimate the distribution of project costs under fixed assumptions about the duration, scope, and, resource requirements of the project.\nIn a dynamic simulation, the input variables change over time, and the simulation models the behavior of the system or process over time. Monte Carlo simulation can be used to estimate the distribution of the output variable(s) at different points in time under changing input conditions. For example, Monte Carlo simulation can be used to estimate the distribution of a portfolio’s returns over a period of time, taking into account changing market conditions and other factors that affect the portfolio’s performance.\nExample: Monte Carlo method applied to approximating the value of π\nThis is done by picking random points \\((x,y \\mid x \\in [-1..1] \\wedge y \\in [-1..1])\\) and checking (using Pythagoras’ theorem) if they are inside the unit circle:\n\\[x^2 + y^2 \\leq 1\\]\nThe ratio of the number of points inside and outside the circle can then be determined as follows:\n\\[\\frac{circle\\,area}{square\\,area} = \\frac{r^2 \\cdot \\pi}{(2 \\cdot r)^2} = \\frac {\\pi}{4} = \\frac {hits\\,in\\,circle\\,area}{generated\\,points\\,in\\,the\\,square} = P\\,{(in\\,the\\,circle)}\\]\nR code\n\nnum_darts &lt;- 1000\nnum_darts_in_circle &lt;- 0\n\nfor(i in 1:num_darts) {\n  x &lt;- runif(n = 1, min = -1, max = 1)\n  y &lt;- runif(n = 1, min = -1, max = 1)\n  if(x^2 + y^2 &lt;= 1) {\n    num_darts_in_circle = num_darts_in_circle + 1\n  }\n}\n\nprint(4 * num_darts_in_circle / num_darts)\n\n[1] 3.048\n\n\nR code for optimized runtime with vector operations\n\nlibrary(\"plotrix\")\n\nnum_darts &lt;- 1000000\nnum_darts_in_circle &lt;- 0\n\nx &lt;- runif(n = num_darts, min = -1, max = 1)\ny &lt;- runif(n = num_darts, min = -1, max = 1)\n\nsum_squares &lt;- x^2 + y^2\nindexes_darts_in_circle &lt;- which(sum_squares &lt;= 1)\nnum_darts_in_circle &lt;- length(indexes_darts_in_circle)\nprint(4 * num_darts_in_circle / num_darts)\n\n[1] 3.141048\n\n\nR code with plotting the results\n\nlibrary(\"plotrix\")\n\nnum_darts &lt;- 100\nnum_darts_in_circle &lt;- 0\n\nx &lt;- runif(n = num_darts, min = -1, max = 1)\ny &lt;- runif(n = num_darts, min = -1, max = 1)\n\nsum_squares &lt;- x^2 + y^2\nindexes_darts_in_circle &lt;- which(sum_squares &lt;= 1)\nnum_darts_in_circle &lt;- length(indexes_darts_in_circle)\n\nplot(x[1:i], y[1:i], xlim = c(-1, 1), ylim = c(-1, 1), asp = 1)\nrect(-1, -1, 1, 1)\ndraw.circle(0, 0, 1, nv = 1000, border = \"red\", col = \"lightblue\", lty = 1, density = 10, lwd = 1)"
  },
  {
    "objectID": "posts/risk matrices/risk matrices.html",
    "href": "posts/risk matrices/risk matrices.html",
    "title": "Risk Matrices – Why they don’t work",
    "section": "",
    "text": "This post provides some insights that can help you use risk matrices more effectively. It adapted from an article I published on my website, B Advisory, on April 1, 2019.\n\nRisk matrices\nRisk matrices are used in risk assessments to define the level of risk by considering the likelihood of occurrence and impact. It is a simple mechanism for visualizing risk and supporting management decision making. Risk matrices are mentioned in several standards such as ISO and NIST. However, risk matrices have several shortcomings.\n\n\nRisk basics\nRisk is expressed as a combination of the likelihood of an event occurring and the impact on the business expressed in the equation:\n\n\n\nFigure 1: Risk Equation\n\n\n\n\nSelective representation of individual risks\nThe risk matrix is a presentation of individual risks, often arbitrarily selected. Dependencies, interactions and dynamics between individual risks are ignored.\nRisk matrices, by their nature, are not suitable for risk aggregation or risk portfolio presentation. Risks cannot simply be added up. Risk aggregation requires risk quantification based on probability distributions or stochastic processes. Monte Carlo simulation is one method used for risk aggregation.\n\n\n\n\n\nFigure 2: Aggregation scheme for a distribution-based determination of the overall risk\n\n\nPoor resolution\nA 5x5 risk matrix is often used for risk assessment.\n\n\n\n\n\nFigure 3: classic 5x5 risk matrix with linear scales*\nMost security risks do not have a linear risk distribution where five categories (rare, unlikely, possible, likely, certain) or (negligible, minor, moderate, critical, catastrophic) would be appropriate. A logarithmic representation for both likelihood and impact provides a much better resolution with a wider spread. A risk matrix with a double-logarithmic scale allows to illustrate even so-called tail risks with extremely low probability and very high impact to be represented.\n\n\n\n\n\nFigure 4: risk matrix with logarithmic scales\n\n\nAmbiguous inputs and outputs\nInputs to risk matrices (likelihood and impact) are often subjective and biased. Different assessors may arrive at opposite ratings for the same risks. Sometimes the likelihood ratings are underscored with frequencies for the occurrence of an event, e.g., three times a year, and the impact categories are defined by financial losses, e.g., $ 500,000, to guide risk assessors and avoid subjectivity and bias 1). However, the actual ratings are still based on the judgment of individuals and are not supported or derived from reliable data. In addition, most risk assessors rely on their experience based on past events and incidents. In fact, risk assessors should be trying to anticipate the likelihood and impact of future events, which is obviously a challenge.\n1) The National Institute of Standards & Technology (NIST) standard SP 800-30r1 “Guide for Conducting Risk Assessments” provides a starting point for rating scales in Appendix G, which should be tailored to fit any organization-specific conditions. For both likelihood and impact, the qualitative values of very low, low, moderate, high, and high are converted to semi-quantitative values of 0-4, 5-20, 21-79, 80-95, and 96-100, respectively.\n\n\nMisleading risk representation\nA correct representation of the semi-quantitative categories defined by NIST SP 800-30r1 results in a completely different risk matrix than the common 5x5 risk matrix.\n\n\n\n\n\nFigure 5: classic 5x5 risk matrix with equal squares\n\n\n\n\n\nFigure 6: 5x5 matrix with rectangles considering the unequal dimensions of the semi-quantitative categories according to NIST SP 800-30r1\nA classic risk matrix with equal squares means that risks with the same risk rating (Risk = Likelihood x Impact) lie on a hyperbola. A circumstance most people are hardly aware of.\n\n\n\n\n\nFigure 7: risk matrix with linear scales and risk isolines1)\nIn comparison a risk matrix with double-logarithmic scale, where the risk isolines are straight lines.\n\n\n\n\n\nFigure 8: risk matrix with logarithmic scale and risk isolines\n1) Isoline: A line of constant value on a map or chart.\n\n\nFalse assumption\nRisk matrices imply that the effort to reduce risk is the same for all risks in the same field of the risk matrix. This is incorrect for obvious reasons. Risk mitigation costs vary widely. Cost/benefit is a critical factor in risk treatment, and management must decide how best to spend limited resources to reduce overall risk.\n\n\nRisk appetite\nRisk appetite is the level of risk that an organization is willing and able to accept. The challenge with risk appetite is how to implement and enforce it and make it relevant to the business on a day-to-day basis, i.e. linking risk appetite to risk acceptance decisions.\nIt is common to visualize risk appetite using a staircase diagram. The picture suggests that all risks from the red, yellow and amber areas should be reduced, i.e. moved to the green area below the risk appetite line.\n\n\n\n\n\nFigure 9: 5x5 matrix with risk appetite line\nRisk acceptance should be a decision-making process at management or board level, not an automatic process triggered by a threshold. Risk acceptance should be a conscious decision based on risk/return, strategic objectives and risk capacity, not based on a baseline approach.\n\n\nBottom line\nIt is worth questioning common and widely used concepts such as the risk matrix. The risk matrix is a good example of how complex issues cannot be simplified as much as one would like.\n\nSimple scoring methods do not alleviate the fundamental problem of limited information.\nSimple risk presentation is in no way appropriate, although risk matrices work perfectly in a PowerPoint business culture.\n\nA double-logarithmic risk matrix is probably not appropriate for most audiences, but perhaps a risk matrix with rectangles is, given the uneven dimensions of the semi-quantitative categories. However, risk matrices should be used with caution, with careful interpretation, explanation, and sound background knowledge."
  },
  {
    "objectID": "posts/triangle distribution/triangle distribution.html",
    "href": "posts/triangle distribution/triangle distribution.html",
    "title": "Triangle Distribution",
    "section": "",
    "text": "The triangular distribution (also known as the Simpson distribution) is a continuous probability distribution. The graph of the probability density function looks like a triangle and gives this distribution its name.\nThe probability density function moves between a minimum \\(a\\), a maximum \\(b\\) and the mode \\(c\\) (the value with the highest probability). The \\(y\\) axis shows the density of the respective probability for a value \\(x \\in [a, b]\\).\nFor a triangular distribution, the maximum of the probability density is at the value of the mode. For the lower and upper limit, the value of the probability density is zero.\nWhen very little concrete data is available to determine the distribution function \\(f(x)\\) of a random variable \\(x\\), the triangular distribution is often used in practice. The triangular distribution is also a popular distribution in Monte Carlo simulation.\nTriangular distributions can take the following forms: symmetric distribution, right-skewed distribution, and left-skewed distribution."
  },
  {
    "objectID": "posts/triangle distribution/triangle distribution.html#probability-density-function-pdf",
    "href": "posts/triangle distribution/triangle distribution.html#probability-density-function-pdf",
    "title": "Triangle Distribution",
    "section": "Probability Density Function (PDF)",
    "text": "Probability Density Function (PDF)\nThe probability density function of the triangular distribution in the interval \\(x \\in [a, b]\\) is defined by\n\\[\\begin{equation} \\tag{1.1}\n    f(x) = \\begin{cases}\n        \\dfrac{2(x-a)}{(b-a)(c-a)} & \\text{for } x \\in [a, c] \\\\\n        \\dfrac{2(b-x)}{(b-a)(b-c)} & \\text{for } x \\in (c, b] \\\\\n        {0} & \\text{else}\n    \\end{cases}\n\\end{equation}\\]\n\n\n\n\n\n\nNote\n\n\n\nThe PDF (probability density function) in the triangular distribution shows how the probability is distributed across different values within a given interval. The triangular distribution is a continuous probability distribution in which the probability first increases linearly, then peaks, and then decreases linearly. The shape of the triangular distribution is determined by three parameters: the left tail value \\(a\\), the right tail value \\(b\\), and the mode \\(c\\) (the value with the highest probability)."
  },
  {
    "objectID": "posts/triangle distribution/triangle distribution.html#title",
    "href": "posts/triangle distribution/triangle distribution.html#title",
    "title": "Triangle Distribution",
    "section": "Title",
    "text": "Title\n…\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "posts/triangle distribution/triangle distribution.html#cumulative-distribution-function-cdf",
    "href": "posts/triangle distribution/triangle distribution.html#cumulative-distribution-function-cdf",
    "title": "Triangle Distribution",
    "section": "Cumulative Distribution Function (CDF)",
    "text": "Cumulative Distribution Function (CDF)\nBy integration, the cumulative distribution function \\(F(x)\\) from equation (1.1) becomes\n\\[\\begin{equation} \\tag{1.2}\n    F(x) = \\begin{cases}\n        \\dfrac{(x-a)^2}{(b-a)(c-a)} & \\text{for } x \\in [a, c] \\\\\n        1-\\dfrac{(b-x)^2}{(b-a)(b-c)} & \\text{for } x \\in (c, b] \\\\\n        {0} & \\text{else}\n    \\end{cases}\n\\end{equation}\\]\n\n\n\n\n\n\nNote\n\n\n\nThe cumulative distribution function (CDF) of a probability distribution indicates the probability that a random variable will take a value less than or equal to a given value. The CDF of a triangular distribution increases slowly at first, peaks at \\(c\\), and then decreases until it reaches the value 1 when the random variable reaches the maximum value \\(b\\). The meaning of the CDF is that it indicates the cumulative probability that the random variable will take \\(a\\) value less than or equal to a certain value."
  },
  {
    "objectID": "posts/triangle distribution/triangle distribution.html#mode",
    "href": "posts/triangle distribution/triangle distribution.html#mode",
    "title": "Triangle Distribution",
    "section": "Mode",
    "text": "Mode\nThe mode is the value that occurs most frequently. The mode of a triangular distribution is the value of \\(x\\) at which the probability density function (PDF) of the reaches its maximum value.\nAt the point \\(c\\) in \\([a,b]\\) (mode), the probability distribution \\(f(x)\\) has its maximum, which is given by \\[ \\begin{equation} \\tag{1.3}\n    F(\\infty) = \\int_{- \\infty}^\\infty {f(x)}{dx} = \\frac{b-a}{2} \\cdot f(c) = 1\n\\end{equation} \\]\nresults to \\[ \\begin{equation} \\tag{1.4}\n    f(c) = \\frac{2}{b-a}\n\\end{equation} \\]\nThe mean and median of a triangular distribution are not necessarily the same as the mode. The mean and median depend on the specific parameters of the distribution, while the mode depends only on the range \\([a, b]\\)."
  },
  {
    "objectID": "posts/triangle distribution/triangle distribution.html#expected-value",
    "href": "posts/triangle distribution/triangle distribution.html#expected-value",
    "title": "Triangle Distribution",
    "section": "Expected Value",
    "text": "Expected Value\nThe expected value is the weighted average of all possible outcomes, taking into account the probabilities of the outcomes. For a triangular distribution, the expected value is calculated by adding the values of the three parameters of the distribution (minimum, maximum and mode) and dividing by 3. \\[ \\begin{equation} \\tag{1.6}\n    E(x) = \\frac{a+b+c}3\n\\end{equation} \\]"
  },
  {
    "objectID": "posts/triangle distribution/triangle distribution.html#mean",
    "href": "posts/triangle distribution/triangle distribution.html#mean",
    "title": "Triangle Distribution",
    "section": "Mean",
    "text": "Mean\nThe mean is the arithmetic average of all observed values. In the case of a triangular distribution, the mean depends on the position of the mode in the distribution. If the mode is exactly halfway between the minimum and maximum, the mean is equal to the expected value. \\[ \\begin{equation} \\tag{1.7}\n    \\bar{x} = \\frac{b-a}2\n\\end{equation} \\]\n\n\n\n\n\n\nNote\n\n\n\nIn an asymmetric triangular distribution, the expected value and the mean are not identical. This is only the case with a symmetric triangular distribution. In general, the expected value of a triangular distribution is a better measure than the mean to indicate the average value of the distribution."
  },
  {
    "objectID": "posts/triangle distribution/triangle distribution.html#interpretation-and-use",
    "href": "posts/triangle distribution/triangle distribution.html#interpretation-and-use",
    "title": "Triangle Distribution",
    "section": "Interpretation and Use",
    "text": "Interpretation and Use\nIn many real-world applications, sparse data is available to estimate a concrete distribution of the population.\nIn management decision making where the Best Case, the Worst Case and the Most Likely Case are known or can be estimated. In this case, the triangular distribution can be used.\nThe triangular distribution can also be used when absolute bounds are to be set for the minimum and maximum, but the distribution is similar to a lognormal distribution."
  },
  {
    "objectID": "posts/triangle distribution/triangle distribution.html#example",
    "href": "posts/triangle distribution/triangle distribution.html#example",
    "title": "Triangle Distribution",
    "section": "Example",
    "text": "Example\nA software developer estimates that it will take an average of 60 hours to program the application. In the best case, he estimates a processing time of 45 hours, but if problems occur, the effort can increase to 150 hours. This results in a triangular distribution with \\(a=45\\) (minimum), \\(b=150\\) (maximum), and \\(c=60\\) (most likely value).\nThe expected value for the project is: \\[E = \\frac{a+b+c}{3} = \\frac{45+150+60}{3} h = 85\\,h\\]\nThe maximum (mode) of the triangular distribution is: \\[f(c) = \\frac{2}{b-a} = \\frac{2}{150-45} = 0.019\\]\nThe probability for the programmer’s estimate is: \\[\nF(x=c) =  \\frac{(c-a)^2}{(b-a)(c-a)} = \\frac{c-a}{b-a} = \\frac{60-45}{150-45} = 0.143\n\\]\nThe probability of the expected value is: \\[\nF(x=E) = 1- \\frac{(b-E)^2}{(b-a)(b-c)} = 1- \\frac{(150-85)^2}{(150-45)(150-60)} = 0.553\n\\]\nConclusion\nThe probability of the developer’s estimate of the project duration (60 h) is with 14% less likely than the probability of the expected value (85 h) at 55%.\n\n\n\n\n\nR Code\n# Triangular Distribution\n\nlibrary(EnvStats)\n# Probability density function (PDF): dtri(q, min, max, mode)\n# Cumulative distribution function (CDF): ptri(q, min, max, mode)\n# q: vector of quantiles, missing values (NA) are allowed\n\n# calculate probability density function (PDF)\ndtri(x = 60, min = 45, max = 150, mode = 60) \n\n# Cumulative distribution function (CDF)\nptri(q = 60, min = 45, max = 150, mode = 60)\n\n# calculate probability density function (PDF)\ndtri(x = 85, min = 45, max = 150, mode = 60) \n\n# Cumulative distribution function (CDF)\nptri(q = 85, min = 45, max = 150, mode = 60) \nConsole output:\n[1] 0.01904762\n\n[1] 0.1428571\n \n[1] 0.01375661\n\n[1] 0.5529101"
  },
  {
    "objectID": "posts/triangle distribution/triangle distribution.html#interpretation",
    "href": "posts/triangle distribution/triangle distribution.html#interpretation",
    "title": "Triangle Distribution",
    "section": "Interpretation",
    "text": "Interpretation\nIn many real-world applications, sparse data is available to estimate a concrete distribution of the population.\nIn management decision making where the Best Case, the Worst Case and the Most Likely Case are known or can be estimated. In this case, the triangular distribution can be used.\nThe triangular distribution can also be used when absolute bounds are to be set for the minimum and maximum, but the distribution is similar to a lognormal distribution."
  },
  {
    "objectID": "posts/triangle distribution/triangle distribution.html#using-the-triangular-distribution-in-practice",
    "href": "posts/triangle distribution/triangle distribution.html#using-the-triangular-distribution-in-practice",
    "title": "Triangle Distribution",
    "section": "Using the triangular distribution in practice",
    "text": "Using the triangular distribution in practice\nThe triangular distribution is used for management decisions where the Best Case, the Worst Case and the Most Likely Case are known or can be estimated."
  },
  {
    "objectID": "posts/confidence interval/confidence interval.html",
    "href": "posts/confidence interval/confidence interval.html",
    "title": "Confidence Intervall",
    "section": "",
    "text": "There are two estimation methods in inferential statistics:\n\npoint estimation\ninterval estimation\n\n\n\nIn point estimation, properties about the population are determined as exact values and are usually given a measure of the estimation error.\n\n\n\nIn interval estimation, an interval is specified for the characteristic of the population, and this is then given a confidence interval (e.g., 95%).\n\n\n\nPoint estimation pretends “pseudo-accuracy”. The estimated parameter value is given with point accuracy, although the true parameter value is very rarely hit exactly.\n\nYou can compare a point estimate to trying to hit a fly (the true parameter value) with a pin (point estimate). With a fly swatter (interval), the probability of hitting the fly is much higher."
  },
  {
    "objectID": "posts/confidence interval/confidence interval.html#parameter-estimation",
    "href": "posts/confidence interval/confidence interval.html#parameter-estimation",
    "title": "Confidence Intervall",
    "section": "",
    "text": "There are two estimation methods in inferential statistics:\n\npoint estimation\ninterval estimation\n\n\n\nIn point estimation, properties about the population are determined as exact values and are usually given a measure of the estimation error.\n\n\n\nIn interval estimation, an interval is specified for the characteristic of the population, and this is then given a confidence interval (e.g., 95%).\n\n\n\nPoint estimation pretends “pseudo-accuracy”. The estimated parameter value is given with point accuracy, although the true parameter value is very rarely hit exactly.\n\nYou can compare a point estimate to trying to hit a fly (the true parameter value) with a pin (point estimate). With a fly swatter (interval), the probability of hitting the fly is much higher."
  },
  {
    "objectID": "posts/confidence interval/confidence interval.html#confidence-interval-definition",
    "href": "posts/confidence interval/confidence interval.html#confidence-interval-definition",
    "title": "Confidence Intervall",
    "section": "Confidence Interval Definition",
    "text": "Confidence Interval Definition\nA confidence interval is a statistical interval that contains the location of a true parameter of a population with a certain probability. A confidence level of 95% contains the true value with 95% probability.\n\nThe intervals \\(\\hat{\\theta}_A\\), \\(\\hat{\\theta}_B\\) and \\(\\hat{\\theta}_E\\) do not contain the true value \\(\\theta\\).\nThe intervals \\(\\hat{\\theta}_A\\) and \\(\\hat{\\theta}_E\\) belong to the 5% of the “least likely” (farthest from the true value) sample values.\nA confidence interval has a lower bound \\(x_u\\) and an upper bound \\(x_o\\).\n\\(\\bar{x}\\) denotes the mean of the data set, \\(z_u\\) and \\(z_o\\) are the z-transformed interval limits.\nThe standard error is the fraction of the standard deviation \\(s_x\\) and the square root of the sample size \\(n\\).\n\\[ \\begin{equation} \\tag{1.0}\n    \\large x_u = \\bar{x} + z_u \\cdot \\frac{s_x}{\\sqrt{n}}\n\\end{equation} \\]\n\\[ \\begin{equation} \\tag{1.1}\n    \\large x_o = \\bar{x} + z_o \\cdot \\frac{s_x}{\\sqrt{n}}\n\\end{equation} \\]"
  },
  {
    "objectID": "r_scripts/r basic plots.html",
    "href": "r_scripts/r basic plots.html",
    "title": "R Basic Plots",
    "section": "",
    "text": "…\nR Code\n\n1 + 1\n\n[1] 2\n\n\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code, only the output is displayed."
  },
  {
    "objectID": "r_scripts/r basic plots.html#title",
    "href": "r_scripts/r basic plots.html#title",
    "title": "R Basic Plots",
    "section": "",
    "text": "…\nR Code\n\n1 + 1\n\n[1] 2\n\n\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code, only the output is displayed."
  },
  {
    "objectID": "posts/bootstrapping/bootstrapping.html",
    "href": "posts/bootstrapping/bootstrapping.html",
    "title": "Bootstrapping",
    "section": "",
    "text": "Bootstrapping is a statistical method that resamples a single data set to create many samples. This process allows to calculate standard errors, construct confidence intervals, and perform hypothesis testing for numerous types of sample statistics.\nThe basic idea of bootstrapping is that inference about a population from sample data (sample → population) can be modeled by resampling the sample data and performing inference about a sample from resampled data (resampled → sample).\n\nFigure 1: Traditional statistical method\n\nFigure 2: Bootstrapping method\nSimple Random Samples (SRS)\nA simple random sample (SRS) of size \\(n\\) consists of \\(n\\) individuals from the population, selected so that each set of \\(n\\) individuals has an equal chance of being the sample actually selected.\nPopulations vs. Samples\nParameters describe populations, statistics describe samples.\n\nThe mean (\\(\\mu\\)) and standard deviation (\\(\\sigma\\)) of a population are parameters.\nThe mean (\\(\\bar{x}\\)) and standard deviation (\\(s\\)) of a sample are statistics."
  },
  {
    "objectID": "posts/bootstrapping/bootstrapping.html#bootstrapping-method",
    "href": "posts/bootstrapping/bootstrapping.html#bootstrapping-method",
    "title": "Bootstrapping",
    "section": "",
    "text": "Bootstrapping is a statistical method that resamples a single data set to create many samples. This process allows to calculate standard errors, construct confidence intervals, and perform hypothesis testing for numerous types of sample statistics.\nThe basic idea of bootstrapping is that inference about a population from sample data (sample → population) can be modeled by resampling the sample data and performing inference about a sample from resampled data (resampled → sample).\n\nFigure 1: Traditional statistical method\n\nFigure 2: Bootstrapping method\nSimple Random Samples (SRS)\nA simple random sample (SRS) of size \\(n\\) consists of \\(n\\) individuals from the population, selected so that each set of \\(n\\) individuals has an equal chance of being the sample actually selected.\nPopulations vs. Samples\nParameters describe populations, statistics describe samples.\n\nThe mean (\\(\\mu\\)) and standard deviation (\\(\\sigma\\)) of a population are parameters.\nThe mean (\\(\\bar{x}\\)) and standard deviation (\\(s\\)) of a sample are statistics."
  },
  {
    "objectID": "posts/bootstrapping/bootstrapping.html#types-of-bootstrapping",
    "href": "posts/bootstrapping/bootstrapping.html#types-of-bootstrapping",
    "title": "Bootstrapping",
    "section": "Types of bootstrapping",
    "text": "Types of bootstrapping\n\nParametric Bootstrapping\nParametric bootstraps resample a known distribution function $F$ (e.g., normal) whose parameters (e.g., mean, variance) are estimated from the sample data.\n\n\nNonparametric Bootstraps\nNonparametric bootstraps make no assumptions \\(\\hat{F}\\) about the underlying data distribution \\(F\\).\nProcess steps:\n\nEstimate the parameters of the hypothesized parametric model (parametric bootstrapping only).\nTake a random sample from the original data by sampling with replacement, with the same number of items as in the original data set.\nCompute the statistic of interest (e.g., mean, median, standard deviation) from the bootstrapped sample.\nRepeat steps \\((2)\\) and \\((3)\\) a large number of times.\nAnalyze the distribution of the bootstrap statistics or statistics of interest (e.g., mean, standard deviation, confidence intervals).\n\n\n\n\n\n\n\nNote\n\n\n\nParametric bootstrapping can be efficient when the underlying data distribution is well known. However, it can lead to biased results if the assumed model is incorrect.\nNonparametric bootstrapping is more flexible because it does not rely on specific distribution assumptions. It can be particularly useful when the underlying data distribution is unknown or complex. However, it may require a larger number of bootstrap samples to obtain accurate estimates compared to parametric bootstrapping.\n\\(\\rightarrow\\) Parametric bootstrapping assumes a specific distribution and estimates parameters from the data, while nonparametric bootstrapping makes no assumption about the distribution and estimates the sampling distribution directly through resampling."
  },
  {
    "objectID": "posts/bootstrapping/bootstrapping.html#bootstrapping-vs.-monte-carlo-simulation",
    "href": "posts/bootstrapping/bootstrapping.html#bootstrapping-vs.-monte-carlo-simulation",
    "title": "Bootstrapping",
    "section": "Bootstrapping vs. Monte Carlo Simulation",
    "text": "Bootstrapping vs. Monte Carlo Simulation\nBootstrapping and Monte Carlo simulation are two different statistical techniques that are used for different purposes. Bootstrapping can be seen as a specific application of Monte Carlo methods, where random resampling is used to estimate sampling distributions and perform statistical inference.\n\nAbout Bootstrapping\nBootstrapping is a resampling technique used to estimate the sampling distribution of a statistic by repeatedly sampling with replacement from the original data.\nAdvantages\n\nNonparametric: Bootstrapping does not assume a specific underlying distribution, making it useful when the distribution of the data is unknown or non-standard.\nEasy to implement: Bootstrapping is conceptually simple and easy to implement.\nUseful with small sample sizes: It works with limited data where traditional statistical tests may be less reliable.\n\nDisadvantages\n\nComputationally intensive: Bootstrapping involves many resampling iterations, which can be computationally expensive for large datasets or complex models.\nData quality dependence: Bootstrapping results are highly dependent on the quality and representativeness of the original data.\nNot suitable for all scenarios: While powerful, bootstrapping may not be the best approach for certain statistical problems, especially those involving complex models or non-standard scenarios."
  },
  {
    "objectID": "posts/bootstrapping/bootstrapping.html#monte-carlo-simulation",
    "href": "posts/bootstrapping/bootstrapping.html#monte-carlo-simulation",
    "title": "Bootstrapping",
    "section": "Monte Carlo Simulation",
    "text": "Monte Carlo Simulation\nMonte Carlo simulation is a technique used to model and analyze the probability of different outcomes in a process that involves uncertainty and randomness. It relies on random sampling to estimate complex mathematical functions or to simulate different scenarios.\nAdvantages\n\nFlexibility: Monte Carlo simulation is applicable to a wide range of problems, including optimization, probability estimation, and risk analysis, making it a versatile tool.\nProvides a range of outcomes: It provides a distribution of possible outcomes, giving insight into the likelihood of different scenarios occurring.\nWorks well with optimization problems: Monte Carlo simulation can be integrated with optimization algorithms to find optimal solutions in complex systems.\n\nDisadvantages\n\nResource intensive: Monte Carlo simulation requires a large number of iterations to produce reliable results, which can be computationally expensive and time consuming.\nConvergence issues: In some cases, Monte Carlo simulation may converge slowly or require special techniques to ensure accurate results.\n\nKey Differences Between Monte Carlo Simulation and Bootstrapping\n\n\n\n\n\n\n\n\nFeature\nMote Carlo simulation\nBootstrapping method\n\n\n\n\nGenerality\nCan be used to estimate the uncertainty of any statistic\nOnly used to estimate the uncertainty of statistics from a sample of data\n\n\nEfficiency\nLess efficient than bootstrapping\nMore efficient than Monte Carlo simulation\n\n\nEase of use\nMore difficult to use than bootstrapping\nEasier to use than Monte Carlo simulation"
  },
  {
    "objectID": "posts/confidence interval/confidence interval.html#r-code",
    "href": "posts/confidence interval/confidence interval.html#r-code",
    "title": "Confidence Intervall",
    "section": "R code",
    "text": "R code\nExample 1 – Confidence interval of dataset\n\n# Calculate confidence interval with function t.test from core package stats\n\n# Sample data\ndata &lt;- c(23, 28, 32, 27, 25, 30, 31, 29, 26, 24)\n\n# Calculate the confidence interval\n# The function t.test performs the Student's t-Tests on vectors of data\n# By default conf.level=0.95\nresult &lt;- t.test(data)\n\n# Extract the confidence interval\nconfidence_interval &lt;- result$conf.int\n\n# Get the lower bound and the upper bound of the confidence interval\nlower_bound &lt;- result$conf.int[1]\nupper_bound &lt;- result$conf.int[2]\n\n# Print lower bound and uppder bound of the confidence interval\ncat(\"Lower bound of the 95% confidence interval:\", lower_bound, \"\\n\")\n\nLower bound of the 95% confidence interval: 25.33415 \n\ncat(\"Upper bound of the 95% confidence interval:\", upper_bound, \"\\n\")\n\nUpper bound of the 95% confidence interval: 29.66585 \n\n# Plot histogram\nplot(data, main = \"Histogram\", xlab = \"Data\", ylab = \"Value\")\n\n# Draw lower bound and upper bound of the confidence interval\nabline(h = lower_bound, col = 'red', lwd = 1, lty = 2)\ntext(x = 1.5, y = lower_bound+0.3, labels = \"lower bound\", col = \"#4D4D4D\")\nabline(h = upper_bound, col = 'red', lwd = 1, lty = 2)\ntext(x = 1.5, y = upper_bound+0.3, labels = \"upper bound\", col = \"#4D4D4D\")\n\n\n\n\nThe 95% confidence interval is [25.3, 29.7].\nExample 2 – Univariate Statistic (Median)\nFor this example, we will generate \\(n = 100\\) observations from a standard normal distribution, and use the median as the parameter/statistic of interest. Note that the true (population) median is zero. Since the median is a univariate statistic, the bootstrap distribution will be a vector of length \\(R + 1\\) containing the bootstrap replicates of the median.\n\n# Univariate Statistic (Median)\n\nlibrary(nptest)\n\nPackage 'nptest' version 1.1\nType 'citation(\"nptest\")' to cite this package.\n\n# generate 100 standard normal observations\nset.seed(1)\nn &lt;- 100\nx &lt;- rnorm(n)\n\n# nonparametric bootstrap\nnpbs &lt;- np.boot(x = x, statistic = median)\nnpbs\n\n\nNonparametric Bootstrap of Univariate Statistic\nusing R = 9999 bootstrap replicates\n\n  t0: 0.1139\n  SE: 0.1394\nBias: 0.0185 \n\nBCa Confidence Intervals:\n      lower  upper\n90% -0.0566 0.3411\n95% -0.0811 0.3673\n99% -0.1351 0.3940\n\n# check t0, SE, and bias\nmedian(x)                          # t0\n\n[1] 0.1139092\n\nsd(npbs$boot.dist)                 # SE\n\n[1] 0.1393567\n\nmean(npbs$boot.dist) - npbs$t0     # Bias\n\n[1] 0.01845341\n\n# bootstrap distribution\nhist(npbs$boot.dist,\n     xlab = \"t*\",\n     ylab = \"Density\",\n     main = \"Bootstrap Distribution\")\n\n# Observed media\nabline(v = npbs$t0,\n       lty = 2,\n       col = \"red\")\n\n# CI 95% lower boundary\nabline(v = npbs$bca[2, 1],\n       lty = 2,\n       col = \"blue\")\n\n# CI 95% upper boundary\nabline(v = npbs$bca[2, 2],\n       lty = 2,\n       col = \"blue\")\n\nlegend(\n  \"topleft\",\n  legend = c(\"t0\", \"CI 95%\"),\n  col = c(\"red\", \"blue\"),\n  lty = 2,\n  bty = \"n\"\n)"
  },
  {
    "objectID": "posts/bootstrapping/bootstrapping.html#r-code",
    "href": "posts/bootstrapping/bootstrapping.html#r-code",
    "title": "Bootstrapping",
    "section": "R code",
    "text": "R code\nThere are several R packages that can be used for bootstrapping calculations with R:\n\nboot\nbootstrap\nnptest\n\n\nExample 1 – Multivariate Statistic (Median)\nFor this example, we will generate \\(n = 100\\) observations from a standard normal distribution, and use the quartiles as the parameters/statistics of interest. Note that the true (population) quartiles are \\(Q1 = qnorm(0.25) = -0.6744898\\), \\(Q2 = qnorm(0.5) = 0\\), and \\(Q3 = qnorm(0.75) = 0.6744898\\). Since the quartiles are a multivariate statistic, the bootstrap distribution will be a matrix of dimension \\(R + 1 × 3\\), where each column contains the bootstrap replicates of the corresponding quartile.\n\n# Multivariate Statistic (Quartiles)\n\nlibrary(nptest)\n\nPackage 'nptest' version 1.1\nType 'citation(\"nptest\")' to cite this package.\n\n# generate 100 standard normal observations\nset.seed(1)\nn &lt;- 100\nx &lt;- rnorm(n)\n\n# nonparametric bootstrap (using ... to enter 'probs' argument)\nnpbs &lt;- np.boot(x = x,\n                statistic = quantile,\n                probs = c(0.25, 0.5, 0.75))\nnpbs\n\n\nNonparametric Bootstrap of Multivariate Statistic\nusing R = 9999 bootstrap replicates\n\n          25%    50%     75%\n  t0: -0.4942 0.1139  0.6915\n  SE:  0.1172 0.1394  0.0933\nBias:  0.0058 0.0185 -0.0170\n\n95% BCa Confidence Intervals:\n          25%     50%    75%\nlower -0.6941 -0.0811 0.5047\nupper -0.2534  0.3673 0.8811\n\n# bootstrap distribution\npar(mfrow = c(1, 3))\nfor (j in 1:3) {\n  hist(\n    npbs$boot.dist[, j],\n    xlab = \"t*\",\n    ylab = \"Density\",\n    main = paste0(\"Bootstrap Distribution\", \": Q\", j)\n  )\n  abline(v = npbs$t0[j],\n         lty = 2,\n         col = \"red\")\n  legend(\n    \"topright\",\n    paste0(\"t0[\", j, \"]\"),\n    lty = 2,\n    col = \"red\",\n    bty = \"n\"\n  )\n}"
  },
  {
    "objectID": "posts/confidence interval/confidence interval.html#example",
    "href": "posts/confidence interval/confidence interval.html#example",
    "title": "Confidence Intervall",
    "section": "Example",
    "text": "Example\nCalculate the 95% confidence interval for the monthly snack spending of a group of people.\nStep 1: Calculate the estimated parameter\nCalculate the mean \\(\\bar{x}\\) from the existing data set.\n\n\n\n# participants\nMonthly snack spending USD\n\n\n\n\n20\n8\n\n\n30\n32\n\n\n10\n0\n\n\n30\n48\n\n\n10\n16\n\n\n\nSample size \\(\\large n = 20+30+10+30+10 = 100\\)\n\\(\\large \\bar{x}=\\frac{20\\cdot8+30\\cdot32+10\\cdot0+30\\cdot48+10\\cdot16}{100}= \\text{USD } 27.2\\)\nStep 2: Transform the interval limits into standard normally distributed values\nWith a confidence level of 95%, we obtain the following interval limits: the lower limit is 2.5% \\(= x_u = 0.025\\) and the upper limit is 97.5% \\(x_o = 0.975\\).\nWe now need to standardize these values by looking at the z-distribution table.\n\nFor the upper bound \\(x_o\\), the z-value is easy to read: \\(x_o = 0.975\\) corresponds to \\(z_o = 1.960\\).\nSince the normal distribution is symmetric and mirrored on the x-axis, the lower limit \\(z_u = -1.960\\) results in\n\n\nWith a mean of \\(\\bar{x} = 27.20\\) USD, a lower limit of \\(z_u= -1.960\\), an upper limit of \\(z_o= 1.960\\) and a sample size of \\(n = 100\\), all required parameters for the formula are given except for the standard deviation \\(s_x\\).\nStep 3: Calculation of variance and standard deviation\nThe variance can be calculated using the values from the table as follows:\n\\(\\large s_x^2=\\ \\frac{\\left(8-27.2\\right)^2\\cdot20+\\left(32-27.2\\right)^2\\cdot30+\\left(0-27.2\\right)^2\\cdot10+\\left(48-27.2\\right)^2\\cdot30+\\left(16-27.2\\right)^2\\cdot10}{100}\\)\n\\(\\large s_x^2=296.96\\)\nThe standard deviation is the square root of the variance:\n\\(\\large s_x=\\sqrt{s_x^2}=\\sqrt{296.96}=17.23\\)\nStep 4: Calculation of lower and upper limit\n\\(\\large x_u=\\ \\bar{x}+\\ z_u\\ \\cdot\\ \\frac{s_x}{\\sqrt n}=27.2+\\left(-1.960\\right)\\cdot\\frac{17.23}{\\sqrt{100}}=23.82\\)\n\\(\\large x_o=\\ \\bar{x}+\\ z_o\\ \\cdot\\ \\frac{s_x}{\\sqrt n}=27.2+1.960\\cdot\\frac{17.23}{\\sqrt{100}}=30.58\\)\nThus, the limits of the contingency interval are at points 23.82 and 30.58. The conclusion is: with 95% confidence, the true mean for the group of people’s monthly snack spending is between USD 23.82 and USDF 30.58."
  },
  {
    "objectID": "posts/confidence interval/confidence interval.html#z-distribution",
    "href": "posts/confidence interval/confidence interval.html#z-distribution",
    "title": "Confidence Intervall",
    "section": "Z-Distribution",
    "text": "Z-Distribution\nBy performing a z-standardization, we can transform the normal distribution into a standard normal distribution. This can be done with any normal distribution (but only with normal distributions). This gives us a distribution that is always the same, with mean \\(µ = 0\\) and standard deviation \\(σ = 1\\). Just like the normal distribution, the standard normal distribution tells you what percentage of values fall within a given range. Now, this range no longer depends on \\(µ\\) and \\(σ\\), but is standardized.\n\n\nIn the standard normal distribution, approximately 68.2% of the values always lie in the range between \\(z = -1\\) and \\(z = +1\\). Or put another way: There is a 68.2% probability that the value is between -1 and +1.\nUsing a z-transformation or z-standardization, you subtract the arithmetic mean from each measured value, divide the resulting difference by the standard deviation, and you get the so-called z-scores.\nAs formula (with \\(x\\) for the respective value, \\(μ\\) for the arithmetic mean and \\(σ\\) for the standard deviation) \\[z=\\frac{x-μ}σ\\]\nAfter the z-transformation\n\nthe arithmetic mean of the transformed series is always zero, and\nthe variance and standard deviation are always 1.\n\nIf a normal distribution is present, the corresponding probabilities can be read from a standard normal distribution table based on the z-values. The standard normal table contains only positive z-values. Since the normal distribution is symmetric and mirrored on the x-axis, the z-value for the lower limit results from the negative z-value for the upper limit \\(z_u = -z_o\\)."
  },
  {
    "objectID": "posts/confidence interval/confidence interval.html#three-sigma-rule",
    "href": "posts/confidence interval/confidence interval.html#three-sigma-rule",
    "title": "Confidence Intervall",
    "section": "Three Sigma Rule",
    "text": "Three Sigma Rule\nThe “Three Sigma Rule” is a statistical rule used to quantify the probability of events within a normally distributed data set.\n\nThe three sigma rule states that\na) 68.3% of the data are within one standard deviation of the mean (\\(\\mu \\pm \\sigma\\)).\nb) 95.4% of the data are within two standard deviations of the mean (\\(\\mu \\pm 2\\sigma\\)).\nc) 99.7% of the data are within 3 standard deviations of the mean (\\(\\mu \\pm 3\\sigma\\)).\n\n\n\n\n\n\nNote\n\n\n\nThe three-sigma rule applies only to normally distributed data sets. The three sigma rule does not apply to skewed triangular distributions."
  }
]